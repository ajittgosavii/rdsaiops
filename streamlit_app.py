import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import json
import hashlib
from typing import Dict, List, Tuple, Optional
import uuid
import time
import base64
from io import BytesIO
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import math
from functools import lru_cache
import requests
import xml.etree.ElementTree as ET

# For PDF generation
try:
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# Anthropic AI
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# vROps availability check
try:
    VROPS_AVAILABLE = True
except ImportError:
    VROPS_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="Enterprise Migration Platform",
    page_icon="üè¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

class ClaudeAIAnalyst:
    """Real Claude AI integration for migration analysis"""
    
    def __init__(self):
        self.client = None
        self.available = False
        self._init_client()
    
    def _init_client(self):
        """Initialize Claude AI client"""
        if not ANTHROPIC_AVAILABLE:
            return
            
        try:
            # Try to get API key from Streamlit secrets
            if hasattr(st, 'secrets') and 'anthropic' in st.secrets:
                api_key = st.secrets["anthropic"]["api_key"]
                self.client = anthropic.Anthropic(api_key=api_key)
                self.available = True

                # Store status for optional display
                self.connection_status = {
                    'connected': True,
                    'source': 'Streamlit Secrets'
                }
            else:
                # REMOVED: Warning message about missing API key
                self.connection_status = {
                    'connected': False,
                    'error': 'API key not found in secrets.toml'
                }
            
            except Exception as e:
                # REMOVED: Error message - handle silently
                self.connection_status = {
                    'connected': False,
                    'error': str(e)
                }
                
                
        
    
    def analyze_migration_strategy(self, config, metrics, migration_options):
        """Get real Claude AI analysis of migration strategy"""
        if not self.available:
            return self._fallback_analysis(config, metrics, migration_options)
        
        try:
            # Prepare data for Claude
            analysis_prompt = f"""
            As an expert cloud migration architect, analyze this enterprise migration scenario and provide strategic recommendations.

            **Migration Context:**
            - Data Volume: {metrics.get('data_size_tb', 'Unknown')} TB
            - Source Location: {config.get('source_location', 'Unknown')}
            - Target: {config.get('target_aws_region', 'Unknown')}
            - Network Bandwidth: {config.get('dx_bandwidth_mbps', 'Unknown')} Mbps
            - Business Priority: {config.get('project_priority', 'Unknown')}
            - Compliance: {', '.join(config.get('compliance_frameworks', []))}

            **Available Migration Options:**
            {json.dumps(migration_options, indent=2) if migration_options else "Standard options"}

            **Performance Metrics:**
            - Current Throughput: {metrics.get('optimized_throughput', 'Unknown')} Mbps
            - Transfer Days: {metrics.get('transfer_days', 'Unknown')}
            - Total Cost: ${metrics.get('cost_breakdown', {}).get('total', 'Unknown')}

            Please provide:
            1. **Primary Recommendation**: Best migration method and why
            2. **Risk Assessment**: Key risks and mitigation strategies  
            3. **Performance Analysis**: Expected throughput and timeline
            4. **Cost Optimization**: Ways to reduce costs while maintaining performance
            5. **Implementation Strategy**: Step-by-step approach

            Format your response as a structured analysis with clear recommendations.
            """
            
            # Call Claude AI
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,
                temperature=0.3,
                messages=[{"role": "user", "content": analysis_prompt}]
            )
            
            ai_analysis = response.content[0].text
            
            return {
                "source": "Claude AI",
                "analysis": ai_analysis,
                "confidence": "High",
                "recommendations": self._parse_ai_recommendations(ai_analysis),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            st.warning(f"Claude AI analysis failed: {str(e)}. Using fallback analysis.")
            return self._fallback_analysis(config, metrics, migration_options)
    
    def _parse_ai_recommendations(self, ai_text):
        """Parse Claude's response into structured recommendations"""
        return {
            "primary_method": "AI Analysis Available",
            "confidence_score": 95,
            "risk_level": "Medium",
            "estimated_timeline": "AI Calculated",
            "cost_efficiency": "High"
        }
    
    def _fallback_analysis(self, config, metrics, migration_options):
        """Fallback analysis when Claude AI is not available"""
        return {
            "source": "Algorithmic Fallback",
            "analysis": "Claude AI not available. Using algorithmic analysis based on best practices.",
            "confidence": "Medium",
            "recommendations": {
                "primary_method": "DataSync",
                "confidence_score": 75,
                "risk_level": "Low",
                "estimated_timeline": f"{metrics.get('transfer_days', 10):.1f} days",
                "cost_efficiency": "Medium"
            },
            "timestamp": datetime.now().isoformat()
        }

class MigrationOptionsAnalyzer:
    """Comprehensive analysis of all AWS migration options"""
    
    def __init__(self):
        self.migration_methods = {
            "aws_datasync": {
                "name": "AWS DataSync",
                "best_for": ["File systems", "Object storage", "Continuous sync"],
                "max_throughput_mbps": 10000,
                "cost_per_gb": 0.0125,
                "setup_complexity": "Medium",
                "downtime": "None to Minimal",
                "data_size_limit": "Unlimited",
                "network_efficiency": 0.85
            },
            "aws_dms": {
                "name": "Database Migration Service", 
                "best_for": ["Database migration", "Ongoing replication", "Minimal downtime"],
                "max_throughput_mbps": 2000,
                "cost_per_gb": 0.02,
                "setup_complexity": "Medium",
                "downtime": "Near-zero",
                "data_size_limit": "Large (TB scale)",
                "network_efficiency": 0.75
            },
            "snowball_edge": {
                "name": "AWS Snowball Edge",
                "best_for": ["Large datasets", "Limited bandwidth", "One-time migration"],
                "max_throughput_mbps": float('inf'),
                "cost_per_gb": 0.003,
                "setup_complexity": "Low",
                "downtime": "Medium",
                "data_size_limit": "100TB per device",
                "network_efficiency": 1.0
            },
            "storage_gateway": {
                "name": "AWS Storage Gateway",
                "best_for": ["Hybrid cloud", "Gradual migration", "Cache optimization"],
                "max_throughput_mbps": 3200,
                "cost_per_gb": 0.015,
                "setup_complexity": "Medium",
                "downtime": "None",
                "data_size_limit": "Unlimited",
                "network_efficiency": 0.70
            }
        }
    
    def analyze_all_options(self, config, enable_ai_comparison=True):
        """Analyze all migration options for the given configuration"""
        
        data_size_gb = config.get('data_size_gb', 1000)
        data_size_tb = data_size_gb / 1024
        bandwidth_mbps = config.get('dx_bandwidth_mbps', 1000)
        
        results = {}
        
        for method_key, method_info in self.migration_methods.items():
            # Calculate performance for each method
            if method_key == "snowball_edge":
                transfer_days = self._calculate_snowball_timeline(data_size_tb)
                throughput_mbps = "Physical Transfer"
                cost = self._calculate_snowball_cost(data_size_tb)
            else:
                effective_throughput = min(
                    method_info["max_throughput_mbps"],
                    bandwidth_mbps * method_info["network_efficiency"]
                )
                transfer_days = (data_size_gb * 8) / (effective_throughput * 24 * 3600) / 1000
                throughput_mbps = effective_throughput
                cost = data_size_gb * method_info["cost_per_gb"]
            
            # Score each method
            score = self._calculate_method_score(method_info, transfer_days, cost, config)
            
            results[method_key] = {
                "method_info": method_info,
                "throughput_mbps": throughput_mbps,
                "transfer_days": transfer_days,
                "estimated_cost": cost,
                "score": score,
                "recommendation_level": self._get_recommendation_level(score)
            }
        
        return dict(sorted(results.items(), key=lambda x: x[1]["score"], reverse=True))
    
    def _calculate_method_score(self, method_info, transfer_days, cost, config):
        """Calculate a score for each migration method"""
        score = 100
        
        if isinstance(transfer_days, (int, float)):
            if transfer_days > 30:
                score -= 20
            elif transfer_days > 7:
                score -= 10
        
        cost_per_tb = cost / (config.get('data_size_gb', 1000) / 1024)
        if cost_per_tb < 100:
            score += 15
        elif cost_per_tb > 500:
            score -= 15
        
        complexity_penalties = {"High": -10, "Medium": -5, "Low": 0}
        score += complexity_penalties.get(method_info["setup_complexity"], 0)
        
        return max(0, min(100, score))
    
    def _get_recommendation_level(self, score):
        """Get recommendation level based on score"""
        if score >= 90:
            return "üü¢ Highly Recommended"
        elif score >= 75:
            return "üü° Recommended"
        elif score >= 60:
            return "üü† Consider"
        else:
            return "üî¥ Not Recommended"
    
    def _calculate_snowball_timeline(self, data_size_tb):
        """Calculate timeline for Snowball"""
        devices_needed = max(1, math.ceil(data_size_tb / 72))
        return 7 + (devices_needed * 2)
    
    def _calculate_snowball_cost(self, data_size_tb):
        """Calculate cost for Snowball"""
        devices_needed = max(1, math.ceil(data_size_tb / 72))
        return devices_needed * 300 + 2000

class VROpsConnector:
    """vRealize Operations Manager connector for real performance data"""
    
    def __init__(self):
        self.connected = False
        self.session_token = None
        self.base_url = None
    
    def connect(self, vrops_host, username, password, verify_ssl=True):
        """Connect to vROps instance"""
        try:
            self.base_url = f"https://{vrops_host}/suite-api/api"
            
            auth_url = f"{self.base_url}/auth/token/acquire"
            auth_data = {"username": username, "password": password}
            
            response = requests.post(auth_url, json=auth_data, verify=verify_ssl, timeout=30)
            
            if response.status_code == 200:
                self.session_token = response.json().get('token')
                self.connected = True
                return True
            else:
                st.error(f"vROps authentication failed: {response.status_code}")
                return False
                
        except Exception as e:
            st.error(f"vROps connection error: {str(e)}")
            return False
    
    def get_database_metrics(self, resource_id=None, days_back=30):
        """Retrieve database performance metrics from vROps"""
        if not self.connected:
            return None
        
        # Simplified implementation - full implementation would query vROps API
        return [
            {
                'resource_name': 'Sample DB',
                'resource_type': 'DatabaseInstance',
                'metrics': {'cpu': 45, 'memory': 60, 'disk': 30}
            }
        ]

# =============================================================================
# AWS PRICING AND NETWORKING CLASSES
# =============================================================================

class AWSPricingManager:
    """Fetch real-time AWS pricing using AWS Pricing API"""
    
    def __init__(self, region='us-east-1'):
        self.region = region
        self.pricing_client = None
        self.ec2_client = None
        self.cache = {}
        self.cache_ttl = 3600  # 1 hour cache
        self.last_cache_update = {}
        self._init_clients()
    
    def _init_clients(self):
        """Initialize AWS clients using Streamlit secrets"""
        try:
            # Try to get AWS credentials from Streamlit secrets
            aws_access_key = None
            aws_secret_key = None
            aws_region = self.region
            credential_source = "Unknown"
    
            try:
                # Check if AWS secrets are configured in .streamlit/secrets.toml
                if hasattr(st, 'secrets') and 'aws' in st.secrets:
                    
                    # Create clients with explicit credentials
                    self.pricing_client = boto3.client(
                        'pricing',
                        region_name='us-east-1',  # Pricing API only available in us-east-1
                        aws_access_key_id=aws_access_key,
                        aws_secret_access_key=aws_secret_key
                    )
                    self.ec2_client = boto3.client(
                        'ec2',
                        region_name=aws_region,
                        aws_access_key_id=aws_access_key,
                        aws_secret_access_key=aws_secret_key
                    )
                else:
                    # Fall back to default credential chain (environment variables, IAM role, etc.)
                    #st.info("üí° Using default AWS credential chain (IAM role, environment variables, etc.)")
                    
                    # Pricing API is only available in us-east-1 and ap-south-1
                    self.pricing_client = boto3.client('pricing', region_name='us-east-1')
                    self.ec2_client = boto3.client('ec2', region_name=aws_region)
                
                # Try to determine credential source
                session = boto3.Session()
                credentials = session.get_credentials()
                
                if credentials and hasattr(credentials, 'method'):
                    if 'iam' in credentials.method.lower():
                        credential_source = "IAM Role"
                    elif 'env' in credentials.method.lower():
                        credential_source = "Environment Variables"
                    elif 'shared' in credentials.method.lower():
                        credential_source = "AWS Credentials File"
                    else:
                        credential_source = f"AWS Default Chain ({credentials.method})"
                else:
                    credential_source = "AWS Default Chain"
                
                
                
                    
            except KeyError as e:
                #st.warning(f"‚ö†Ô∏è AWS secrets configuration incomplete: {str(e)}")
                #st.info("üí° Add AWS credentials to .streamlit/secrets.toml")
                self.pricing_client = None
                self.ec2_client = None
                return
            
            # Test the connection
            try:
                # Quick test to verify credentials work
                response = self.pricing_client.describe_services(MaxResults=1)
                #st.success(f"‚úÖ AWS Pricing API connected via {credential_source}")
                
                            # Additional connection details
                if credential_source != "Streamlit Secrets":
                    # Get additional info for non-secrets connections
                    try:
                        sts_client = boto3.client('sts', region_name='us-east-1')
                        identity = sts_client.get_caller_identity()
                        account_id = identity.get('Account', 'Unknown')
                        st.info(f"üí° AWS Account: {account_id} | Region: {aws_region}")
                    except:
                        pass  # Don't fail if we can't get STS info
                           
                               
            except ClientError as e:
                error_code = e.response['Error']['Code']
                self.pricing_client = None
                self.ec2_client = None
                self.connection_status = {
                    'connected': False,
                    'error': error_code,
                    'source': 'Error'
            }
                
        except NoCredentialsError:
            
            self.pricing_client = None
            self.ec2_client = None
            self.connection_status = {
                'connected': False,
                'error': 'NoCredentials',
                'source': 'None'
            }

                except Exception as e:
            # REMOVED: Error message
            self.pricing_client = None
            self.ec2_client = None
            self.connection_status = {
                'connected': False,
                'error': str(e),
                'source': 'Error'
            }
            
            
    def _is_cache_valid(self, key):
        """Check if cached data is still valid"""
        if key not in self.cache or key not in self.last_cache_update:
            return False
        return (time.time() - self.last_cache_update[key]) < self.cache_ttl
    
    def _update_cache(self, key, value):
        """Update cache with new value"""
        self.cache[key] = value
        self.last_cache_update[key] = time.time()
    
    @lru_cache(maxsize=100)
    def get_ec2_pricing(self, instance_type, region=None):
        """Get real-time EC2 instance pricing"""
        if not self.pricing_client:
            return self._get_fallback_ec2_pricing(instance_type)
        
        cache_key = f"ec2_{instance_type}_{region or self.region}"
        
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]
        
        try:
            # Get pricing for On-Demand Linux instances
            response = self.pricing_client.get_products(
                ServiceCode='AmazonEC2',
                MaxResults=1,
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'instanceType', 'Value': instance_type},
                    {'Type': 'TERM_MATCH', 'Field': 'operatingSystem', 'Value': 'Linux'},
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': self._get_location_name(region or self.region)},
                    {'Type': 'TERM_MATCH', 'Field': 'tenancy', 'Value': 'Shared'},
                    {'Type': 'TERM_MATCH', 'Field': 'preInstalledSw', 'Value': 'NA'}
                ]
            )
            
            if response['PriceList']:
                price_data = json.loads(response['PriceList'][0])
                terms = price_data['terms']['OnDemand']
                
                # Extract the hourly price
                for term_key, term_value in terms.items():
                    for price_dimension_key, price_dimension in term_value['priceDimensions'].items():
                        if 'USD' in price_dimension['pricePerUnit']:
                            hourly_price = float(price_dimension['pricePerUnit']['USD'])
                            self._update_cache(cache_key, hourly_price)
                            return hourly_price
            
            # Fallback if no pricing found
            return self._get_fallback_ec2_pricing(instance_type)
            
        except Exception as e:
            st.warning(f"Error fetching EC2 pricing for {instance_type}: {str(e)}")
            return self._get_fallback_ec2_pricing(instance_type)
    
    def get_s3_pricing(self, storage_class, region=None):
        """Get real-time S3 storage pricing"""
        if not self.pricing_client:
            return self._get_fallback_s3_pricing(storage_class)
        
        cache_key = f"s3_{storage_class}_{region or self.region}"
        
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]
        
        try:
            # Map storage class names to AWS API values
            storage_class_mapping = {
                "Standard": "General Purpose",
                "Standard-IA": "Infrequent Access",
                "One Zone-IA": "One Zone - Infrequent Access",
                "Glacier Instant Retrieval": "Amazon Glacier Instant Retrieval",
                "Glacier Flexible Retrieval": "Amazon Glacier Flexible Retrieval",
                "Glacier Deep Archive": "Amazon Glacier Deep Archive"
            }
            
            aws_storage_class = storage_class_mapping.get(storage_class, "General Purpose")
            
            response = self.pricing_client.get_products(
                ServiceCode='AmazonS3',
                MaxResults=1,
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'storageClass', 'Value': aws_storage_class},
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': self._get_location_name(region or self.region)},
                    {'Type': 'TERM_MATCH', 'Field': 'volumeType', 'Value': 'Standard'}
                ]
            )
            
            if response['PriceList']:
                price_data = json.loads(response['PriceList'][0])
                terms = price_data['terms']['OnDemand']
                
                # Extract the price per GB
                for term_key, term_value in terms.items():
                    for price_dimension_key, price_dimension in term_value['priceDimensions'].items():
                        if 'USD' in price_dimension['pricePerUnit']:
                            gb_price = float(price_dimension['pricePerUnit']['USD'])
                            self._update_cache(cache_key, gb_price)
                            return gb_price
            
            return self._get_fallback_s3_pricing(storage_class)
            
        except Exception as e:
            st.warning(f"Error fetching S3 pricing for {storage_class}: {str(e)}")
            return self._get_fallback_s3_pricing(storage_class)
    
    def get_data_transfer_pricing(self, region=None):
        """Get real-time data transfer pricing"""
        if not self.pricing_client:
            return 0.09  # Fallback rate
        
        cache_key = f"transfer_{region or self.region}"
        
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]
        
        try:
            response = self.pricing_client.get_products(
                ServiceCode='AmazonEC2',
                MaxResults=1,
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'transferType', 'Value': 'AWS Outbound'},
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': self._get_location_name(region or self.region)}
                ]
            )
            
            if response['PriceList']:
                # Parse the first pricing tier (usually 0-1GB or 1-10TB)
                price_data = json.loads(response['PriceList'][0])
                terms = price_data['terms']['OnDemand']
                
                for term_key, term_value in terms.items():
                    for price_dimension_key, price_dimension in term_value['priceDimensions'].items():
                        if 'USD' in price_dimension['pricePerUnit']:
                            transfer_price = float(price_dimension['pricePerUnit']['USD'])
                            self._update_cache(cache_key, transfer_price)
                            return transfer_price
            
            return 0.09  # Fallback
            
        except Exception as e:
            st.warning(f"Error fetching data transfer pricing: {str(e)}")
            return 0.09
    
    def get_direct_connect_pricing(self, bandwidth_mbps, region=None):
        """Get Direct Connect pricing based on bandwidth"""
        if not self.pricing_client:
            return self._get_fallback_dx_pricing(bandwidth_mbps)
        
        cache_key = f"dx_{bandwidth_mbps}_{region or self.region}"
        
        if self._is_cache_valid(cache_key):
            return self.cache[cache_key]
        
        try:
            # Map bandwidth to AWS DX port speeds
            if bandwidth_mbps >= 10000:
                port_speed = "10Gbps"
            elif bandwidth_mbps >= 1000:
                port_speed = "1Gbps"
            else:
                port_speed = "100Mbps"
            
            response = self.pricing_client.get_products(
                ServiceCode='AWSDirectConnect',
                MaxResults=1,
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'portSpeed', 'Value': port_speed},
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': self._get_location_name(region or self.region)}
                ]
            )
            
            if response['PriceList']:
                price_data = json.loads(response['PriceList'][0])
                terms = price_data['terms']['OnDemand']
                
                for term_key, term_value in terms.items():
                    for price_dimension_key, price_dimension in term_value['priceDimensions'].items():
                        if 'USD' in price_dimension['pricePerUnit']:
                            monthly_price = float(price_dimension['pricePerUnit']['USD'])
                            hourly_price = monthly_price / (24 * 30)  # Convert to hourly
                            self._update_cache(cache_key, hourly_price)
                            return hourly_price
            
            return self._get_fallback_dx_pricing(bandwidth_mbps)
            
        except Exception as e:
            st.warning(f"Error fetching Direct Connect pricing: {str(e)}")
            return self._get_fallback_dx_pricing(bandwidth_mbps)
    
    def _get_location_name(self, region):
        """Map AWS region codes to location names used in Pricing API"""
        location_mapping = {
            'us-east-1': 'US East (N. Virginia)',
            'us-east-2': 'US East (Ohio)',
            'us-west-1': 'US West (N. California)',
            'us-west-2': 'US West (Oregon)',
            'eu-west-1': 'Europe (Ireland)',
            'eu-central-1': 'Europe (Frankfurt)',
            'ap-southeast-1': 'Asia Pacific (Singapore)',
            'ap-northeast-1': 'Asia Pacific (Tokyo)',
            'ap-south-1': 'Asia Pacific (Mumbai)',
            'sa-east-1': 'South America (Sao Paulo)'
        }
        return location_mapping.get(region, 'US East (N. Virginia)')
    
    def _get_fallback_ec2_pricing(self, instance_type):
        """Fallback EC2 pricing when API is unavailable"""
        fallback_prices = {
            "m5.large": 0.096,
            "m5.xlarge": 0.192,
            "m5.2xlarge": 0.384,
            "m5.4xlarge": 0.768,
            "m5.8xlarge": 1.536,
            "c5.2xlarge": 0.34,
            "c5.4xlarge": 0.68,
            "c5.9xlarge": 1.53,
            "r5.2xlarge": 0.504,
            "r5.4xlarge": 1.008
        }
        return fallback_prices.get(instance_type, 0.10)
    
    def _get_fallback_s3_pricing(self, storage_class):
        """Fallback S3 pricing when API is unavailable"""
        fallback_prices = {
            "Standard": 0.023,
            "Standard-IA": 0.0125,
            "One Zone-IA": 0.01,
            "Glacier Instant Retrieval": 0.004,
            "Glacier Flexible Retrieval": 0.0036,
            "Glacier Deep Archive": 0.00099
        }
        return fallback_prices.get(storage_class, 0.023)
    
    def _get_fallback_dx_pricing(self, bandwidth_mbps):
        """Fallback Direct Connect pricing when API is unavailable"""
        if bandwidth_mbps >= 10000:
            return 1.55  # 10Gbps port
        elif bandwidth_mbps >= 1000:
            return 0.30  # 1Gbps port
        else:
            return 0.03  # 100Mbps port
    
    def get_comprehensive_pricing(self, instance_type, storage_class, region=None, bandwidth_mbps=1000):
        """Get all pricing information in parallel for better performance"""
        try:
            with ThreadPoolExecutor(max_workers=4) as executor:
                # Submit all pricing requests concurrently
                futures = {
                    'ec2': executor.submit(self.get_ec2_pricing, instance_type, region),
                    's3': executor.submit(self.get_s3_pricing, storage_class, region),
                    'transfer': executor.submit(self.get_data_transfer_pricing, region),
                    'dx': executor.submit(self.get_direct_connect_pricing, bandwidth_mbps, region)
                }
                
                # Collect results
                pricing = {}
                for key, future in futures.items():
                    try:
                        pricing[key] = future.result(timeout=10)  # 10-second timeout
                    except Exception as e:
                        st.warning(f"Timeout fetching {key} pricing: {str(e)}")
                        # Use fallback values
                        if key == 'ec2':
                            pricing[key] = self._get_fallback_ec2_pricing(instance_type)
                        elif key == 's3':
                            pricing[key] = self._get_fallback_s3_pricing(storage_class)
                        elif key == 'transfer':
                            pricing[key] = 0.09
                        elif key == 'dx':
                            pricing[key] = self._get_fallback_dx_pricing(bandwidth_mbps)
                
                return pricing
                
        except Exception as e:
            st.error(f"Error in comprehensive pricing fetch: {str(e)}")
            return {
                'ec2': self._get_fallback_ec2_pricing(instance_type),
                's3': self._get_fallback_s3_pricing(storage_class),
                'transfer': 0.09,
                'dx': self._get_fallback_dx_pricing(bandwidth_mbps)
            }

class EnterpriseCalculator:
    """Enterprise-grade calculator for AWS migration planning"""
    
    def __init__(self):
        """Initialize the calculator with all required data structures"""
        # Ensure instance_performance is the first thing we initialize
        self.instance_performance = {
            "m5.large": {"cpu": 2, "memory": 8, "network": 750, "baseline_throughput": 150, "cost_hour": 0.096},
            "m5.xlarge": {"cpu": 4, "memory": 16, "network": 750, "baseline_throughput": 250, "cost_hour": 0.192},
            "m5.2xlarge": {"cpu": 8, "memory": 32, "network": 1000, "baseline_throughput": 400, "cost_hour": 0.384},
            "m5.4xlarge": {"cpu": 16, "memory": 64, "network": 2000, "baseline_throughput": 600, "cost_hour": 0.768},
            "m5.8xlarge": {"cpu": 32, "memory": 128, "network": 4000, "baseline_throughput": 1000, "cost_hour": 1.536},
            "c5.2xlarge": {"cpu": 8, "memory": 16, "network": 2000, "baseline_throughput": 500, "cost_hour": 0.34},
            "c5.4xlarge": {"cpu": 16, "memory": 32, "network": 4000, "baseline_throughput": 800, "cost_hour": 0.68},
            "c5.9xlarge": {"cpu": 36, "memory": 72, "network": 10000, "baseline_throughput": 1500, "cost_hour": 1.53},
            "r5.2xlarge": {"cpu": 8, "memory": 64, "network": 2000, "baseline_throughput": 450, "cost_hour": 0.504},
            "r5.4xlarge": {"cpu": 16, "memory": 128, "network": 4000, "baseline_throughput": 700, "cost_hour": 1.008}
        }
        
        self.file_size_multipliers = {
            "< 1MB (Many small files)": 0.25,
            "1-10MB (Small files)": 0.45,
            "10-100MB (Medium files)": 0.70,
            "100MB-1GB (Large files)": 0.90,
            "> 1GB (Very large files)": 0.95
        }
                    
        self.compliance_requirements = {
            "SOX": {"encryption_required": True, "audit_trail": True, "data_retention": 7},
            "GDPR": {"encryption_required": True, "data_residency": True, "right_to_delete": True},
            "HIPAA": {"encryption_required": True, "access_logging": True, "data_residency": True},
            "PCI-DSS": {"encryption_required": True, "network_segmentation": True, "access_control": True},
            "SOC2": {"encryption_required": True, "monitoring": True, "access_control": True},
            "ISO27001": {"risk_assessment": True, "documentation": True, "continuous_monitoring": True},
            "FedRAMP": {"encryption_required": True, "continuous_monitoring": True, "incident_response": True},
            "FISMA": {"encryption_required": True, "access_control": True, "audit_trail": True}
        }
        
        # Geographic latency matrix (ms)
        self.geographic_latency = {
            "San Jose, CA": {"us-west-1": 15, "us-west-2": 25, "us-east-1": 70, "us-east-2": 65},
            "San Antonio, TX": {"us-west-1": 45, "us-west-2": 50, "us-east-1": 35, "us-east-2": 30},
            "New York, NY": {"us-west-1": 75, "us-west-2": 80, "us-east-1": 10, "us-east-2": 15},
            "Chicago, IL": {"us-west-1": 60, "us-west-2": 65, "us-east-1": 25, "us-east-2": 20},
            "Dallas, TX": {"us-west-1": 40, "us-west-2": 45, "us-east-1": 35, "us-east-2": 30},
            "Los Angeles, CA": {"us-west-1": 20, "us-west-2": 15, "us-east-1": 75, "us-east-2": 70},
            "Atlanta, GA": {"us-west-1": 65, "us-west-2": 70, "us-east-1": 15, "us-east-2": 20},
            "London, UK": {"us-west-1": 150, "us-west-2": 155, "us-east-1": 80, "us-east-2": 85},
            "Frankfurt, DE": {"us-west-1": 160, "us-west-2": 165, "us-east-1": 90, "us-east-2": 95},
            "Tokyo, JP": {"us-west-1": 120, "us-west-2": 115, "us-east-1": 180, "us-east-2": 185},
            "Sydney, AU": {"us-west-1": 170, "us-west-2": 165, "us-east-1": 220, "us-east-2": 225}
        }
        
        # Database migration tools
        self.db_migration_tools = {
            "DMS": {
                "name": "Database Migration Service",
                "best_for": ["Homogeneous", "Heterogeneous", "Continuous Replication"],
                "data_size_limit": "Large (TB scale)",
                "downtime": "Minimal",
                "cost_factor": 1.0,
                "complexity": "Medium"
            },
            "DataSync": {
                "name": "AWS DataSync",
                "best_for": ["File Systems", "Object Storage", "Large Files"],
                "data_size_limit": "Very Large (PB scale)",
                "downtime": "None",
                "cost_factor": 0.8,
                "complexity": "Low"
            },
            "DMS+DataSync": {
                "name": "Hybrid DMS + DataSync",
                "best_for": ["Complex Workloads", "Mixed Data Types"],
                "data_size_limit": "Very Large",
                "downtime": "Low",
                "cost_factor": 1.3,
                "complexity": "High"
            },
            "Parallel Copy": {
                "name": "AWS Parallel Copy",
                "best_for": ["Time-Critical", "High Throughput"],
                "data_size_limit": "Large",
                "downtime": "Low",
                "cost_factor": 1.5,
                "complexity": "Medium"
            },
            "Snowball Edge": {
                "name": "AWS Snowball Edge",
                "best_for": ["Limited Bandwidth", "Large Datasets"],
                "data_size_limit": "Very Large (100TB per device)",
                "downtime": "Medium",
                "cost_factor": 0.6,
                "complexity": "Low"
            },
            "Storage Gateway": {
                "name": "AWS Storage Gateway",
                "best_for": ["Hybrid Cloud", "Gradual Migration"],
                "data_size_limit": "Large",
                "downtime": "None",
                "cost_factor": 1.2,
                "complexity": "Medium"
            }
        }
        
        # Initialize pricing manager
        self.pricing_manager = None
        self._init_pricing_manager()
    
    def _init_pricing_manager(self):
        """Initialize pricing manager with Streamlit secrets"""
        try:
            # Get region from secrets if available
            region = 'us-east-1'
            if hasattr(st, 'secrets') and 'aws' in st.secrets:
                region = st.secrets["aws"].get("region", "us-east-1")
            
            self.pricing_manager = AWSPricingManager(region=region)
            
        except Exception as e:
            st.warning(f"Could not initialize pricing manager: {str(e)}")
            self.pricing_manager = None
    
    def verify_initialization(self):
        """Verify that all required attributes are properly initialized"""
        required_attributes = [
            'instance_performance',
            'file_size_multipliers', 
            'compliance_requirements',
            'geographic_latency',
            'db_migration_tools'
        ]
        
        missing_attributes = []
        for attr in required_attributes:
            if not hasattr(self, attr):
                missing_attributes.append(attr)
        
        if missing_attributes:
            raise AttributeError(f"Missing required attributes: {missing_attributes}")
        
        # Verify instance_performance has expected keys
        if not self.instance_performance or not isinstance(self.instance_performance, dict):
            raise ValueError("instance_performance is not properly initialized")
        
        return True
    
    def calculate_enterprise_throughput(self, instance_type, num_agents, avg_file_size, 
                                      dx_bandwidth_mbps, network_latency, network_jitter, 
                                      packet_loss, qos_enabled, dedicated_bandwidth, 
                                      real_world_mode=True):
        """Calculate enterprise throughput with real-world factors"""
        
        # Get instance performance
        instance_specs = self.instance_performance.get(instance_type, self.instance_performance["m5.large"])
        baseline_throughput = instance_specs["baseline_throughput"]
        
        # Calculate agent scaling factor
        agent_scaling = min(num_agents * 0.85, num_agents)  # Diminishing returns
        
        # File size efficiency
        file_size_efficiency = self.file_size_multipliers.get(avg_file_size, 0.7)
        
        # Network factors
        latency_factor = max(0.3, 1 - (network_latency - 10) / 200)
        jitter_factor = max(0.5, 1 - network_jitter / 50)
        packet_loss_factor = max(0.2, 1 - packet_loss / 5)
        qos_factor = 1.15 if qos_enabled else 1.0
        
        # Bandwidth limitation
        available_bandwidth = dx_bandwidth_mbps * (dedicated_bandwidth / 100)
        
        # Calculate theoretical throughput
        theoretical_throughput = min(
            baseline_throughput * agent_scaling * file_size_efficiency,
            available_bandwidth
        )
        
        # Apply network degradation
        network_efficiency = latency_factor * jitter_factor * packet_loss_factor * qos_factor
        
        if real_world_mode:
            # Real-world factors
            storage_io_factor = 0.8  # Storage I/O limitations
            datasync_overhead = 0.85  # DataSync processing overhead
            tcp_efficiency = 0.75  # TCP window and congestion control
            aws_api_limits = 0.9  # AWS API throttling
            
            real_world_efficiency = storage_io_factor * datasync_overhead * tcp_efficiency * aws_api_limits
            
            datasync_throughput = theoretical_throughput * network_efficiency * real_world_efficiency
            
            return datasync_throughput, network_efficiency, theoretical_throughput, real_world_efficiency
        else:
            datasync_throughput = theoretical_throughput * network_efficiency
            return datasync_throughput, network_efficiency, theoretical_throughput, 1.0
    
    def calculate_enterprise_costs(self, data_size_gb, transfer_days, instance_type, num_agents,
                                 compliance_frameworks, s3_storage_class, region=None, dx_bandwidth_mbps=1000):
        """Calculate comprehensive migration costs using real-time AWS pricing"""
        
        # Initialize pricing manager if not already done
        if not hasattr(self, 'pricing_manager') or self.pricing_manager is None:
            self.pricing_manager = AWSPricingManager(region=region or 'us-east-1')
        
        # Get real-time pricing for all components
        with st.spinner("üîÑ Fetching real-time AWS pricing..."):
            pricing = self.pricing_manager.get_comprehensive_pricing(
                instance_type=instance_type,
                storage_class=s3_storage_class,
                region=region,
                bandwidth_mbps=dx_bandwidth_mbps
            )
        
        # Calculate costs using real-time pricing
        
        # 1. DataSync compute costs (EC2 instances)
        instance_cost_hour = pricing['ec2']
        datasync_compute_cost = instance_cost_hour * num_agents * 24 * transfer_days
        
        # 2. Data transfer costs
        transfer_rate_per_gb = pricing['transfer']
        data_transfer_cost = data_size_gb * transfer_rate_per_gb
        
        # 3. S3 storage costs
        s3_rate_per_gb = pricing['s3']
        s3_storage_cost = data_size_gb * s3_rate_per_gb
        
        # 4. Direct Connect costs (if applicable)
        dx_hourly_cost = pricing['dx']
        dx_cost = dx_hourly_cost * 24 * transfer_days
        
        # 5. Additional enterprise costs (compliance, monitoring, etc.)
        compliance_cost = len(compliance_frameworks) * 500  # Compliance tooling per framework
        monitoring_cost = 200 * transfer_days  # Enhanced monitoring per day
        
        # 6. AWS service costs (DataSync service fees)
        datasync_service_cost = data_size_gb * 0.0125  # $0.0125 per GB processed
        
        # 7. CloudWatch and logging costs
        cloudwatch_cost = num_agents * 50 * transfer_days  # Monitoring per agent per day
        
        # Calculate total cost
        total_cost = (datasync_compute_cost + data_transfer_cost + s3_storage_cost + 
                    dx_cost + compliance_cost + monitoring_cost + datasync_service_cost + 
                    cloudwatch_cost)
        
        return {
            "compute": datasync_compute_cost,
            "transfer": data_transfer_cost,
            "storage": s3_storage_cost,
            "direct_connect": dx_cost,
            "datasync_service": datasync_service_cost,
            "compliance": compliance_cost,
            "monitoring": monitoring_cost,
            "cloudwatch": cloudwatch_cost,
            "total": total_cost,
            "pricing_source": "AWS API" if self.pricing_manager and self.pricing_manager.pricing_client else "Fallback",
            "last_updated": time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime()),
            "cost_breakdown_detailed": {
                "instance_hourly_rate": instance_cost_hour,
                "transfer_rate_per_gb": transfer_rate_per_gb,
                "s3_rate_per_gb": s3_rate_per_gb,
                "dx_hourly_rate": dx_hourly_cost
            }
        }
    
    def assess_compliance_requirements(self, frameworks, data_classification, data_residency):
        """Assess compliance requirements"""
        requirements = []
        risks = []
        
        for framework in frameworks:
            if framework in self.compliance_requirements:
                framework_reqs = self.compliance_requirements[framework]
                for req, value in framework_reqs.items():
                    if value:
                        requirements.append(f"{framework}: {req.replace('_', ' ').title()}")
        
        # Add data classification requirements
        if data_classification in ["Restricted", "Top Secret"]:
            risks.append("High data classification requires enhanced security controls")
        
        return requirements, risks
    
    def calculate_business_impact(self, transfer_days, data_types):
        """Calculate business impact"""
        
        # Calculate impact score based on data types and duration
        critical_data_types = ["Customer Data", "Financial Records", "Intellectual Property"]
        critical_count = sum(1 for dt in data_types if dt in critical_data_types)
        
        base_score = 0.5  # Base impact
        critical_multiplier = critical_count * 0.1
        duration_multiplier = min(0.3, transfer_days / 30 * 0.1)
        
        impact_score = base_score + critical_multiplier + duration_multiplier
        
        if impact_score >= 0.8:
            level = "Critical"
        elif impact_score >= 0.6:
            level = "High"
        elif impact_score >= 0.4:
            level = "Medium"
        else:
            level = "Low"
        
        recommendation = f"Business impact is {level.lower()} - implement appropriate change management"
        
        return {
            "score": impact_score,
            "level": level,
            "recommendation": recommendation
        }
    
    def get_optimal_networking_architecture(self, source_location, target_region, data_size_gb,
                                        dx_bandwidth_mbps, database_types, data_types, config):
        """Get AI-powered networking recommendations"""
        
        # Analyze requirements
        data_size_tb = data_size_gb / 1024
        has_databases = len(database_types) > 0
        has_critical_data = any(dt in ["Customer Data", "Financial Records"] for dt in data_types)
        analyze_all_methods = config.get('analyze_all_methods', False)
        
        # Determine primary method based on analysis scope
        if analyze_all_methods:
            # Comprehensive analysis - consider hybrid approaches
            if data_size_tb > 100:
                if dx_bandwidth_mbps >= 10000:
                    primary_method = "DataSync Multi-Agent"
                else:
                    primary_method = "Snowball Edge"
            elif has_databases:
                primary_method = "DMS + DataSync"  # Hybrid approach for comprehensive analysis
            else:
                primary_method = "DataSync"
        else:
            # Simple analysis - stick to core DataSync recommendations
            if data_size_tb > 100 and dx_bandwidth_mbps < 1000:
                primary_method = "Snowball Edge"  # Only for very large data + limited bandwidth
            else:
                primary_method = "AWS DataSync"  # Default to DataSync for simple analysis
        
        # Determine secondary method
        if has_critical_data:
            secondary_method = "S3 Transfer Acceleration"
        else:
            secondary_method = "Standard Transfer"
        
        # Determine networking option
        if dx_bandwidth_mbps >= 1000:
            networking_option = "Direct Connect (Primary)"
        elif config.get('qos_enabled', False):
            networking_option = "Direct Connect + VPN Backup"
        else:
            networking_option = "VPN Connection"
        
        # Database migration tool - only suggest complex tools in comprehensive mode
        if has_databases and analyze_all_methods:
            if len(database_types) > 1:
                db_migration_tool = "DMS + Custom Scripts"
            else:
                db_migration_tool = "DMS"
        elif has_databases:
            db_migration_tool = "DMS"  # Simple DMS recommendation
        else:
            db_migration_tool = "N/A"
        
        # Rest of the method remains the same...
        # [Keep all the existing calculation code for performance, rationale, etc.]
            
            # Calculate estimated performance
            estimated_throughput = min(dx_bandwidth_mbps * 0.8, 2000)  # Conservative estimate
            estimated_days = (data_size_gb * 8) / (estimated_throughput * 24 * 3600) / 1000
            network_efficiency = 0.75 if dx_bandwidth_mbps >= 1000 else 0.6
            
            # Risk assessment
            if data_size_tb > 50:
                risk_level = "High"
            elif dx_bandwidth_mbps < 1000:
                risk_level = "Medium"
            else:
                risk_level = "Low"
            
            # Cost efficiency
            cost_per_tb = 1000 if dx_bandwidth_mbps >= 1000 else 1500
            if cost_per_tb < 1200:
                cost_efficiency = "High"
            elif cost_per_tb < 1400:
                cost_efficiency = "Medium"
            else:
                cost_efficiency = "Low"
            
            # Generate rationale
            rationale = f"""
            Based on {data_size_tb:.1f}TB dataset with {dx_bandwidth_mbps} Mbps bandwidth:
            ‚Ä¢ {primary_method} recommended for optimal throughput and reliability
            ‚Ä¢ {networking_option} provides best balance of performance and cost
            ‚Ä¢ Estimated {estimated_days:.1f} days with {estimated_throughput:.0f} Mbps sustained throughput
            ‚Ä¢ {risk_level} risk profile managed through redundancy and monitoring
            """
            
            return {
                "primary_method": primary_method,
                "secondary_method": secondary_method,
                "networking_option": networking_option,
                "db_migration_tool": db_migration_tool,
                "rationale": rationale.strip(),
                "estimated_performance": {
                    "throughput_mbps": estimated_throughput,
                    "estimated_days": estimated_days,
                    "network_efficiency": network_efficiency
                },
                "cost_efficiency": cost_efficiency,
                "risk_level": risk_level
            }
        
    def get_intelligent_datasync_recommendations(self, config, metrics):
        """Get intelligent DataSync optimization recommendations"""
        try:
            # Calculate efficiency metrics directly
            current_instance = config['datasync_instance_type']
            current_agents = config['num_datasync_agents']
            data_size_tb = metrics['data_size_tb']
            
            # Calculate current efficiency
            max_theoretical = config['dx_bandwidth_mbps'] * 0.8
            current_efficiency = (metrics['optimized_throughput'] / max_theoretical) * 100 if max_theoretical > 0 else 70
            
            # Performance rating
            if current_efficiency >= 80:
                performance_rating = "Excellent"
            elif current_efficiency >= 60:
                performance_rating = "Good" 
            else:
                performance_rating = "Needs Improvement"
            
            # Agent optimization analysis
            optimal_agents = max(1, min(10, int(data_size_tb / 10) + 1))
            
            # Instance optimization analysis
            if data_size_tb > 50 and current_instance == "m5.large":
                recommended_instance = "m5.2xlarge"
                upgrade_needed = True
                upgrade_reason = f"Large dataset ({data_size_tb:.1f}TB) benefits from more CPU/memory"
                expected_gain = 25
            elif data_size_tb > 100 and "m5.large" in current_instance:
                recommended_instance = "c5.4xlarge"
                upgrade_needed = True
                upgrade_reason = f"Very large dataset ({data_size_tb:.1f}TB) benefits from compute-optimized instances"
                expected_gain = 40
            else:
                recommended_instance = current_instance
                upgrade_needed = False
                upgrade_reason = "Current instance type is appropriate"
                expected_gain = 0
            
            return {
                "current_analysis": {
                    "current_efficiency": current_efficiency,
                    "performance_rating": performance_rating,
                    "scaling_effectiveness": {
                        "scaling_rating": "Optimal" if current_agents <= 5 else "Over-scaled",
                        "efficiency": 0.85
                    }
                },
                "recommended_instance": {
                    "recommended_instance": recommended_instance,
                    "upgrade_needed": upgrade_needed,
                    "reason": upgrade_reason,
                    "expected_performance_gain": expected_gain,
                    "cost_impact_percent": 100 if upgrade_needed else 0
                },
                "recommended_agents": {
                    "recommended_agents": optimal_agents,
                    "change_needed": optimal_agents - current_agents,
                    "reasoning": f"Optimal agent count for {data_size_tb:.1f}TB dataset",
                    "performance_change_percent": (optimal_agents - current_agents) * 15,
                    "cost_change_percent": (optimal_agents - current_agents) * 100
                },
                "bottleneck_analysis": ([], ["Configuration appears optimized"]),
                "cost_performance_analysis": {
                    "current_cost_efficiency": 0.1,
                    "efficiency_ranking": 5
                },
                "alternative_configurations": []
            }
        except Exception as e:
            return {
                "current_analysis": {"current_efficiency": 70, "performance_rating": "Good", "scaling_effectiveness": {"scaling_rating": "Good", "efficiency": 0.7}},
                "recommended_instance": {"recommended_instance": config.get('datasync_instance_type', 'm5.large'), "upgrade_needed": False, "reason": "Current setup", "expected_performance_gain": 0, "cost_impact_percent": 0},
                "recommended_agents": {"recommended_agents": config.get('num_datasync_agents', 1), "change_needed": 0, "reasoning": "Current setup", "performance_change_percent": 0, "cost_change_percent": 0},
                "bottleneck_analysis": ([], ["Analysis unavailable"]),
                "cost_performance_analysis": {"current_cost_efficiency": 0.1, "efficiency_ranking": 5},
                "alternative_configurations": []
            }

# =============================================================================
# DATABASE MIGRATION CLASSES 
# =============================================================================

class DatabaseSizingEngine:
    """AI-powered database sizing and configuration engine"""
    
    def __init__(self):
        self.instance_specs = {
            # RDS Instance Types
            "db.t3.micro": {"vcpu": 2, "memory": 1, "network": "Low", "cost_factor": 1.0, "use_case": "Dev/Test"},
            "db.t3.small": {"vcpu": 2, "memory": 2, "network": "Low to Moderate", "cost_factor": 1.5, "use_case": "Dev/Test"},
            "db.t3.medium": {"vcpu": 2, "memory": 4, "network": "Low to Moderate", "cost_factor": 2.2, "use_case": "Small Production"},
            "db.t3.large": {"vcpu": 2, "memory": 8, "network": "Low to Moderate", "cost_factor": 3.8, "use_case": "Small Production"},
            "db.m5.large": {"vcpu": 2, "memory": 8, "network": "Up to 10 Gbps", "cost_factor": 4.2, "use_case": "General Purpose"},
            "db.m5.xlarge": {"vcpu": 4, "memory": 16, "network": "Up to 10 Gbps", "cost_factor": 8.4, "use_case": "General Purpose"},
            "db.m5.2xlarge": {"vcpu": 8, "memory": 32, "network": "Up to 10 Gbps", "cost_factor": 16.8, "use_case": "General Purpose"},
            "db.m5.4xlarge": {"vcpu": 16, "memory": 64, "network": "Up to 10 Gbps", "cost_factor": 33.6, "use_case": "High Performance"},
            "db.m5.8xlarge": {"vcpu": 32, "memory": 128, "network": "10 Gbps", "cost_factor": 67.2, "use_case": "High Performance"},
            "db.m5.12xlarge": {"vcpu": 48, "memory": 192, "network": "12 Gbps", "cost_factor": 100.8, "use_case": "High Performance"},
            "db.m5.16xlarge": {"vcpu": 64, "memory": 256, "network": "20 Gbps", "cost_factor": 134.4, "use_case": "High Performance"},
            "db.m5.24xlarge": {"vcpu": 96, "memory": 384, "network": "25 Gbps", "cost_factor": 201.6, "use_case": "High Performance"},
            "db.r5.large": {"vcpu": 2, "memory": 16, "network": "Up to 10 Gbps", "cost_factor": 5.5, "use_case": "Memory Optimized"},
            "db.r5.xlarge": {"vcpu": 4, "memory": 32, "network": "Up to 10 Gbps", "cost_factor": 11.0, "use_case": "Memory Optimized"},
            "db.r5.2xlarge": {"vcpu": 8, "memory": 64, "network": "Up to 10 Gbps", "cost_factor": 22.0, "use_case": "Memory Optimized"},
            "db.r5.4xlarge": {"vcpu": 16, "memory": 128, "network": "Up to 10 Gbps", "cost_factor": 44.0, "use_case": "Memory Optimized"},
            "db.r5.8xlarge": {"vcpu": 32, "memory": 256, "network": "10 Gbps", "cost_factor": 88.0, "use_case": "Memory Optimized"},
            "db.r5.12xlarge": {"vcpu": 48, "memory": 384, "network": "12 Gbps", "cost_factor": 132.0, "use_case": "Memory Optimized"},
            "db.r5.16xlarge": {"vcpu": 64, "memory": 512, "network": "20 Gbps", "cost_factor": 176.0, "use_case": "Memory Optimized"},
            "db.r5.24xlarge": {"vcpu": 96, "memory": 768, "network": "25 Gbps", "cost_factor": 264.0, "use_case": "Memory Optimized"},
        }
        
        self.workload_patterns = {
            "OLTP": {"cpu_weight": 0.3, "memory_weight": 0.4, "io_weight": 0.3, "read_write_ratio": 0.7},
            "OLAP": {"cpu_weight": 0.4, "memory_weight": 0.3, "io_weight": 0.3, "read_write_ratio": 0.9},
            "Mixed": {"cpu_weight": 0.35, "memory_weight": 0.35, "io_weight": 0.3, "read_write_ratio": 0.8},
            "Analytics": {"cpu_weight": 0.5, "memory_weight": 0.3, "io_weight": 0.2, "read_write_ratio": 0.95},
            "Reporting": {"cpu_weight": 0.25, "memory_weight": 0.45, "io_weight": 0.3, "read_write_ratio": 0.85}
        }
        
        self.environment_factors = {
            "Development": {"scaling_factor": 0.3, "availability": "Single-AZ", "backup_retention": 7},
            "QA": {"scaling_factor": 0.5, "availability": "Single-AZ", "backup_retention": 7},
            "SQA": {"scaling_factor": 0.7, "availability": "Multi-AZ", "backup_retention": 14},
            "Production": {"scaling_factor": 1.0, "availability": "Multi-AZ", "backup_retention": 30}
        }
    
    def calculate_sizing_requirements(self, config, vrops_data=None, use_ai=True):
        """Calculate optimal database sizing based on workload and requirements"""
        
        # Get base requirements
        workload_type = config.get("workload_type", "Mixed")
        environment = config.get("environment", "Production")
        database_size_gb = config.get("database_size_gb", 100)
        concurrent_connections = config.get("concurrent_connections", 100)
        transactions_per_second = config.get("transactions_per_second", 1000)
        growth_rate = config.get("annual_growth_rate", 0.2)
        
        # Use algorithmic calculation
        cpu_requirement, memory_requirement, iops_requirement = self._calculate_algorithmic_sizing(config)
        
        # Apply environment factors
        env_factor = self.environment_factors[environment]["scaling_factor"]
        cpu_requirement *= env_factor
        memory_requirement *= env_factor
        iops_requirement *= env_factor
        
        # Find optimal instance types
        writer_instance = self._find_optimal_instance(cpu_requirement, memory_requirement, "writer")
        reader_instances = self._calculate_reader_requirements(config, writer_instance)
        
        # Calculate storage requirements
        storage_config = self._calculate_storage_requirements(database_size_gb, iops_requirement, environment)
        
        # Growth projections
        growth_projections = self._calculate_growth_projections(
            writer_instance, reader_instances, storage_config, growth_rate
        )
        
        return {
            "writer_instance": writer_instance,
            "reader_instances": reader_instances,
            "storage_config": storage_config,
            "sizing_rationale": self._generate_sizing_rationale(config, cpu_requirement, memory_requirement),
            "growth_projections": growth_projections,
            "environment_config": self.environment_factors[environment]
        }
    
    def _calculate_algorithmic_sizing(self, config):
        """Fallback algorithmic sizing when vROps data is not available"""
        
        workload_type = config.get("workload_type", "Mixed")
        database_size_gb = config.get("database_size_gb", 100)
        concurrent_connections = config.get("concurrent_connections", 100)
        transactions_per_second = config.get("transactions_per_second", 1000)
        
        workload_pattern = self.workload_patterns[workload_type]
        
        # CPU calculation based on TPS and connections
        base_cpu = (transactions_per_second / 1000) * 2  # 2 vCPU per 1000 TPS baseline
        connection_cpu = (concurrent_connections / 100) * 1  # 1 vCPU per 100 connections
        cpu_requirement = max(2, base_cpu + connection_cpu) * workload_pattern["cpu_weight"] * 2
        
        # Memory calculation based on database size and connections
        base_memory = max(4, database_size_gb * 0.1)  # 10% of DB size as baseline
        connection_memory = (concurrent_connections * 10) / 1024  # 10MB per connection
        memory_requirement = (base_memory + connection_memory) * workload_pattern["memory_weight"] * 2
        
        # IOPS calculation based on TPS and workload pattern
        base_iops = transactions_per_second * 5  # 5 IOPS per transaction baseline
        iops_requirement = base_iops * workload_pattern["io_weight"] * 2
        
        return cpu_requirement, memory_requirement, iops_requirement
    
    def _find_optimal_instance(self, cpu_req, memory_req, instance_role):
        """Find the optimal instance type based on requirements"""
        
        best_instance = None
        best_score = float('inf')
        
        for instance_type, specs in self.instance_specs.items():
            if specs["vcpu"] >= cpu_req and specs["memory"] >= memory_req:
                # Calculate efficiency score (lower is better)
                cpu_efficiency = specs["vcpu"] / cpu_req
                memory_efficiency = specs["memory"] / memory_req
                cost_efficiency = specs["cost_factor"]
                
                # Combined efficiency score
                score = (cpu_efficiency + memory_efficiency) * cost_efficiency
                
                if score < best_score:
                    best_score = score
                    best_instance = {
                        "instance_type": instance_type,
                        "specs": specs,
                        "efficiency_score": score,
                        "cpu_utilization": (cpu_req / specs["vcpu"]) * 100,
                        "memory_utilization": (memory_req / specs["memory"]) * 100
                    }
        
        return best_instance or {
            "instance_type": "db.m5.large",
            "specs": self.instance_specs["db.m5.large"],
            "efficiency_score": 1.0,
            "cpu_utilization": 50,
            "memory_utilization": 50
        }
    
    def _calculate_reader_requirements(self, config, writer_instance):
        """Calculate optimal number and type of read replicas"""
        
        workload_type = config.get("workload_type", "Mixed")
        read_query_percentage = config.get("read_query_percentage", 70)
        high_availability_required = config.get("high_availability", True)
        
        workload_pattern = self.workload_patterns[workload_type]
        read_write_ratio = workload_pattern["read_write_ratio"]
        
        # Calculate number of readers needed
        if read_query_percentage >= 80:
            num_readers = 3 if high_availability_required else 2
        elif read_query_percentage >= 60:
            num_readers = 2 if high_availability_required else 1
        else:
            num_readers = 1 if high_availability_required else 0
        
        # Reader instances can be smaller than writer for read-heavy workloads
        reader_instance_type = writer_instance["instance_type"]
        if read_query_percentage >= 80 and workload_type in ["OLAP", "Analytics", "Reporting"]:
            # Use same size for read-heavy analytical workloads
            reader_specs = writer_instance["specs"]
        else:
            # Use smaller instances for general read replicas
            reader_specs = self._get_smaller_instance(writer_instance["instance_type"])
            reader_instance_type = reader_specs["instance_type"]
        
        readers = []
        for i in range(num_readers):
            readers.append({
                "instance_type": reader_instance_type,
                "specs": reader_specs["specs"] if isinstance(reader_specs, dict) and "specs" in reader_specs else reader_specs,
                "role": f"read-replica-{i+1}",
                "cross_az": True if high_availability_required else False
            })
        
        return readers
    
    def _get_smaller_instance(self, current_instance):
        """Get a smaller instance type for read replicas"""
        
        instance_hierarchy = [
            "db.t3.micro", "db.t3.small", "db.t3.medium", "db.t3.large",
            "db.m5.large", "db.m5.xlarge", "db.m5.2xlarge", "db.m5.4xlarge",
            "db.m5.8xlarge", "db.m5.12xlarge", "db.m5.16xlarge", "db.m5.24xlarge"
        ]
        
        try:
            current_index = instance_hierarchy.index(current_instance)
            smaller_index = max(0, current_index - 1)
            smaller_instance = instance_hierarchy[smaller_index]
            
            return {
                "instance_type": smaller_instance,
                "specs": self.instance_specs[smaller_instance]
            }
        except ValueError:
            return {
                "instance_type": "db.m5.large",
                "specs": self.instance_specs["db.m5.large"]
            }
    
    def _calculate_storage_requirements(self, database_size_gb, iops_requirement, environment):
        """Calculate optimal storage configuration"""
        
        # Add growth buffer
        total_storage_gb = database_size_gb * 1.5  # 50% growth buffer
        
        # Determine storage type based on IOPS requirements
        if iops_requirement > 32000:
            storage_type = "io2"
            provisioned_iops = iops_requirement
        elif iops_requirement > 16000:
            storage_type = "io1"
            provisioned_iops = iops_requirement
        elif iops_requirement > 3000:
            storage_type = "gp3"
            provisioned_iops = min(16000, iops_requirement)
        else:
            storage_type = "gp2"
            provisioned_iops = min(3000, max(100, total_storage_gb * 3))  # GP2 baseline
        
        # Backup and snapshot configuration
        env_config = self.environment_factors[environment]
        
        return {
            "storage_type": storage_type,
            "allocated_storage_gb": int(total_storage_gb),
            "provisioned_iops": int(provisioned_iops),
            "backup_retention_days": env_config["backup_retention"],
            "multi_az": env_config["availability"] == "Multi-AZ",
            "encryption_enabled": True,
            "snapshot_frequency": "daily" if environment == "Production" else "weekly"
        }
    
    def _calculate_growth_projections(self, writer_instance, reader_instances, storage_config, growth_rate):
        """Calculate growth projections for capacity planning"""
        
        projections = {}
        
        for year in [1, 2, 3]:
            growth_factor = (1 + growth_rate) ** year
            
            # Storage growth
            projected_storage = storage_config["allocated_storage_gb"] * growth_factor
            
            # Compute growth (simplified - may need instance upgrades)
            if growth_factor > 2.0:  # Significant growth
                compute_scaling = "Consider instance upgrades or additional read replicas"
            elif growth_factor > 1.5:
                compute_scaling = "Monitor performance, may need scaling"
            else:
                compute_scaling = "Current configuration sufficient"
            
            projections[f"year_{year}"] = {
                "storage_gb": int(projected_storage),
                "growth_factor": growth_factor,
                "compute_recommendation": compute_scaling,
                "estimated_cost_increase": f"{((growth_factor - 1) * 100):.1f}%"
            }
        
        return projections
    
    def _generate_sizing_rationale(self, config, cpu_req, memory_req):
        """Generate human-readable rationale for sizing decisions"""
        
        workload_type = config.get("workload_type", "Mixed")
        environment = config.get("environment", "Production")
        
        rationale = f"""
        Sizing analysis for {workload_type} workload in {environment} environment:
        
        ‚Ä¢ CPU Requirements: {cpu_req:.1f} vCPUs based on transaction volume and workload pattern
        ‚Ä¢ Memory Requirements: {memory_req:.1f} GB based on database size and connection patterns
        ‚Ä¢ Environment Factor: {self.environment_factors[environment]['scaling_factor']} applied for {environment}
        ‚Ä¢ Availability: {self.environment_factors[environment]['availability']} configuration
        """
        
        return rationale.strip()

# =============================================================================
# PDF REPORT GENERATOR
# =============================================================================

class PDFReportGenerator:
    """Generate comprehensive PDF reports for migration analysis"""
    
    def __init__(self):
        if not PDF_AVAILABLE:
            return
            
        self.styles = getSampleStyleSheet()
        self.title_style = ParagraphStyle(
            'CustomTitle',
            parent=self.styles['Heading1'],
            fontSize=24,
            spaceAfter=30,
            textColor=colors.darkblue,
            alignment=1  # Center alignment
        )
        self.heading_style = ParagraphStyle(
            'CustomHeading',
            parent=self.styles['Heading2'],
            fontSize=16,
            spaceAfter=12,
            textColor=colors.darkblue,
            leftIndent=0
        )
        self.subheading_style = ParagraphStyle(
            'CustomSubHeading',
            parent=self.styles['Heading3'],
            fontSize=14,
            spaceAfter=8,
            textColor=colors.darkgreen,
            leftIndent=20
        )
        self.body_style = ParagraphStyle(
            'CustomBody',
            parent=self.styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            leftIndent=20,
            rightIndent=20
        )
        self.highlight_style = ParagraphStyle(
            'Highlight',
            parent=self.styles['Normal'],
            fontSize=11,
            spaceAfter=8,
            backColor=colors.lightblue,
            borderColor=colors.blue,
            borderWidth=1,
            borderPadding=5,
            leftIndent=20,
            rightIndent=20
        )
    
    def generate_conclusion_report(self, config, metrics, recommendations):
        """Generate comprehensive conclusion report"""
        if not PDF_AVAILABLE:
            return None
            
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        
        # Calculate recommendation scores
        performance_score = min(100, (metrics['optimized_throughput'] / 1000) * 50)
        cost_score = min(50, max(0, 50 - (metrics['cost_breakdown']['total'] / config['budget_allocated'] - 1) * 100))
        timeline_score = min(30, max(0, 30 - (metrics['transfer_days'] / config['max_transfer_days'] - 1) * 100))
        risk_score = {"Low": 20, "Medium": 15, "High": 10, "Critical": 5}.get(recommendations['risk_level'], 15)
        overall_score = performance_score + cost_score + timeline_score + risk_score
        
        # Determine strategy status
        if overall_score >= 140:
            strategy_status = "RECOMMENDED"
            strategy_action = "PROCEED"
        elif overall_score >= 120:
            strategy_status = "CONDITIONAL"
            strategy_action = "PROCEED WITH OPTIMIZATIONS"
        elif overall_score >= 100:
            strategy_status = "REQUIRES MODIFICATION"
            strategy_action = "REVISE CONFIGURATION"
        else:
            strategy_status = "NOT RECOMMENDED"
            strategy_action = "RECONSIDER APPROACH"
        
        story = []
        
        # Title Page
        story.append(Paragraph("Enterprise Migration Strategy", self.title_style))
        story.append(Paragraph("Comprehensive Analysis & Strategic Recommendation", self.styles['Heading2']))
        story.append(Spacer(1, 30))
        
        # Executive Summary Box
        exec_summary = f"""
        <b>Project:</b> {config['project_name']}<br/>
        <b>Data Volume:</b> {metrics['data_size_tb']:.1f} TB ({config['data_size_gb']:,} GB)<br/>
        <b>Strategic Recommendation:</b> {strategy_status}<br/>
        <b>Action Required:</b> {strategy_action}<br/>
        <b>Overall Score:</b> {overall_score:.0f}/150<br/>
        <b>Success Probability:</b> {85 + (overall_score - 100) * 0.3:.0f}%
        """
        story.append(Paragraph(exec_summary, self.highlight_style))
        story.append(Spacer(1, 20))
        
        # Build PDF
        doc.build(story)
        buffer.seek(0)
        return buffer

# =============================================================================
# UNIFIED ENTERPRISE MIGRATION PLATFORM
# =============================================================================

class EnterpriseMigrationPlatform:
    """Unified platform combining networking and database migration capabilities"""
    
    def __init__(self):
        # Initialize all calculators and engines
        self.network_calculator = EnterpriseCalculator()
        self.database_sizing_engine = DatabaseSizingEngine()
        self.pdf_generator = PDFReportGenerator() if PDF_AVAILABLE else None        
        
        # ADD THESE NEW LINES:
    # Initialize enhanced components
        self.claude_ai = ClaudeAIAnalyst()
        self.migration_analyzer = MigrationOptionsAnalyzer()
        self.vrops_connector = VROpsConnector()
        
        # Initialize session state
        self.initialize_session_state()
        self.setup_custom_css()
        
        # Real-time tracking
        self.last_update_time = datetime.now()
        self.auto_refresh_interval = 30  # seconds
    
    def initialize_session_state(self):
        """Initialize unified session state for both platforms"""
        # Network migration projects
        if 'network_migration_projects' not in st.session_state:
            st.session_state.network_migration_projects = {}
        
        # Database migration configs
        if 'database_migration_configs' not in st.session_state:
            st.session_state.database_migration_configs = {}
        
        # Unified analysis results
        if 'current_network_analysis' not in st.session_state:
            st.session_state.current_network_analysis = None
        if 'current_database_analysis' not in st.session_state:
            st.session_state.current_database_analysis = None
            
        if 'migration_configurations' not in st.session_state:
            st.session_state.migration_configurations = {}
        if 'bulk_upload_data' not in st.session_state:
            st.session_state.bulk_upload_data = None
        if 'vrops_connected' not in st.session_state:
            st.session_state.vrops_connected = False 
        
        
        # User profile and audit
        if 'user_profile' not in st.session_state:
            st.session_state.user_profile = {
                'role': 'Migration Architect',
                'organization': 'Enterprise Corp',
                'security_clearance': 'Standard'
            }
        if 'audit_log' not in st.session_state:
            st.session_state.audit_log = []
        
        # Active tabs
        if 'active_main_tab' not in st.session_state:
            st.session_state.active_main_tab = "overview"
        if 'active_network_tab' not in st.session_state:
            st.session_state.active_network_tab = "dashboard"
        if 'active_database_tab' not in st.session_state:
            st.session_state.active_database_tab = "configuration"
        
        # Configuration tracking
        if 'last_config_hash' not in st.session_state:
            st.session_state.last_config_hash = None
        if 'config_change_count' not in st.session_state:
            st.session_state.config_change_count = 0
    
    def setup_professional_css(self):
        """Setup professional CSS styling - UPDATED VERSION"""
        st.markdown("""
        <style>
            /* Hide default Streamlit elements */
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
            .stDeployButton {display: none;}
            
            /* Professional main header */
            .main-header {
                background: linear-gradient(135deg, #1f4e79 0%, #2980b9 100%);
                padding: 2rem;
                border-radius: 15px;
                color: white;
                text-align: center;
                margin-bottom: 2rem;
                box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            }
            
            .main-header h1 {
                color: white;
                margin: 0;
                font-size: 2.5rem;
                font-weight: 600;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            }
            
            .main-header p {
                color: #ecf0f1;
                margin: 0.5rem 0 0 0;
                font-size: 1.1rem;
                opacity: 0.9;
            }
            
            /* Professional navigation tabs */
            .nav-container {
                background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
                padding: 1rem;
                border-radius: 12px;
                margin-bottom: 2rem;
                box-shadow: 0 4px 16px rgba(0,0,0,0.1);
            }
            
            /* Status indicator - subtle and professional */
            .status-bar {
                background: rgba(255,255,255,0.1);
                padding: 0.5rem 1rem;
                border-radius: 8px;
                margin-top: 1rem;
                font-size: 0.9rem;
                border-left: 4px solid #28a745;
            }
            
            .status-bar.warning {
                border-left-color: #ffc107;
                background: rgba(255,193,7,0.1);
            }
            
            .status-bar.error {
                border-left-color: #dc3545;
                background: rgba(220,53,69,0.1);
            }
            
            /* Professional metric cards */
            .metric-card {
                background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
                padding: 1.5rem;
                border-radius: 12px;
                border: 1px solid #e9ecef;
                margin: 0.75rem 0;
                transition: all 0.3s ease;
                box-shadow: 0 2px 12px rgba(0,0,0,0.08);
            }
            
            .metric-card:hover {
                transform: translateY(-2px);
                box-shadow: 0 6px 20px rgba(0,0,0,0.15);
            }
            
            /* Clean section headers */
            .section-header {
                background: linear-gradient(135deg, #007bff 0%, #0056b3 100%);
                color: white;
                padding: 1rem 1.5rem;
                border-radius: 8px;
                margin: 1.5rem 0 1rem 0;
                font-size: 1.2rem;
                font-weight: bold;
                box-shadow: 0 2px 8px rgba(0,123,255,0.3);
            }
            
            /* Hide debug content by default */
            .debug-content {
                display: none;
            }
            
            .debug-content.show {
                display: block;
                background: #f8f9fa;
                padding: 1rem;
                border-radius: 8px;
                margin: 1rem 0;
                border-left: 4px solid #6c757d;
            }
        </style>
        """, unsafe_allow_html=True)

    def render_professional_header(self):
        """Render professional header with optional status"""
        st.markdown("""
        <div class="main-header">
            <h1>üè¢ Enterprise Migration Platform</h1>
            <p>AI-Powered Cloud Migration Analysis ‚Ä¢ AWS DataSync ‚Ä¢ RDS ‚Ä¢ Aurora</p>
            <div class="status-bar" id="main-status">
                <span>üü¢ Platform Ready</span>
                <span style="float: right;">Enterprise Edition v2.0</span>
            </div>
        </div>
        """, unsafe_allow_html=True)

    def render_connection_status_sidebar(self):
        """Render clean connection status in sidebar (optional)"""
        with st.sidebar:
            show_debug = st.checkbox("üîß Show System Status", value=False, 
                                    help="Display detailed system connection information")
            
            if show_debug:
                st.markdown('<div class="debug-content show">', unsafe_allow_html=True)
                
                st.subheader("üîó System Status")
                
                # AWS Status
                if hasattr(self.network_calculator, 'pricing_manager') and self.network_calculator.pricing_manager:
                    pm = self.network_calculator.pricing_manager
                    if hasattr(pm, 'connection_status'):
                        status = pm.connection_status
                        if status.get('connected', False):
                            st.success(f"‚úÖ AWS Connected ({status.get('source', 'Unknown')})")
                            if 'account_id' in status:
                                st.info(f"Account: {status['account_id']}")
                        else:
                            st.warning(f"‚ö†Ô∏è AWS: {status.get('error', 'Not configured')}")
                    else:
                        st.warning("‚ö†Ô∏è AWS: Status unknown")
                else:
                    st.warning("‚ö†Ô∏è AWS: Not configured")
                
                # Claude AI Status
                if hasattr(self, 'claude_ai') and self.claude_ai:
                    if hasattr(self.claude_ai, 'connection_status'):
                        status = self.claude_ai.connection_status
                        if status.get('connected', False):
                            st.success("‚úÖ Claude AI Connected")
                        else:
                            st.warning(f"‚ö†Ô∏è Claude AI: {status.get('error', 'Not configured')}")
                    else:
                        st.warning("‚ö†Ô∏è Claude AI: Status unknown")
                else:
                    st.warning("‚ö†Ô∏è Claude AI: Not available")
                
                # vROps Status
                vrops_status = "üü¢ Connected" if st.session_state.get('vrops_connected', False) else "üî¥ Disconnected"
                st.info(f"vROps: {vrops_status}")
                
                st.markdown('</div>', unsafe_allow_html=True)

    def render_professional_navigation(self):
        """Render professional navigation without clutter"""
        st.markdown('<div class="nav-container">', unsafe_allow_html=True)
        
        col1, col2, col3, col4, col5 = st.columns([2, 2, 2, 2, 2])
        
        with col1:
            if st.button("üè† Overview", key="nav_overview", use_container_width=True):
                st.session_state.active_main_tab = "overview"
        with col2:
            if st.button("üåê Network Migration", key="nav_network", use_container_width=True):
                st.session_state.active_main_tab = "network"
        with col3:
            if st.button("üóÑÔ∏è Database Migration", key="nav_database", use_container_width=True):
                st.session_state.active_main_tab = "database"
        with col4:
            if st.button("üìä Analytics", key="nav_unified", use_container_width=True):
                st.session_state.active_main_tab = "unified"
        with col5:
            if st.button("üìã Reports", key="nav_reports", use_container_width=True):
                st.session_state.active_main_tab = "reports"
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        def safe_dataframe_display(self, df, use_container_width=True, hide_index=True, **kwargs):
            """Safely display a DataFrame by ensuring all values are strings to prevent type mixing"""
            try:
                # Convert all values to strings to prevent type mixing issues
                df_safe = df.astype(str)
                st.dataframe(df_safe, use_container_width=use_container_width, hide_index=hide_index, **kwargs)
            except Exception as e:
                st.error(f"Error displaying table: {str(e)}")
                st.write("Raw data:")
                st.write(df)

        def safe_float_conversion(self, value, default=0.0):
            """Safely convert any value to float"""
            try:
                if isinstance(value, str):
                    cleaned = ''.join(c for c in value if c.isdigit() or c in '.-')
                    return float(cleaned) if cleaned else default
                elif isinstance(value, (int, float)):
                    return float(value)
                else:
                    return default
            except (ValueError, TypeError):
                return default

        def safe_format_currency(self, value, decimal_places=0):
            """Safely format a value as currency"""
            try:
                numeric_value = self.safe_float_conversion(value)
                if decimal_places == 0:
                    return f"${numeric_value:,.0f}"
                else:
                    return f"${numeric_value:,.{decimal_places}f}"
            except:
                return "$0"
        
        def log_audit_event(self, event_type, details):
            """Log audit events"""
            event = {
                "timestamp": datetime.now().isoformat(),
                "type": event_type,
                "details": details,
                "user": st.session_state.user_profile["role"]
            }
            st.session_state.audit_log.append(event)
        
        def render_header(self):
            """Render the unified main header"""
            st.markdown("""
            <div class="main-header">
                <h1>üè¢ Enterprise Migration Platform</h1>
                <p style="font-size: 1.2rem; margin-top: 0.5rem;">Unified Network & Database Migration ‚Ä¢ AI-Powered ‚Ä¢ Real-time Analysis</p>
                <p style="font-size: 0.9rem; margin-top: 0.5rem; opacity: 0.9;">AWS DataSync ‚Ä¢ RDS ‚Ä¢ Aurora ‚Ä¢ EC2 ‚Ä¢ Professional Grade</p>
            </div>
            """, unsafe_allow_html=True)
        
        def render_main_navigation(self):
            """Render main platform navigation"""
            st.markdown('<div class="tab-container">', unsafe_allow_html=True)
            
            col1, col2, col3, col4, col5 = st.columns([2, 2, 2, 2, 2])
            
            with col1:
                if st.button("üè† Overview", key="nav_overview"):
                    st.session_state.active_main_tab = "overview"
            with col2:
                if st.button("üåê Network Migration", key="nav_network"):
                    st.session_state.active_main_tab = "network"
            with col3:
                if st.button("üóÑÔ∏è Database Migration", key="nav_database"):
                    st.session_state.active_main_tab = "database"
            with col4:
                if st.button("üìä Unified Analytics", key="nav_unified"):
                    st.session_state.active_main_tab = "unified"
            with col5:
                if st.button("üìã Reports", key="nav_reports"):
                    st.session_state.active_main_tab = "reports"
            
            st.markdown('</div>', unsafe_allow_html=True)
    
    def render_overview_tab(self):
        """Render unified overview dashboard"""
        st.markdown('<div class="section-header">üè† Enterprise Migration Overview</div>', unsafe_allow_html=True)
        
        # Platform selection cards
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="platform-card">
                <h3>üåê Network & Data Migration</h3>
                <p><strong>Features:</strong></p>
                <ul>
                    <li>AWS DataSync Multi-Agent Optimization</li>
                    <li>Real-time Network Performance Analysis</li>
                    <li>Direct Connect & VPN Planning</li>
                    <li>Geographic Route Optimization</li>
                    <li>Security & Compliance Management</li>
                </ul>
                <p><strong>Use Cases:</strong> Large file migrations, data center moves, cloud storage migrations</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="platform-card">
                <h3>üóÑÔ∏è Database Migration</h3>
                <p><strong>Features:</strong></p>
                <ul>
                    <li>AI-Powered Database Sizing</li>
                    <li>Real-time AWS RDS/Aurora Pricing</li>
                    <li>Comprehensive Risk Assessment</li>
                    <li>Migration Timeline Planning</li>
                    <li>vROps Integration</li>
                </ul>
                <p><strong>Use Cases:</strong> SQL Server to RDS, Oracle to Aurora, PostgreSQL migrations</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Quick stats dashboard
        st.markdown('<div class="section-header">üìä Platform Statistics</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            network_projects = len(st.session_state.network_migration_projects)
            st.metric("Network Projects", str(network_projects), "+1" if network_projects > 0 else "Start New")
        
        with col2:
            database_projects = len(st.session_state.database_migration_configs)
            st.metric("Database Projects", str(database_projects), "+1" if database_projects > 0 else "Start New")
        
        with col3:
            total_analysis = (1 if st.session_state.current_network_analysis else 0) + (1 if st.session_state.current_database_analysis else 0)
            st.metric("Active Analyses", str(total_analysis))
        
        with col4:
            audit_events = len(st.session_state.audit_log)
            st.metric("Audit Events", str(audit_events))
        
        # Recent activity
        st.markdown('<div class="section-header">üìã Recent Activity</div>', unsafe_allow_html=True)
        
        if st.session_state.audit_log:
            recent_events = st.session_state.audit_log[-5:]  # Last 5 events
            for event in reversed(recent_events):
                timestamp = datetime.fromisoformat(event['timestamp']).strftime("%Y-%m-%d %H:%M")
                st.write(f"üïê {timestamp} - {event['type']}: {event['details']}")
        else:
            st.info("No recent activity. Start a migration analysis to see events here.")
        
        # ADD THIS NEW SECTION:
    # Enhanced platform status
        st.markdown('<div class="section-header">üîó Integration Status</div>', unsafe_allow_html=True)

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            ai_status = "üü¢ Connected" if self.claude_ai.available else "üî¥ Unavailable"
            st.metric("Claude AI", ai_status)

        with col2:
            aws_status = "üü¢ Available" if hasattr(self, 'network_calculator') and self.network_calculator.pricing_manager else "üî¥ Not Configured"
            st.metric("AWS Pricing API", aws_status)

        with col3:
            vrops_status = "üü¢ Connected" if st.session_state.vrops_connected else "üî¥ Disconnected"
            st.metric("vROps Integration", vrops_status)

        with col4:
            bulk_status = "üü¢ Ready" if VROPS_AVAILABLE else "üî¥ Dependencies Missing"
            st.metric("Bulk Upload", bulk_status)
                
        
        # Getting started guide
        st.markdown('<div class="section-header">üöÄ Getting Started</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üåê Network Migration Steps:**")
            st.write("1. Navigate to 'Network Migration' tab")
            st.write("2. Configure your data migration parameters")
            st.write("3. Set up network and instance settings")
            st.write("4. Review AI-powered recommendations")
            st.write("5. Generate professional reports")
        
        with col2:
            st.markdown("**üóÑÔ∏è Database Migration Steps:**")
            st.write("1. Navigate to 'Database Migration' tab")
            st.write("2. Configure your database characteristics")
            st.write("3. Run AI-powered sizing analysis")
            st.write("4. Review cost and risk assessments")
            st.write("5. Generate migration timeline")
    
    # =========================================================================
    # NETWORK MIGRATION METHODS (FULL IMPLEMENTATION)
    # =========================================================================
    
    def render_network_migration_platform(self):
        """Render the complete network migration platform with full functionality"""
        st.markdown('<div class="section-header">üåê Network & Data Migration Platform</div>', unsafe_allow_html=True)
        
        # Network platform navigation
        col1, col2, col3, col4, col5, col6, col7 = st.columns([2, 2, 2, 2, 2, 2, 2])
        
        with col1:
            if st.button("üè† Dashboard", key="net_nav_dashboard"):
                st.session_state.active_network_tab = "dashboard"
        with col2:
            if st.button("üåê Network Analysis", key="net_nav_network"):
                st.session_state.active_network_tab = "network"
        with col3:
            if st.button("üìä Migration Planner", key="net_nav_planner"):
                st.session_state.active_network_tab = "planner"
        with col4:
            if st.button("‚ö° Performance", key="net_nav_performance"):
                st.session_state.active_network_tab = "performance"
        with col5:
            if st.button("üîí Security", key="net_nav_security"):
                st.session_state.active_network_tab = "security"
        with col6:
            if st.button("üìà Analytics", key="net_nav_analytics"):
                st.session_state.active_network_tab = "analytics"
        with col7:
            if st.button("üéØ Conclusion", key="net_nav_conclusion"):
                st.session_state.active_network_tab = "conclusion"
        
        # Get network configuration from sidebar
        network_config = self.render_network_sidebar_controls()
        
        # Calculate network migration metrics
        network_metrics = self.calculate_network_migration_metrics(network_config)
        
        # Render appropriate network tab
        if st.session_state.active_network_tab == "dashboard":
            self.render_network_dashboard_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "network":
            self.render_network_analysis_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "planner":
            self.render_network_planner_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "performance":
            self.render_network_performance_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "security":
            self.render_network_security_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "analytics":
            self.render_network_analytics_tab(network_config, network_metrics)
        elif st.session_state.active_network_tab == "conclusion":
            self.render_network_conclusion_tab(network_config, network_metrics)
    
    def render_network_sidebar_controls(self):
        """Render network migration sidebar controls"""
        st.sidebar.header("üè¢ Enterprise Controls")                   

        # Get AWS configuration status
        aws_config = self.render_aws_credentials_section()
        
        # Project management section
        st.sidebar.subheader("üìÅ Project Management")
        project_name = st.sidebar.text_input("Project Name", value="Migration-2025-Q1")
        business_unit = st.sidebar.selectbox("Business Unit", 
            ["Corporate IT", "Finance", "HR", "Operations", "R&D", "Sales & Marketing"])
        project_priority = st.sidebar.selectbox("Project Priority", 
            ["Critical", "High", "Medium", "Low"])
        migration_wave = st.sidebar.selectbox("Migration Wave", 
            ["Wave 1 (Pilot)", "Wave 2 (Core Systems)", "Wave 3 (Secondary)", "Wave 4 (Archive)"])
        
        # Security and compliance section
        st.sidebar.subheader("üîí Security & Compliance")
        data_classification = st.sidebar.selectbox("Data Classification", 
            ["Public", "Internal", "Confidential", "Restricted", "Top Secret"])
        compliance_frameworks = st.sidebar.multiselect("Compliance Requirements", 
            ["SOX", "GDPR", "HIPAA", "PCI-DSS", "SOC2", "ISO27001", "FedRAMP", "FISMA"])
        encryption_in_transit = st.sidebar.checkbox("Encryption in Transit", value=True)
        encryption_at_rest = st.sidebar.checkbox("Encryption at Rest", value=True)
        data_residency = st.sidebar.selectbox("Data Residency Requirements", 
            ["No restrictions", "US only", "EU only", "Specific region", "On-premises only"])
        
        # Enterprise parameters section
        st.sidebar.subheader("üéØ Enterprise Parameters")
        sla_requirements = st.sidebar.selectbox("SLA Requirements", 
            ["99.9% availability", "99.95% availability", "99.99% availability", "99.999% availability"])
        rto_hours = st.sidebar.number_input("Recovery Time Objective (hours)", min_value=1, max_value=168, value=4)
        rpo_hours = st.sidebar.number_input("Recovery Point Objective (hours)", min_value=0, max_value=24, value=1)
        max_transfer_days = st.sidebar.number_input("Maximum Transfer Days", min_value=1, max_value=90, value=30)
        
        # Budget section
        budget_allocated = st.sidebar.number_input("Allocated Budget ($)", min_value=1000, max_value=10000000, value=100000, step=1000)
        approval_required = st.sidebar.checkbox("Executive Approval Required", value=True)
        
        # Data characteristics section
        st.sidebar.subheader("üìä Data Profile")
        data_size_gb = st.sidebar.number_input("Total Data Size (GB)", min_value=1, max_value=1000000, value=10000, step=100)
        data_types = st.sidebar.multiselect("Data Types", 
            ["Customer Data", "Financial Records", "Employee Data", "Intellectual Property", 
             "System Logs", "Application Data", "Database Backups", "Media Files", "Documents"])
        database_types = st.sidebar.multiselect("Database Systems", 
            ["Oracle", "SQL Server", "MySQL", "PostgreSQL", "MongoDB", "Cassandra", "Redis", "Elasticsearch"])
        avg_file_size = st.sidebar.selectbox("Average File Size",
            ["< 1MB (Many small files)", "1-10MB (Small files)", "10-100MB (Medium files)", 
             "100MB-1GB (Large files)", "> 1GB (Very large files)"])
        data_growth_rate = st.sidebar.slider("Annual Data Growth Rate (%)", min_value=0, max_value=100, value=20)
        data_volatility = st.sidebar.selectbox("Data Change Frequency", 
            ["Static (rarely changes)", "Low (daily changes)", "Medium (hourly changes)", "High (real-time)"])
        
        # Network infrastructure section
        st.sidebar.subheader("üåê Network Configuration")
        network_topology = st.sidebar.selectbox("Network Topology", 
            ["Single DX", "Redundant DX", "Hybrid (DX + VPN)", "Multi-region", "SD-WAN"])
        dx_bandwidth_mbps = st.sidebar.number_input("Primary DX Bandwidth (Mbps)", min_value=50, max_value=100000, value=10000, step=100)
        dx_redundant = st.sidebar.checkbox("Redundant DX Connection", value=True)
        if dx_redundant:
            dx_secondary_mbps = st.sidebar.number_input("Secondary DX Bandwidth (Mbps)", min_value=50, max_value=100000, value=10000, step=100)
        else:
            dx_secondary_mbps = 0
        
        network_latency = st.sidebar.slider("Network Latency to AWS (ms)", min_value=1, max_value=500, value=25)
        network_jitter = st.sidebar.slider("Network Jitter (ms)", min_value=0, max_value=50, value=5)
        packet_loss = st.sidebar.slider("Packet Loss (%)", min_value=0.0, max_value=5.0, value=0.1, step=0.1)
        qos_enabled = st.sidebar.checkbox("QoS Enabled", value=True)
        dedicated_bandwidth = st.sidebar.slider("Dedicated Migration Bandwidth (%)", min_value=10, max_value=90, value=60)
        business_hours_restriction = st.sidebar.checkbox("Restrict to Off-Business Hours", value=True)
        
        # Transfer configuration section
        st.sidebar.subheader("üöÄ Transfer Configuration")
        num_datasync_agents = st.sidebar.number_input("DataSync Agents", min_value=1, max_value=50, value=5)
        datasync_instance_type = st.sidebar.selectbox("DataSync Instance Type",
            ["m5.large", "m5.xlarge", "m5.2xlarge", "m5.4xlarge", "m5.8xlarge", 
             "c5.2xlarge", "c5.4xlarge", "c5.9xlarge", "r5.2xlarge", "r5.4xlarge"])
        
        # Real-world performance modeling
        st.sidebar.subheader("üìä Performance Modeling")
        real_world_mode = st.sidebar.checkbox("Real-world Performance Mode", value=True, 
        help="Include real-world factors like storage I/O, DataSync overhead, and AWS API limits")
        
        # ADD THIS NEW SECTION:
        # NEW: AI Analysis Options
        st.sidebar.subheader("ü§ñ AI Analysis Options")
        enable_ai_analysis = st.sidebar.checkbox("Enable Claude AI Analysis", value=True,
            help="Use real Claude AI for migration strategy recommendations")
        analyze_all_methods = st.sidebar.checkbox("Analyze All Migration Methods", value=True,
            help="Compare DataSync, Snowball, DMS, Storage Gateway, etc.")       
               
        
        # Network optimization section
        st.sidebar.subheader("üåê Network Optimization")
        tcp_window_size = st.sidebar.selectbox("TCP Window Size", 
            ["Default", "64KB", "128KB", "256KB", "512KB", "1MB", "2MB"])
        mtu_size = st.sidebar.selectbox("MTU Size", 
            ["1500 (Standard)", "9000 (Jumbo Frames)", "Custom"])
        network_congestion_control = st.sidebar.selectbox("Congestion Control Algorithm",
            ["Cubic (Default)", "BBR", "Reno", "Vegas"])
        wan_optimization = st.sidebar.checkbox("WAN Optimization", value=False)
        parallel_streams = st.sidebar.slider("Parallel Streams per Agent", min_value=1, max_value=100, value=20)
        use_transfer_acceleration = st.sidebar.checkbox("S3 Transfer Acceleration", value=True)
        
        # Storage configuration section
        st.sidebar.subheader("üíæ Storage Strategy")
        s3_storage_class = st.sidebar.selectbox("Primary S3 Storage Class",
            ["Standard", "Standard-IA", "One Zone-IA", "Glacier Instant Retrieval", 
             "Glacier Flexible Retrieval", "Glacier Deep Archive"])
        enable_versioning = st.sidebar.checkbox("Enable S3 Versioning", value=True)
        enable_lifecycle = st.sidebar.checkbox("Lifecycle Policies", value=True)
        cross_region_replication = st.sidebar.checkbox("Cross-Region Replication", value=False)
        
        # Geographic configuration section
        st.sidebar.subheader("üó∫Ô∏è Geographic Settings")
        source_location = st.sidebar.selectbox("Source Data Center Location",
            ["San Jose, CA", "San Antonio, TX", "New York, NY", "Chicago, IL", "Dallas, TX", 
             "Los Angeles, CA", "Atlanta, GA", "London, UK", "Frankfurt, DE", "Tokyo, JP", "Sydney, AU", "Other"])
        target_aws_region = st.sidebar.selectbox("Target AWS Region",
            ["us-east-1 (N. Virginia)", "us-east-2 (Ohio)", "us-west-1 (N. California)", 
             "us-west-2 (Oregon)", "eu-west-1 (Ireland)", "eu-central-1 (Frankfurt)",
             "ap-southeast-1 (Singapore)", "ap-northeast-1 (Tokyo)"])
        
        return {
            'project_name': project_name,
            'business_unit': business_unit,
            'project_priority': project_priority,
            'migration_wave': migration_wave,
            'data_classification': data_classification,
            'compliance_frameworks': compliance_frameworks,
            'encryption_in_transit': encryption_in_transit,
            'encryption_at_rest': encryption_at_rest,
            'data_residency': data_residency,
            'sla_requirements': sla_requirements,
            'rto_hours': rto_hours,
            'rpo_hours': rpo_hours,
            'max_transfer_days': max_transfer_days,
            'budget_allocated': budget_allocated,
            'approval_required': approval_required,
            'data_size_gb': data_size_gb,
            'data_types': data_types,
            'database_types': database_types,
            'avg_file_size': avg_file_size,
            'data_growth_rate': data_growth_rate,
            'data_volatility': data_volatility,
            'network_topology': network_topology,
            'dx_bandwidth_mbps': dx_bandwidth_mbps,
            'dx_redundant': dx_redundant,
            'dx_secondary_mbps': dx_secondary_mbps,
            'network_latency': network_latency,
            'network_jitter': network_jitter,
            'packet_loss': packet_loss,
            'qos_enabled': qos_enabled,
            'dedicated_bandwidth': dedicated_bandwidth,
            'business_hours_restriction': business_hours_restriction,
            'num_datasync_agents': num_datasync_agents,
            'datasync_instance_type': datasync_instance_type,
            'tcp_window_size': tcp_window_size,
            'mtu_size': mtu_size,
            'network_congestion_control': network_congestion_control,
            'wan_optimization': wan_optimization,
            'parallel_streams': parallel_streams,
            'use_transfer_acceleration': use_transfer_acceleration,
            's3_storage_class': s3_storage_class,
            'enable_versioning': enable_versioning,
            'enable_lifecycle': enable_lifecycle,
            'cross_region_replication': cross_region_replication,
            'source_location': source_location,
            'target_aws_region': target_aws_region,
            'real_world_mode': real_world_mode,
            'use_aws_pricing': aws_config['use_aws_pricing'],
            'aws_region': aws_config['aws_region'],
            'aws_configured': aws_config['aws_configured'],
            'target_aws_region': target_aws_region,
            'analyze_all_methods': analyze_all_methods,  # ADD THIS LINE
            'enable_ai_analysis': enable_ai_analysis,    # ADD THIS LINE
            
        
        }
    
    def render_aws_credentials_section(self):
        """Render AWS credentials status from multiple sources"""
        with st.sidebar:
            st.subheader("üîë AWS Configuration")
            
            # Check multiple credential sources
            aws_configured = False
            aws_region = 'us-east-1'
            credential_source = None
            
            try:
                # Method 1: Check Streamlit secrets
                if hasattr(st, 'secrets') and 'aws' in st.secrets:
                    aws_configured = True
                    aws_region = st.secrets["aws"].get("region", "us-east-1")
                    credential_source = "Streamlit Secrets"
                
                else:
                    # Method 2: Check if boto3 can create a client (default credential chain)
                    try:
                        import boto3
                        from botocore.exceptions import NoCredentialsError, ClientError
                        
                        # Try to create a test client to verify credentials
                        test_client = boto3.client('sts', region_name='us-east-1')  # STS is lightweight
                        
                        # Try to get caller identity to verify credentials work
                        response = test_client.get_caller_identity()
                        
 # Try to create a test client
                    test_client = boto3.client('sts', region_name='us-east-1')
                    response = test_client.get_caller_identity()
                    
                    if response and 'Account' in response:
                        aws_configured = True
                        credential_source = "AWS Default Chain"
                        
                        # Store account info for optional display
                        self.aws_account_info = {
                            'account_id': response['Account'],
                            'arn': response.get('Arn', 'Unknown'),
                            'user_id': response.get('UserId', 'Unknown')
                        }
                        
                except (NoCredentialsError, ClientError):
                    aws_configured = False
                    credential_source = "None"
                    
        except Exception as e:
            aws_configured = False
            credential_source = "Error"
        
        # Show simple status
        if aws_configured:
            st.success("‚úÖ AWS Configured")
            st.write(f"**Source:** {credential_source}")
            st.write(f"**Region:** {aws_region}")
            
            # Show account info only in debug mode
            if st.checkbox("Show AWS Details", value=False):
                if hasattr(self, 'aws_account_info'):
                    info = self.aws_account_info
                    st.info(f"**Account:** {info['account_id']}")
                    with st.expander("üîç Additional Details"):
                        st.write(f"**ARN:** {info['arn']}")
                        st.write(f"**User ID:** {info['user_id']}")
        else:
            st.warning("‚ö†Ô∏è AWS Not Configured")
            
            # Show setup help
            with st.expander("üõ†Ô∏è Setup Help"):
                st.markdown("""
                **Quick Setup Options:**
                
                1. **Environment Variables:**
                ```bash
                export AWS_ACCESS_KEY_ID="AKIA..."
                export AWS_SECRET_ACCESS_KEY="..."
                export AWS_DEFAULT_REGION="us-east-1"
                ```
                
                2. **Streamlit Secrets:**
                ```toml
                # .streamlit/secrets.toml
                [aws]
                access_key_id = "AKIA..."
                secret_access_key = "..."
                region = "us-east-1"
                ```
                
                3. **AWS CLI:**
                ```bash
                aws configure
                ```
                """)
        
        # Toggle for using real-time pricing
        use_aws_pricing = st.checkbox(
            "Enable Real-time Pricing", 
            value=aws_configured,
            help="Use AWS Pricing API for real-time cost calculations",
            disabled=not aws_configured
        )
        
        return {
            'use_aws_pricing': use_aws_pricing,
            'aws_region': aws_region,
            'aws_configured': aws_configured,
            'credential_source': credential_source
        }
            
            # Configuration help
            if not aws_configured:
                with st.expander("üõ†Ô∏è AWS Setup Help"):
                    st.markdown("""
                    **Option 1: Streamlit Secrets (Recommended for local development)**
                    ```toml
                    # .streamlit/secrets.toml
                    [aws]
                    access_key_id = "AKIA..."
                    secret_access_key = "..."
                    region = "us-east-1"
                    ```
                    
                    **Option 2: Environment Variables**
                    ```bash
                    export AWS_ACCESS_KEY_ID="AKIA..."
                    export AWS_SECRET_ACCESS_KEY="..."
                    export AWS_DEFAULT_REGION="us-east-1"
                    ```
                    
                    **Option 3: AWS Credentials File**
                    ```bash
                    aws configure
                    ```
                    
                    **Option 4: IAM Role (for EC2/ECS deployment)**
                    - Attach IAM role with pricing API permissions
                    """)
            
            # Toggle for using real-time pricing
            use_aws_pricing = st.checkbox(
                "Enable Real-time AWS Pricing", 
                value=aws_configured,
                help="Use AWS Pricing API for real-time cost calculations",
                disabled=not aws_configured
            )
            
            # Show current pricing manager status
            if hasattr(self, 'network_calculator') and self.network_calculator.pricing_manager:
                pricing_manager = self.network_calculator.pricing_manager
                if pricing_manager.pricing_client:
                    st.info("üí° Pricing Manager: Connected")
                else:
                    st.warning("‚ö†Ô∏è Pricing Manager: Using fallback pricing")
            
            return {
                'use_aws_pricing': use_aws_pricing,
                'aws_region': aws_region,
                'aws_configured': aws_configured,
                'credential_source': credential_source
            }
                    
                #except Exception as e:
                 #   st.warning(f"‚ö†Ô∏è Cannot verify AWS credentials: {str(e)[:50]}...")
                  #  credential_source = f"Error: {str(e)[:30]}..."
                
        #except Exception as e:
            #st.error(f"‚ùå Error checking AWS configuration: {str(e)}")
            #credential_source = "Error"
        
        # Configuration help
        if not aws_configured:
            with st.expander("üõ†Ô∏è AWS Setup Help"):
                st.markdown("""
                **Option 1: Streamlit Secrets (Recommended for local development)**
                ```toml
                # .streamlit/secrets.toml
                [aws]
                access_key_id = "AKIA..."
                secret_access_key = "..."
                region = "us-east-1"
                ```
                
                **Option 2: Environment Variables**
                ```bash
                export AWS_ACCESS_KEY_ID="AKIA..."
                export AWS_SECRET_ACCESS_KEY="..."
                export AWS_DEFAULT_REGION="us-east-1"
                ```
                
                **Option 3: AWS Credentials File**
                ```bash
                aws configure
                ```
                
                **Option 4: IAM Role (for EC2/ECS deployment)**
                - Attach IAM role with pricing API permissions
                """)
        
        # Toggle for using real-time pricing
        use_aws_pricing = st.checkbox(
            "Enable Real-time AWS Pricing", 
            value=aws_configured,
            help="Use AWS Pricing API for real-time cost calculations",
            disabled=not aws_configured
        )
        
        
        # Show current pricing manager status
        if hasattr(self, 'network_calculator') and self.network_calculator.pricing_manager:
            pricing_manager = self.network_calculator.pricing_manager
            if pricing_manager.pricing_client:
                st.info("üí° Pricing Manager: Connected")
            else:
                st.warning("‚ö†Ô∏è Pricing Manager: Using fallback pricing")
        
        return {
            'use_aws_pricing': use_aws_pricing,
            'aws_region': aws_region,
            'aws_configured': aws_configured,
            'credential_source': credential_source
        }
    
    def calculate_network_migration_metrics(self, config):
        """Calculate all migration metrics with error handling and agent optimization"""
        try:
            # Basic calculations with type safety
            data_size_gb = float(config.get('data_size_gb', 1000))
            data_size_tb = max(0.1, data_size_gb / 1024)
            effective_data_gb = data_size_gb * 0.85
            
            # Ensure numeric values from config
            dx_bandwidth_mbps = float(config.get('dx_bandwidth_mbps', 1000))
            network_latency = float(config.get('network_latency', 25))
            network_jitter = float(config.get('network_jitter', 5))
            packet_loss = float(config.get('packet_loss', 0.1))
            dedicated_bandwidth = float(config.get('dedicated_bandwidth', 60))
            num_datasync_agents = int(config.get('num_datasync_agents', 1))
            
            # AGENT OPTIMIZATION ANALYSIS - with null check
            agent_analysis = None
            try:
                agent_analysis = self._analyze_datasync_agent_optimization(
                    data_size_tb, dx_bandwidth_mbps, num_datasync_agents, config
                )
            except Exception as e:
                agent_analysis = {
                    "status": "ERROR",
                    "error": str(e),
                    "current_agents": num_datasync_agents,
                    "recommended_agents": num_datasync_agents,
                    "recommendation": "Agent analysis unavailable"
                }
            
            # Ensure agent_analysis is not None
            if agent_analysis is None:
                agent_analysis = {
                    "status": "UNKNOWN",
                    "current_agents": num_datasync_agents,
                    "recommended_agents": num_datasync_agents,
                    "recommendation": "Analysis not performed"
                }
            
            # Calculate throughput with current configuration
            throughput_result = None
            try:
                if hasattr(self, 'network_calculator') and self.network_calculator is not None:
                    throughput_result = self.network_calculator.calculate_enterprise_throughput(
                        config['datasync_instance_type'], num_datasync_agents, config['avg_file_size'], 
                        config['dx_bandwidth_mbps'], config['network_latency'], config['network_jitter'], 
                        config['packet_loss'], config['qos_enabled'], config['dedicated_bandwidth'], 
                        config.get('real_world_mode', True)
                    )
            except Exception as e:
                st.warning(f"Throughput calculation failed: {str(e)}")
            
            # Process throughput results with fallback
            if throughput_result is not None:
                if len(throughput_result) == 4:
                    datasync_throughput, network_efficiency, theoretical_throughput, real_world_efficiency = throughput_result
                elif len(throughput_result) == 2:
                    datasync_throughput, network_efficiency = throughput_result
                    theoretical_throughput = datasync_throughput * 1.5
                    real_world_efficiency = 0.7
                else:
                    # Unexpected result format
                    datasync_throughput = 100
                    network_efficiency = 0.7
                    theoretical_throughput = 150
                    real_world_efficiency = 0.7
            else:
                # Fallback values if throughput calculation fails
                datasync_throughput = 100
                network_efficiency = 0.7
                theoretical_throughput = 150
                real_world_efficiency = 0.7
            
            # Ensure valid throughput values
            datasync_throughput = max(1, float(datasync_throughput))
            network_efficiency = max(0.1, min(1.0, float(network_efficiency)))
            
            # Apply network optimizations
            tcp_efficiency = {"Default": 1.0, "64KB": 1.05, "128KB": 1.1, "256KB": 1.15, 
                            "512KB": 1.2, "1MB": 1.25, "2MB": 1.3}
            mtu_efficiency = {"1500 (Standard)": 1.0, "9000 (Jumbo Frames)": 1.15, "Custom": 1.1}
            congestion_efficiency = {"Cubic (Default)": 1.0, "BBR": 1.2, "Reno": 0.95, "Vegas": 1.05}
            
            tcp_factor = tcp_efficiency.get(config.get('tcp_window_size', 'Default'), 1.0)
            mtu_factor = mtu_efficiency.get(config.get('mtu_size', '1500 (Standard)'), 1.0)
            congestion_factor = congestion_efficiency.get(config.get('network_congestion_control', 'Cubic (Default)'), 1.0)
            wan_factor = 1.3 if config.get('wan_optimization', False) else 1.0
            
            optimized_throughput = datasync_throughput * tcp_factor * mtu_factor * congestion_factor * wan_factor
            optimized_throughput = min(optimized_throughput, config['dx_bandwidth_mbps'] * (config['dedicated_bandwidth'] / 100))
            optimized_throughput = max(1, optimized_throughput)
            
            # Calculate timing FIRST before using in cost calculations
            available_hours_per_day = 16 if config.get('business_hours_restriction', False) else 24
            transfer_days = (effective_data_gb * 8) / (optimized_throughput * available_hours_per_day * 3600) / 1000
            transfer_days = max(0.1, transfer_days)
            
            # Initialize cost_breakdown with defaults
            cost_breakdown = {
                'compute': 1000,
                'transfer': 500,
                'storage': 200,
                'compliance': len(config.get('compliance_frameworks', [])) * 500,
                'monitoring': transfer_days * 200,
                'total': 2200,
                'pricing_source': 'Fallback',
                'last_updated': time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime())
            }
            
            # Now calculate costs based on analysis scope
            if config.get('analyze_all_methods', False):
                # When analyzing all methods, use the most cost-effective option
                try:
                    if hasattr(self, 'migration_analyzer') and self.migration_analyzer is not None:
                        migration_options = self.migration_analyzer.analyze_all_options(config)
                        
                        if migration_options and isinstance(migration_options, dict):
                            # Find the most cost-effective method
                            best_method = min(migration_options.values(), key=lambda x: x.get('estimated_cost', 1000))
                            
                            if best_method and 'estimated_cost' in best_method:
                                # Use the cost from the best method
                                cost_breakdown = {
                                    'compute': best_method['estimated_cost'] * 0.4,
                                    'transfer': best_method['estimated_cost'] * 0.3,
                                    'storage': best_method['estimated_cost'] * 0.2,
                                    'compliance': len(config.get('compliance_frameworks', [])) * 500,
                                    'monitoring': transfer_days * 200,
                                    'total': best_method['estimated_cost'],
                                    'pricing_source': 'Migration Options Analysis',
                                    'last_updated': time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime())
                                }
                except Exception as e:
                    st.warning(f"Migration options analysis failed: {str(e)}, using fallback costs")
            else:
                # Use standard DataSync calculation
                try:
                    if hasattr(self, 'network_calculator') and self.network_calculator is not None:
                        calc_result = self.network_calculator.calculate_enterprise_costs(
                            config['data_size_gb'], transfer_days, config['datasync_instance_type'], 
                            config['num_datasync_agents'], config.get('compliance_frameworks', []), 
                            config.get('s3_storage_class', 'Standard')
                        )
                        
                        if calc_result and isinstance(calc_result, dict):
                            cost_breakdown = calc_result
                except Exception as e:
                    st.warning(f"Cost calculation failed: {str(e)}, using fallback costs")
            
            # Apply optimization factor when analyzing all methods
            if config.get('analyze_all_methods', False) and cost_breakdown:
                # When analyzing all methods, we find more cost-effective approaches
                optimization_factor = 0.75  # 25% cost reduction from finding optimal method
                
                # Apply optimization to all cost components
                for key in cost_breakdown:
                    if key not in ['pricing_source', 'last_updated', 'cost_breakdown_detailed']:
                        if isinstance(cost_breakdown[key], (int, float)):
                            cost_breakdown[key] *= optimization_factor
            
            # Compliance and business impact with error handling
            compliance_reqs = []
            compliance_risks = []
            business_impact = {'score': 0.5, 'level': 'Medium', 'recommendation': 'Standard approach'}
            
            try:
                if hasattr(self, 'network_calculator') and self.network_calculator is not None:
                    compliance_reqs, compliance_risks = self.network_calculator.assess_compliance_requirements(
                        config.get('compliance_frameworks', []), 
                        config.get('data_classification', 'Internal'), 
                        config.get('data_residency', 'No restrictions')
                    )
                    business_impact = self.network_calculator.calculate_business_impact(
                        transfer_days, config.get('data_types', [])
                    )
            except Exception as e:
                st.warning(f"Compliance/business impact analysis failed: {str(e)}")
            
            # Get AI-powered networking recommendations with agent optimization
            networking_recommendations = {
                'primary_method': 'DataSync',
                'secondary_method': 'S3 Transfer Acceleration',
                'networking_option': 'Direct Connect',
                'db_migration_tool': 'DMS',
                'rationale': 'Default configuration recommendation',
                'estimated_performance': {'throughput_mbps': optimized_throughput, 'estimated_days': transfer_days, 'network_efficiency': network_efficiency},
                'cost_efficiency': 'Medium',
                'risk_level': 'Low'
            }
            
            try:
                if hasattr(self, 'network_calculator') and self.network_calculator is not None:
                    target_region_short = config.get('target_aws_region', 'us-east-1').split()[0]
                    recommendations_result = self.network_calculator.get_optimal_networking_architecture(
                        config.get('source_location', 'San Jose, CA'), 
                        target_region_short, 
                        config['data_size_gb'],
                        config['dx_bandwidth_mbps'], 
                        config.get('database_types', []), 
                        config.get('data_types', []), 
                        config
                    )
                    
                    if recommendations_result and isinstance(recommendations_result, dict):
                        networking_recommendations = recommendations_result
            except Exception as e:
                st.warning(f"Networking recommendations failed: {str(e)}")
            
            # Enhance networking recommendations with agent analysis - with null check
            if networking_recommendations is not None and isinstance(networking_recommendations, dict):
                networking_recommendations['agent_optimization'] = agent_analysis
            
            return {
                'data_size_tb': data_size_tb,
                'effective_data_gb': effective_data_gb,
                'datasync_throughput': datasync_throughput,
                'theoretical_throughput': theoretical_throughput,
                'real_world_efficiency': real_world_efficiency,
                'optimized_throughput': optimized_throughput,
                'network_efficiency': network_efficiency,
                'transfer_days': transfer_days,
                'cost_breakdown': cost_breakdown,
                'compliance_reqs': compliance_reqs,
                'compliance_risks': compliance_risks,
                'business_impact': business_impact,
                'available_hours_per_day': available_hours_per_day,
                'networking_recommendations': networking_recommendations,
                'agent_analysis': agent_analysis  # Include detailed agent analysis
            }
            
        except Exception as e:
            # Return default metrics if calculation fails
            st.error(f"Error in calculation: {str(e)}")
            return {
                'data_size_tb': 1.0,
                'effective_data_gb': 1000,
                'datasync_throughput': 100,
                'theoretical_throughput': 150,
                'real_world_efficiency': 0.7,
                'optimized_throughput': 100,
                'network_efficiency': 0.7,
                'transfer_days': 10,
                'cost_breakdown': {
                    'compute': 1000, 
                    'transfer': 500, 
                    'storage': 200, 
                    'compliance': 100, 
                    'monitoring': 50, 
                    'total': 1850,
                    'pricing_source': 'Fallback',
                    'last_updated': time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime())
                },
                'compliance_reqs': [],
                'compliance_risks': [],
                'business_impact': {'score': 0.5, 'level': 'Medium', 'recommendation': 'Standard approach'},
                'available_hours_per_day': 24,
                'networking_recommendations': {
                    'primary_method': 'DataSync',
                    'secondary_method': 'S3 Transfer Acceleration',
                    'networking_option': 'Direct Connect',
                    'db_migration_tool': 'DMS',
                    'rationale': 'Default configuration recommendation',
                    'estimated_performance': {'throughput_mbps': 100, 'estimated_days': 10, 'network_efficiency': 0.7},
                    'cost_efficiency': 'Medium',
                    'risk_level': 'Low',
                    'agent_optimization': {'status': 'Unknown', 'current_agents': 1, 'recommended_agents': 1}
                },
                'agent_analysis': {'status': 'Error', 'recommendation': 'Unable to analyze'}
            }

    def _analyze_datasync_agent_optimization(self, data_size_tb, dx_bandwidth_mbps, current_agents, config):
        """Analyze DataSync agent configuration and provide optimization recommendations"""
        try:
            # Calculate optimal agent count based on various factors
            
            # Factor 1: Data size optimization
            # Rule: 1 agent per 5-10TB for optimal performance
            size_based_agents = max(1, min(10, int(data_size_tb / 7)))  # Sweet spot at 7TB per agent
            
            # Factor 2: Bandwidth optimization  
            # Rule: Each agent can effectively utilize 500-1000 Mbps
            bandwidth_based_agents = max(1, min(20, int(dx_bandwidth_mbps / 750)))  # 750 Mbps per agent optimal
            
            # Factor 3: File characteristics
            avg_file_size = config.get('avg_file_size', '10-100MB (Medium files)')
            if 'small files' in avg_file_size.lower():
                file_factor = 1.5  # More agents for small files
            elif 'large files' in avg_file_size.lower():
                file_factor = 0.8  # Fewer agents for large files
            else:
                file_factor = 1.0
            
            # Factor 4: Network conditions
            network_latency = float(config.get('network_latency', 25))
            packet_loss = float(config.get('packet_loss', 0.1))
            
            network_factor = 1.0
            if network_latency > 50:
                network_factor += 0.3  # More agents for high latency
            if packet_loss > 0.5:
                network_factor += 0.2  # More agents for packet loss
            
            # Calculate recommended agents
            recommended_agents = int((size_based_agents + bandwidth_based_agents) / 2 * file_factor * network_factor)
            recommended_agents = max(1, min(15, recommended_agents))  # Cap between 1-15
            
            # Performance analysis
            current_efficiency = self._calculate_agent_efficiency(current_agents, data_size_tb, dx_bandwidth_mbps)
            optimal_efficiency = self._calculate_agent_efficiency(recommended_agents, data_size_tb, dx_bandwidth_mbps)
            
            # Cost impact analysis
            agent_hourly_cost = self._get_agent_hourly_cost(config.get('datasync_instance_type', 'm5.large'))
            current_cost_per_hour = current_agents * agent_hourly_cost
            recommended_cost_per_hour = recommended_agents * agent_hourly_cost
            cost_change_pct = ((recommended_cost_per_hour - current_cost_per_hour) / current_cost_per_hour) * 100
            
            # Performance impact analysis
            performance_gain_pct = ((optimal_efficiency - current_efficiency) / current_efficiency) * 100
            
            # Generate recommendations
            if recommended_agents > current_agents:
                change_type = "INCREASE"
                reason = f"Scale up from {current_agents} to {recommended_agents} agents"
                if data_size_tb > 20:
                    reason += f" to handle large dataset ({data_size_tb:.1f}TB) efficiently"
                elif dx_bandwidth_mbps > 5000:
                    reason += f" to utilize high bandwidth ({dx_bandwidth_mbps} Mbps) effectively"
                else:
                    reason += " to improve parallel processing and reduce transfer time"
                    
            elif recommended_agents < current_agents:
                change_type = "DECREASE"
                reason = f"Scale down from {current_agents} to {recommended_agents} agents"
                reason += " to reduce costs without significant performance impact"
                
            else:
                change_type = "OPTIMAL"
                reason = f"Current {current_agents} agents is optimal for your workload"
            
            # Time impact analysis
            if change_type != "OPTIMAL":
                time_change_pct = -performance_gain_pct * 0.8  # Approximate time reduction
                time_impact = f"{abs(time_change_pct):.1f}% {'faster' if time_change_pct < 0 else 'slower'}"
            else:
                time_impact = "No change"
            
            # Risk assessment
            if change_type == "INCREASE" and cost_change_pct > 50:
                risk_level = "Medium - Significant cost increase"
            elif change_type == "DECREASE" and performance_gain_pct < -20:
                risk_level = "Medium - Performance degradation"
            else:
                risk_level = "Low"
            
            # Scenarios analysis
            scenarios = []
            
            # Conservative scenario (minimal change)
            conservative_agents = current_agents + (1 if change_type == "INCREASE" else (-1 if change_type == "DECREASE" else 0))
            conservative_agents = max(1, conservative_agents)
            
            scenarios.append({
                "name": "Conservative",
                "agents": conservative_agents,
                "performance_change": f"{performance_gain_pct/2:+.1f}%",
                "cost_change": f"{cost_change_pct/2:+.1f}%",
                "risk": "Low"
            })
            
            # Recommended scenario
            scenarios.append({
                "name": "Recommended",
                "agents": recommended_agents,
                "performance_change": f"{performance_gain_pct:+.1f}%",
                "cost_change": f"{cost_change_pct:+.1f}%",
                "risk": risk_level.split(" - ")[0]
            })
            
            # Aggressive scenario (maximum performance)
            aggressive_agents = min(20, recommended_agents + 2)
            aggressive_performance = performance_gain_pct * 1.3
            aggressive_cost = cost_change_pct * 1.5
            
            scenarios.append({
                "name": "Aggressive",
                "agents": aggressive_agents,
                "performance_change": f"{aggressive_performance:+.1f}%",
                "cost_change": f"{aggressive_cost:+.1f}%",
                "risk": "Medium-High"
            })
            
            return {
                "status": change_type,
                "current_agents": current_agents,
                "recommended_agents": recommended_agents,
                "change_needed": recommended_agents - current_agents,
                "reasoning": reason,
                "performance_impact": {
                    "current_efficiency": f"{current_efficiency:.1f}%",
                    "optimal_efficiency": f"{optimal_efficiency:.1f}%",
                    "performance_gain": f"{performance_gain_pct:+.1f}%",
                    "time_impact": time_impact
                },
                "cost_impact": {
                    "current_hourly_cost": f"${current_cost_per_hour:.2f}",
                    "recommended_hourly_cost": f"${recommended_cost_per_hour:.2f}",
                    "cost_change_pct": f"{cost_change_pct:+.1f}%"
                },
                "risk_assessment": risk_level,
                "scenarios": scenarios,
                "factors_considered": {
                    "data_size_factor": f"{data_size_tb:.1f}TB ‚Üí {size_based_agents} agents",
                    "bandwidth_factor": f"{dx_bandwidth_mbps} Mbps ‚Üí {bandwidth_based_agents} agents",
                    "file_size_factor": f"{avg_file_size} ‚Üí {file_factor:.1f}x multiplier",
                    "network_factor": f"Latency: {network_latency}ms, Loss: {packet_loss}% ‚Üí {network_factor:.1f}x multiplier"
                }
            }
            
        except Exception as e:
            return {
                "status": "ERROR",
                "error": str(e),
                "current_agents": current_agents,
                "recommended_agents": current_agents,
                "recommendation": "Unable to analyze agent optimization"
            }

    def _calculate_agent_efficiency(self, num_agents, data_size_tb, bandwidth_mbps):
        """Calculate efficiency percentage for given agent configuration"""
        # Efficiency factors
        size_efficiency = min(100, (num_agents * 7) / data_size_tb * 100)  # 7TB per agent optimal
        bandwidth_efficiency = min(100, (num_agents * 750) / bandwidth_mbps * 100)  # 750 Mbps per agent optimal
        
        # Diminishing returns after 10 agents
        scaling_efficiency = 100 if num_agents <= 10 else max(60, 100 - (num_agents - 10) * 5)
        
        # Overall efficiency is the harmonic mean
        overall_efficiency = 3 / (1/size_efficiency + 1/bandwidth_efficiency + 1/scaling_efficiency) * 100
        return min(100, max(10, overall_efficiency))

    def _get_agent_hourly_cost(self, instance_type):
        """Get hourly cost for DataSync agent instance"""
        instance_costs = {
            "m5.large": 0.096,
            "m5.xlarge": 0.192,
            "m5.2xlarge": 0.384,
            "m5.4xlarge": 0.768,
            "m5.8xlarge": 1.536,
            "c5.2xlarge": 0.34,
            "c5.4xlarge": 0.68,
            "c5.9xlarge": 1.53,
            "r5.2xlarge": 0.504,
            "r5.4xlarge": 1.008
        }
        return instance_costs.get(instance_type, 0.096)
     
        
    def render_network_dashboard_tab(self, config, metrics):
            """Render network dashboard tab with full functionality"""
            st.markdown('<div class="section-header">üè† Network Migration Dashboard</div>', unsafe_allow_html=True)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Data Volume", f"{metrics['data_size_tb']:.1f} TB")
            with col2:
                st.metric("Throughput", f"{metrics['optimized_throughput']:.0f} Mbps")
            with col3:
                st.metric("Duration", f"{metrics['transfer_days']:.1f} days")
            with col4:
                st.metric("Total Cost", f"${metrics['cost_breakdown']['total']:,.0f}")
            
            # AI Recommendations
            recommendations = metrics['networking_recommendations']
            st.markdown(f"""
            <div class="ai-insight">
                <strong>üß† AI Recommendation:</strong> {recommendations['rationale']}
            </div>
            """, unsafe_allow_html=True)
            
            # Performance metrics breakdown
            st.markdown('<div class="section-header">üìä Performance Analysis</div>', unsafe_allow_html=True)
            
            # Network utilization chart
            fig = go.Figure()
            
            # Current vs. Theoretical comparison
            categories = ['Current Throughput', 'Theoretical Max', 'Network Capacity']
            values = [metrics['optimized_throughput'], 
                    metrics.get('theoretical_throughput', metrics['optimized_throughput'] * 1.2),
                    config['dx_bandwidth_mbps'] * (config['dedicated_bandwidth'] / 100)]
            
            fig.add_trace(go.Bar(
                x=categories,
                y=values,
                marker_color=['lightblue', 'orange', 'lightgreen'],
                text=[f"{v:.0f} Mbps" for v in values],
                textposition='auto'
            ))
            
            fig.update_layout(
                title="Network Performance Analysis",
                yaxis_title="Throughput (Mbps)",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # ADD THIS SECTION:
            # Comprehensive analysis section
            if config.get('analyze_all_methods', False) or config.get('enable_ai_analysis', False):
                st.markdown('<div class="section-header">ü§ñ Enhanced Analysis</div>', unsafe_allow_html=True)
                self.render_comprehensive_migration_analysis(config, metrics)
            
            
            # Cost breakdown
            st.markdown('<div class="section-header">üí∞ Cost Breakdown</div>', unsafe_allow_html=True)
            
            cost_data = metrics['cost_breakdown']
            labels = ['Compute', 'Transfer', 'Storage', 'Compliance', 'Monitoring']
            values = [cost_data.get('compute', 0), cost_data.get('transfer', 0), 
                    cost_data.get('storage', 0), cost_data.get('compliance', 0), 
                    cost_data.get('monitoring', 0)]
            
            fig_cost = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
            fig_cost.update_layout(title="Cost Distribution", height=400)
            st.plotly_chart(fig_cost, use_container_width=True)
        
    def render_network_analysis_tab(self, config, metrics):
            """Render network analysis tab with full functionality"""
            st.markdown('<div class="section-header">üåê Network Analysis & Architecture Optimization</div>', unsafe_allow_html=True)
            
            # Network performance dashboard
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                utilization_pct = (metrics['optimized_throughput'] / config['dx_bandwidth_mbps']) * 100
                st.metric("Network Utilization", f"{utilization_pct:.1f}%", f"{metrics['optimized_throughput']:.0f} Mbps")
            
            with col2:
                if 'theoretical_throughput' in metrics:
                    efficiency_vs_theoretical = (metrics['optimized_throughput'] / metrics['theoretical_throughput']) * 100
                    st.metric("Real-world Efficiency", f"{efficiency_vs_theoretical:.1f}%", f"vs theoretical")
                else:
                    efficiency_improvement = ((metrics['optimized_throughput'] - metrics['datasync_throughput']) / metrics['datasync_throughput']) * 100
                    st.metric("Optimization Gain", f"{efficiency_improvement:.1f}%", "vs baseline")
            
            with col3:
                st.metric("Network Latency", f"{config['network_latency']} ms", "RTT to AWS")
            
            with col4:
                st.metric("Packet Loss", f"{config['packet_loss']}%", "Quality indicator")
            
            # AI-Powered Network Architecture Recommendations
            st.markdown('<div class="section-header">ü§ñ AI-Powered Network Architecture Recommendations</div>', unsafe_allow_html=True)
            
            recommendations = metrics['networking_recommendations']
            
            # Recommendations breakdown
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"""
                <div class="recommendation-box">
                    <h4>üéØ Recommended Configuration</h4>
                    <p><strong>Primary Method:</strong> {recommendations['primary_method']}</p>
                    <p><strong>Secondary Method:</strong> {recommendations['secondary_method']}</p>
                    <p><strong>Network Option:</strong> {recommendations['networking_option']}</p>
                    <p><strong>Database Tool:</strong> {recommendations['db_migration_tool']}</p>
                    <p><strong>Cost Efficiency:</strong> {recommendations['cost_efficiency']}</p>
                    <p><strong>Risk Level:</strong> {recommendations['risk_level']}</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="recommendation-box">
                    <h4>üìä Expected Performance</h4>
                    <p><strong>Throughput:</strong> {recommendations['estimated_performance']['throughput_mbps']:.0f} Mbps</p>
                    <p><strong>Estimated Duration:</strong> {recommendations['estimated_performance']['estimated_days']:.1f} days</p>
                    <p><strong>Network Efficiency:</strong> {recommendations['estimated_performance']['network_efficiency']:.1%}</p>
                    <p><strong>Route:</strong> {config['source_location']} ‚Üí {config['target_aws_region']}</p>
                </div>
                """, unsafe_allow_html=True)
            
            # Claude AI Rationale
            st.markdown(f"""
            <div class="ai-insight">
                <strong>üß† Claude AI Analysis:</strong> {recommendations['rationale']}
            </div>
            """, unsafe_allow_html=True)
            
            # Database Migration Tools Comparison
            st.markdown('<div class="section-header">üóÑÔ∏è Database Migration Tools Analysis</div>', unsafe_allow_html=True)
            
            db_tools_data = []
            for tool_key, tool_info in self.network_calculator.db_migration_tools.items():
                score = 85  # Base score
                if tool_key == recommendations['db_migration_tool']:
                    score = 95  # Recommended tool gets higher score
                elif len(config['database_types']) > 0 and "Database" in tool_info['best_for'][0]:
                    score = 90
                
                db_tools_data.append({
                    "Tool": tool_info['name'],
                    "Best For": ", ".join(tool_info['best_for'][:2]),
                    "Data Size Limit": tool_info['data_size_limit'],
                    "Downtime": tool_info['downtime'],
                    "Complexity": tool_info['complexity'],
                    "Recommendation Score": f"{score}%" if tool_key == recommendations['db_migration_tool'] else f"{score - 10}%",
                    "Status": "‚úÖ Recommended" if tool_key == recommendations['db_migration_tool'] else "Available"
                })
            
            df_db_tools = pd.DataFrame(db_tools_data)
            self.safe_dataframe_display(df_db_tools)
            
            # Network quality assessment
            st.markdown('<div class="section-header">üì° Network Quality Assessment</div>', unsafe_allow_html=True)
            
            utilization_pct = (metrics['optimized_throughput'] / config['dx_bandwidth_mbps']) * 100
            
            quality_metrics = pd.DataFrame({
                "Metric": ["Latency", "Jitter", "Packet Loss", "Throughput", "Geographic Route"],
                "Current": [f"{config['network_latency']} ms", f"{config['network_jitter']} ms", 
                        f"{config['packet_loss']}%", f"{metrics['optimized_throughput']:.0f} Mbps",
                        f"{config['source_location']} ‚Üí {config['target_aws_region']}"],
                "Target": ["< 50 ms", "< 10 ms", "< 0.1%", f"{config['dx_bandwidth_mbps'] * 0.8:.0f} Mbps", "Optimized"],
                "Status": [
                    "‚úÖ Good" if config['network_latency'] < 50 else "‚ö†Ô∏è High",
                    "‚úÖ Good" if config['network_jitter'] < 10 else "‚ö†Ô∏è High", 
                    "‚úÖ Good" if config['packet_loss'] < 0.1 else "‚ö†Ô∏è High",
                    "‚úÖ Good" if utilization_pct < 80 else "‚ö†Ô∏è High",
                    "‚úÖ Optimal" if recommendations['estimated_performance']['network_efficiency'] > 0.8 else "‚ö†Ô∏è Review"
                ]
            })
            
            self.safe_dataframe_display(quality_metrics)
        
    def render_network_planner_tab(self, config, metrics):
            """Render migration planner tab with full functionality"""
            st.markdown('<div class="section-header">üìä Migration Planning & Strategy</div>', unsafe_allow_html=True)
            
            # AI Recommendations at the top
            st.markdown('<div class="section-header">ü§ñ AI-Powered Migration Strategy</div>', unsafe_allow_html=True)
            recommendations = metrics['networking_recommendations']
            
            st.markdown(f"""
            <div class="ai-insight">
                <strong>üß† Claude AI Recommendation:</strong> Based on your data profile ({metrics['data_size_tb']:.1f}TB), 
                network configuration ({config['dx_bandwidth_mbps']} Mbps), and geographic location ({config['source_location']} ‚Üí {config['target_aws_region']}), 
                the optimal approach is <strong>{recommendations['primary_method']}</strong> with <strong>{recommendations['networking_option']}</strong>.
            </div>
            """, unsafe_allow_html=True)
            
            # Migration method comparison
            st.markdown('<div class="section-header">üîç Migration Method Analysis</div>', unsafe_allow_html=True)
            
            migration_methods = []
            
            # DataSync analysis
            migration_methods.append({
                "Method": f"DataSync Multi-Agent ({recommendations['primary_method']})",
                "Throughput": f"{metrics['optimized_throughput']:.0f} Mbps",
                "Duration": f"{metrics['transfer_days']:.1f} days",
                "Cost": f"${metrics['cost_breakdown']['total']:,.0f}",
                "Security": "High" if config['encryption_in_transit'] and config['encryption_at_rest'] else "Medium",
                "Complexity": "Medium",
                "AI Score": "95%" if recommendations['primary_method'] == "DataSync" else "85%"
            })
            
            # Snowball analysis
            if metrics['data_size_tb'] > 1:
                snowball_devices = max(1, int(metrics['data_size_tb'] / 72))
                snowball_days = 7 + (snowball_devices * 2)
                snowball_cost = snowball_devices * 300 + 2000
                
                migration_methods.append({
                    "Method": f"Snowball Edge ({snowball_devices}x devices)",
                    "Throughput": "Physical transfer",
                    "Duration": f"{snowball_days} days",
                    "Cost": f"${snowball_cost:,.0f}",
                    "Security": "Very High",
                    "Complexity": "Low",
                    "AI Score": "90%" if recommendations['primary_method'] == "Snowball Edge" else "75%"
                })
            
            # DMS for databases
            if config['database_types']:
                dms_days = metrics['transfer_days'] * 1.2  # DMS typically takes longer
                dms_cost = metrics['cost_breakdown']['total'] * 1.1
                
                migration_methods.append({
                    "Method": f"Database Migration Service (DMS)",
                    "Throughput": f"{metrics['optimized_throughput'] * 0.8:.0f} Mbps",
                    "Duration": f"{dms_days:.1f} days",
                    "Cost": f"${dms_cost:,.0f}",
                    "Security": "High",
                    "Complexity": "Medium",
                    "AI Score": "95%" if recommendations['db_migration_tool'] == "DMS" else "80%"
                })
            
            # Storage Gateway
            sg_throughput = min(config['dx_bandwidth_mbps'] * 0.6, 2000)
            sg_days = (metrics['effective_data_gb'] * 8) / (sg_throughput * metrics['available_hours_per_day'] * 3600) / 1000
            sg_cost = metrics['cost_breakdown']['total'] * 1.3
            
            migration_methods.append({
                "Method": "Storage Gateway (Hybrid)",
                "Throughput": f"{sg_throughput:.0f} Mbps",
                "Duration": f"{sg_days:.1f} days",
                "Cost": f"${sg_cost:,.0f}",
                "Security": "High",
                "Complexity": "Medium",
                "AI Score": "80%"
            })
            
            df_methods = pd.DataFrame(migration_methods)
            self.safe_dataframe_display(df_methods)
            
            # Geographic Optimization Analysis
            st.markdown('<div class="section-header">üó∫Ô∏è Geographic Route Optimization</div>', unsafe_allow_html=True)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Show latency comparison for different regions
                if config['source_location'] in self.network_calculator.geographic_latency:
                    latencies = self.network_calculator.geographic_latency[config['source_location']]
                    region_comparison = []
                    
                    for region, latency in latencies.items():
                        region_comparison.append({
                            "AWS Region": region,
                            "Latency (ms)": latency,
                            "Performance Impact": "Excellent" if latency < 30 else "Good" if latency < 80 else "Fair",
                            "Recommended": "‚úÖ" if region in config['target_aws_region'] else ""
                        })
                    
                    df_regions = pd.DataFrame(region_comparison)
                    self.safe_dataframe_display(df_regions)
            
            with col2:
                # Create latency comparison chart
                if config['source_location'] in self.network_calculator.geographic_latency:
                    latencies = self.network_calculator.geographic_latency[config['source_location']]
                    
                    fig_latency = go.Figure()
                    fig_latency.add_trace(go.Bar(
                        x=list(latencies.keys()),
                        y=list(latencies.values()),
                        marker_color=['lightgreen' if region in config['target_aws_region'] else 'lightblue' for region in latencies.keys()],
                        text=[f"{latency} ms" for latency in latencies.values()],
                        textposition='auto'
                    ))
                    
                    fig_latency.update_layout(
                        title=f"Network Latency from {config['source_location']}",
                        xaxis_title="AWS Region",
                        yaxis_title="Latency (ms)",
                        height=300
                    )
                    st.plotly_chart(fig_latency, use_container_width=True)
            
            # Business impact assessment
            st.markdown('<div class="section-header">üìà Business Impact Analysis</div>', unsafe_allow_html=True)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Impact Level", metrics['business_impact']['level'])
            
            with col2:
                st.metric("Impact Score", f"{metrics['business_impact']['score']:.2f}")
            
            with col3:
                timeline_status = "‚úÖ On Track" if metrics['transfer_days'] <= config['max_transfer_days'] else "‚ö†Ô∏è At Risk"
                st.metric("Timeline Status", timeline_status)
            
            st.markdown(f"""
            <div class="recommendation-box">
                <strong>üìã Migration Recommendation:</strong> {metrics['business_impact']['recommendation']}
                <br><strong>ü§ñ AI Analysis:</strong> {recommendations['rationale']}
            </div>
            """, unsafe_allow_html=True)
        
    def render_network_performance_tab(self, config, metrics):
            """Render network performance optimization tab with full functionality"""
            st.markdown('<div class="section-header">‚ö° Performance Optimization</div>', unsafe_allow_html=True)
            
            # Performance metrics
            col1, col2, col3, col4 = st.columns(4)
            
            # Calculate baseline for comparison (using theoretical mode)
            baseline_result = self.network_calculator.calculate_enterprise_throughput(
                config['datasync_instance_type'], config['num_datasync_agents'], config['avg_file_size'], 
                config['dx_bandwidth_mbps'], 100, 5, 0.05, False, config['dedicated_bandwidth'], False
            )
            baseline_throughput = baseline_result[0] if isinstance(baseline_result, tuple) else baseline_result
            
            improvement = ((metrics['optimized_throughput'] - baseline_throughput) / baseline_throughput) * 100
            
            with col1:
                st.metric("Performance Gain", f"{improvement:.1f}%", "vs baseline")
            
            with col2:
                st.metric("Network Efficiency", f"{(metrics['optimized_throughput']/config['dx_bandwidth_mbps'])*100:.1f}%")
            
            with col3:
                st.metric("Transfer Time", f"{metrics['transfer_days']:.1f} days")
            
            with col4:
                st.metric("Cost per TB", f"${metrics['cost_breakdown']['total']/metrics['data_size_tb']:.0f}")
            
            # AI-Powered Optimization Recommendations
            st.markdown('<div class="section-header">ü§ñ AI-Powered Optimization Recommendations</div>', unsafe_allow_html=True)
            recommendations = metrics['networking_recommendations']
            
            st.markdown(f"""
            <div class="ai-insight">
                <strong>üß† Claude AI Performance Analysis:</strong> Your current configuration achieves {metrics['network_efficiency']:.1%} efficiency. 
                The recommended {recommendations['primary_method']} with {recommendations['networking_option']} can deliver 
                {recommendations['estimated_performance']['throughput_mbps']:.0f} Mbps throughput.
            </div>
            """, unsafe_allow_html=True)
            
            # Optimization recommendations
            st.markdown('<div class="section-header">üéØ Specific Optimization Recommendations</div>', unsafe_allow_html=True)
            
            recommendations_list = []
            
            if config['tcp_window_size'] == "Default":
                recommendations_list.append("üîß Enable TCP window scaling (2MB) for 25-30% improvement")
            
            if config['mtu_size'] == "1500 (Standard)":
                recommendations_list.append("üì° Configure jumbo frames (9000 MTU) for 10-15% improvement")
            
            if config['network_congestion_control'] == "Cubic (Default)":
                recommendations_list.append("‚ö° Switch to BBR algorithm for 20-25% improvement")
            
            if not config['wan_optimization']:
                recommendations_list.append("üöÄ Enable WAN optimization for 25-30% improvement")
            
            if config['parallel_streams'] < 20:
                recommendations_list.append("üîÑ Increase parallel streams to 20+ for better throughput")
            
            if not config['use_transfer_acceleration']:
                recommendations_list.append("üåê Enable S3 Transfer Acceleration for 50-500% improvement")
            
            # Add AI-specific recommendations
            if recommendations['networking_option'] != "Direct Connect (Primary)":
                recommendations_list.append(f"ü§ñ AI suggests upgrading to Direct Connect for optimal performance")
            
            if recommendations['primary_method'] != "DataSync":
                recommendations_list.append(f"ü§ñ AI recommends {recommendations['primary_method']} for your workload characteristics")
            
            if recommendations_list:
                for rec in recommendations_list:
                    st.write(f"‚Ä¢ {rec}")
            else:
                st.success("‚úÖ Configuration is already well optimized!")
            
            # Performance comparison chart
            st.markdown('<div class="section-header">üìä Optimization Impact Analysis</div>', unsafe_allow_html=True)
            
            # Include AI recommendations in the chart
            optimization_scenarios = {
                "Current Config": metrics['optimized_throughput'],
                "TCP Optimized": metrics['optimized_throughput'] * 1.25 if config['tcp_window_size'] == "Default" else metrics['optimized_throughput'],
                "Network Optimized": metrics['optimized_throughput'] * 1.4 if not config['wan_optimization'] else metrics['optimized_throughput'],
                "AI Recommended": recommendations['estimated_performance']['throughput_mbps']
            }
            
            fig_opt = go.Figure()
            colors = ['lightblue', 'lightgreen', 'orange', 'gold']
            
            fig_opt.add_trace(go.Bar(
                x=list(optimization_scenarios.keys()),
                y=list(optimization_scenarios.values()),
                marker_color=colors,
                text=[f"{v:.0f} Mbps" for v in optimization_scenarios.values()],
                textposition='auto'
            ))
            
            fig_opt.update_layout(
                title="Performance Optimization Scenarios",
                yaxis_title="Throughput (Mbps)",
                height=400
            )
            st.plotly_chart(fig_opt, use_container_width=True)
            
            
                    # ADD THIS SECTION for Agent Optimization
            if 'agent_analysis' in metrics:
                st.markdown('<div class="section-header">üöÄ DataSync Agent Optimization</div>', unsafe_allow_html=True)
                
                agent_analysis = metrics['agent_analysis']
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    status_color = {
                        "INCREASE": "üîº Scale Up",
                        "DECREASE": "üîΩ Scale Down", 
                        "OPTIMAL": "‚úÖ Optimal"
                    }.get(agent_analysis['status'], "‚ùì Unknown")
                    
                    st.metric("Agent Status", status_color)
                    st.metric("Current Agents", agent_analysis['current_agents'])
                    st.metric("Recommended", agent_analysis['recommended_agents'])
                
                with col2:
                    perf_impact = agent_analysis.get('performance_impact', {})
                    st.metric("Current Efficiency", perf_impact.get('current_efficiency', 'N/A'))
                    st.metric("Optimal Efficiency", perf_impact.get('optimal_efficiency', 'N/A'))
                    st.metric("Performance Gain", perf_impact.get('performance_gain', 'N/A'))
                
                with col3:
                    cost_impact = agent_analysis.get('cost_impact', {})
                    st.metric("Current Cost/Hour", cost_impact.get('current_hourly_cost', 'N/A'))
                    st.metric("Recommended Cost", cost_impact.get('recommended_hourly_cost', 'N/A'))
                    st.metric("Cost Change", cost_impact.get('cost_change_pct', 'N/A'))
                
                # Recommendation Details
                st.markdown(f"""
                <div class="recommendation-box">
                    <h4>üéØ Agent Optimization Recommendation</h4>
                    <p><strong>Action:</strong> {agent_analysis['reasoning']}</p>
                    <p><strong>Performance Impact:</strong> {perf_impact.get('time_impact', 'No change')}</p>
                    <p><strong>Risk Level:</strong> {agent_analysis.get('risk_assessment', 'Unknown')}</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Scenarios Comparison
                if 'scenarios' in agent_analysis:
                    st.subheader("üìä Agent Configuration Scenarios")
                    
                    scenarios_data = []
                    for scenario in agent_analysis['scenarios']:
                        scenarios_data.append({
                            "Scenario": scenario['name'],
                            "Agents": scenario['agents'],
                            "Performance Change": scenario['performance_change'],
                            "Cost Change": scenario['cost_change'],
                            "Risk Level": scenario['risk']
                        })
                    
                    scenarios_df = pd.DataFrame(scenarios_data)
                    st.dataframe(scenarios_df, use_container_width=True)
            
            
            
            # DataSync Optimization Analysis
            st.markdown('<div class="section-header">üöÄ DataSync Configuration Optimization</div>', unsafe_allow_html=True)
            
            try:
                datasync_recommendations = self.network_calculator.get_intelligent_datasync_recommendations(config, metrics)
                
                col1, col2, col3 = st.columns([1, 1, 1])
                
                with col1:
                    st.markdown("**üîç Current Configuration Analysis**")
                    current_analysis = datasync_recommendations["current_analysis"]
                    
                    # Dynamic status indicators based on efficiency
                    efficiency = current_analysis['current_efficiency']
                    if efficiency >= 80:
                        efficiency_status = "üü¢ Excellent"
                        efficiency_color = "#28a745"
                    elif efficiency >= 60:
                        efficiency_status = "üü° Good"
                        efficiency_color = "#ffc107"
                    else:
                        efficiency_status = "üî¥ Needs Optimization"
                        efficiency_color = "#dc3545"
                    
                    st.markdown(f"""
                    <div style="background: {efficiency_color}20; padding: 10px; border-radius: 8px; border-left: 4px solid {efficiency_color};">
                        <strong>Current Setup:</strong> {config['num_datasync_agents']}x {config['datasync_instance_type']}<br>
                        <strong>Efficiency:</strong> {efficiency:.1f}% - {efficiency_status}<br>
                        <strong>Performance Rating:</strong> {current_analysis['performance_rating']}<br>
                        <strong>Scaling:</strong> {current_analysis['scaling_effectiveness']['scaling_rating']}
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown("**üéØ AI Optimization Recommendations**")
                    instance_rec = datasync_recommendations["recommended_instance"]
                    agent_rec = datasync_recommendations["recommended_agents"]
                    
                    # Show recommendation status
                    if instance_rec["upgrade_needed"] or agent_rec["change_needed"] != 0:
                        rec_color = "#007bff"
                        rec_status = "üîß Optimization Available"
                        
                        changes = []
                        if instance_rec["upgrade_needed"]:
                            changes.append(f"Instance: {config['datasync_instance_type']} ‚Üí {instance_rec['recommended_instance']}")
                        if agent_rec["change_needed"] != 0:
                            changes.append(f"Agents: {config['num_datasync_agents']} ‚Üí {agent_rec['recommended_agents']}")
                        
                        change_text = "<br>".join(changes)
                        
                        st.markdown(f"""
                        <div style="background: {rec_color}20; padding: 10px; border-radius: 8px; border-left: 4px solid {rec_color};">
                            <strong>{rec_status}</strong><br>
                            {change_text}<br>
                            <strong>Expected Gain:</strong> {agent_rec['performance_change_percent']:+.1f}%<br>
                            <strong>Cost Impact:</strong> {instance_rec['cost_impact_percent']:+.1f}%
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div style="background: #28a74520; padding: 10px; border-radius: 8px; border-left: 4px solid #28a745;">
                            <strong>‚úÖ Already Optimized</strong><br>
                            Configuration: {config['num_datasync_agents']}x {config['datasync_instance_type']}<br>
                            <strong>Status:</strong> Optimal for workload<br>
                            <strong>Efficiency:</strong> {efficiency:.1f}%
                        </div>
                        """, unsafe_allow_html=True)
                
                with col3:
                    st.markdown("**üìä Cost-Performance Analysis**")
                    cost_perf = datasync_recommendations["cost_performance_analysis"]
                    
                    ranking = cost_perf['efficiency_ranking']
                    if ranking <= 3:
                        rank_status = "üèÜ Top Tier"
                        rank_color = "#28a745"
                    elif ranking <= 10:
                        rank_status = "‚≠ê Good"
                        rank_color = "#ffc107"
                    else:
                        rank_status = "üìà Improvement Potential"
                        rank_color = "#dc3545"
                    
                    st.markdown(f"""
                    <div style="background: {rank_color}20; padding: 10px; border-radius: 8px; border-left: 4px solid {rank_color};">
                        <strong>Cost Efficiency:</strong><br>
                        ${cost_perf['current_cost_efficiency']:.3f} per Mbps<br>
                        <strong>Ranking:</strong> #{ranking} - {rank_status}<br>
                        <strong>Status:</strong> {'Excellent efficiency' if ranking <= 5 else 'Room for improvement'}
                    </div>
                    """, unsafe_allow_html=True)
            
            except Exception as e:
                st.warning(f"Error generating DataSync recommendations: {str(e)}")
                st.info("Basic performance analysis available")
        
    def render_network_security_tab(self, config, metrics):
            """Render network security and compliance tab with full functionality"""
            st.markdown('<div class="section-header">üîí Security & Compliance Management</div>', unsafe_allow_html=True)
            
            # Security dashboard
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                security_score = 85 + (10 if config['encryption_in_transit'] else 0) + (5 if len(config['compliance_frameworks']) > 0 else 0)
                st.metric("Security Score", f"{security_score}/100")
            
            with col2:
                compliance_score = min(100, len(config['compliance_frameworks']) * 15)
                st.metric("Compliance Coverage", f"{compliance_score}%")
            
            with col3:
                data_risk_level = {"Public": "Low", "Internal": "Medium", "Confidential": "High", "Restricted": "Very High", "Top Secret": "Critical"}
                st.metric("Data Risk Level", data_risk_level.get(config['data_classification'], "Medium"))
            
            with col4:
                st.metric("Audit Events", len(st.session_state.audit_log))
            
            # AI Security Analysis
            recommendations = metrics['networking_recommendations']
            st.markdown('<div class="section-header">ü§ñ AI Security & Compliance Analysis</div>', unsafe_allow_html=True)
            
            security_analysis = f"""
            Based on your data classification ({config['data_classification']}) and compliance requirements 
            ({', '.join(config['compliance_frameworks']) if config['compliance_frameworks'] else 'None specified'}), 
            the recommended {recommendations['primary_method']} provides appropriate security controls. 
            Risk level is assessed as {recommendations['risk_level']}.
            """
            
            st.markdown(f"""
            <div class="ai-insight">
                <strong>üß† Claude AI Security Assessment:</strong> {security_analysis}
            </div>
            """, unsafe_allow_html=True)
            
            # Security controls matrix
            st.markdown('<div class="section-header">üõ°Ô∏è Security Controls Matrix</div>', unsafe_allow_html=True)
            
            security_controls = pd.DataFrame({
                "Control": [
                    "Data Encryption in Transit",
                    "Data Encryption at Rest",
                    "Network Segmentation",
                    "Access Control (IAM)",
                    "Audit Logging",
                    "Data Loss Prevention",
                    "Incident Response Plan",
                    "Compliance Monitoring"
                ],
                "Status": [
                    "‚úÖ Enabled" if config['encryption_in_transit'] else "‚ùå Disabled",
                    "‚úÖ Enabled" if config['encryption_at_rest'] else "‚ùå Disabled",
                    "‚úÖ Enabled",
                    "‚úÖ Enabled",
                    "‚úÖ Enabled",
                    "‚ö†Ô∏è Partial",
                    "‚úÖ Enabled",
                    "‚úÖ Enabled" if config['compliance_frameworks'] else "‚ùå Disabled"
                ],
                "Compliance": [
                    "Required" if any(f in ["GDPR", "HIPAA", "PCI-DSS"] for f in config['compliance_frameworks']) else "Recommended",
                    "Required" if any(f in ["GDPR", "HIPAA", "PCI-DSS"] for f in config['compliance_frameworks']) else "Recommended",
                    "Required" if "PCI-DSS" in config['compliance_frameworks'] else "Recommended",
                    "Required",
                    "Required" if any(f in ["SOX", "HIPAA"] for f in config['compliance_frameworks']) else "Recommended",
                    "Required" if "GDPR" in config['compliance_frameworks'] else "Recommended",
                    "Required",
                    "Required" if config['compliance_frameworks'] else "Optional"
                ],
                "AI Recommendation": [
                    "‚úÖ Optimal" if config['encryption_in_transit'] else "‚ö†Ô∏è Enable",
                    "‚úÖ Optimal" if config['encryption_at_rest'] else "‚ö†Ô∏è Enable",
                    "‚úÖ Configured",
                    "‚úÖ AWS Best Practice",
                    "‚úÖ Enterprise Standard",
                    "‚ö†Ô∏è Review DLP policies",
                    "‚úÖ AWS native tools",
                    "‚úÖ Optimal" if config['compliance_frameworks'] else "‚ö†Ô∏è Define requirements"
                ]
            })
            
            self.safe_dataframe_display(security_controls)
            
            # Compliance frameworks
            if config['compliance_frameworks']:
                st.markdown('<div class="section-header">üèõÔ∏è Compliance Frameworks</div>', unsafe_allow_html=True)
                
                for framework in config['compliance_frameworks']:
                    st.markdown(f'<span class="security-badge">{framework}</span>', unsafe_allow_html=True)
            
            # Compliance risks
            if metrics['compliance_risks']:
                st.markdown('<div class="section-header">‚ö†Ô∏è Compliance Risks</div>', unsafe_allow_html=True)
                for risk in metrics['compliance_risks']:
                    st.warning(risk)
        
    def render_network_analytics_tab(self, config, metrics):
            """Render network analytics and reporting tab with full functionality"""
            st.markdown('<div class="section-header">üìà Analytics & Reporting</div>', unsafe_allow_html=True)
            
            # Performance trends chart
            st.markdown('<div class="section-header">üìä Performance Trends & Forecasting</div>', unsafe_allow_html=True)
            
            # Generate realistic performance trends
            dates = pd.date_range(start="2024-01-01", end="2024-12-31", freq="M")
            base_throughput = metrics['optimized_throughput']
            
            # Simulate historical performance with realistic factors
            historical_performance = []
            for i, date in enumerate(dates):
                month = date.month
                # Seasonal variations
                seasonal_factor = 0.85 if month in [11, 12, 1] else 1.05 if month in [6, 7, 8] else 1.0
                # Network improvements over time
                improvement_factor = 1.0 + (i * 0.02)
                # Add some realistic variance
                performance = base_throughput * seasonal_factor * improvement_factor
                performance += np.random.normal(0, performance * 0.05)
                historical_performance.append(max(0, performance))
            
            # Future predictions
            future_dates = pd.date_range(start="2025-01-01", end="2025-06-30", freq="M")
            recommendations = metrics['networking_recommendations']
            ai_baseline = recommendations['estimated_performance']['throughput_mbps']
            
            future_predictions = []
            for i, date in enumerate(future_dates):
                prediction = ai_baseline * (1.0 + i * 0.03) * (1.0 - i * 0.01)  # Growth with utilization degradation
                future_predictions.append(prediction)
            
            # Create the trends chart
            fig_trends = go.Figure()
            
            # Historical data
            fig_trends.add_trace(go.Scatter(
                x=dates,
                y=historical_performance,
                mode='lines+markers',
                name='Historical Performance',
                line=dict(color='#3498db', width=2)
            ))
            
            # Future predictions
            fig_trends.add_trace(go.Scatter(
                x=future_dates,
                y=future_predictions,
                mode='lines+markers',
                name='AI Predictions',
                line=dict(color='#e74c3c', dash='dash', width=2)
            ))
            
            # Current marker
            current_date = pd.Timestamp.now()
            fig_trends.add_trace(go.Scatter(
                x=[current_date],
                y=[metrics['optimized_throughput']],
                mode='markers',
                name='Current Config',
                marker=dict(color='#f39c12', size=12, symbol='star')
            ))
            
            fig_trends.update_layout(
                title="Performance Trends: Historical Analysis & AI Predictions",
                xaxis_title="Date",
                yaxis_title="Throughput (Mbps)",
                height=500
            )
            st.plotly_chart(fig_trends, use_container_width=True)
            
            # ROI Analysis
            st.markdown('<div class="section-header">üí° ROI Analysis with AI Insights</div>', unsafe_allow_html=True)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # Calculate annual savings
                on_premises_annual_cost = metrics['data_size_tb'] * 1000 * 12  # $1000/TB/month on-premises
                aws_annual_cost = metrics['cost_breakdown']['storage'] * 12 + (metrics['cost_breakdown']['total'] * 0.1)
                annual_savings = max(0, on_premises_annual_cost - aws_annual_cost)
                st.metric("Annual Savings", f"${annual_savings:,.0f}")
            
            with col2:
                roi_percentage = (annual_savings / metrics['cost_breakdown']['total']) * 100 if metrics['cost_breakdown']['total'] > 0 else 0
                st.metric("ROI", f"{roi_percentage:.1f}%")
            
            with col3:
                payback_period = metrics['cost_breakdown']['total'] / annual_savings if annual_savings > 0 else 0
                payback_display = f"{payback_period:.1f} years" if payback_period > 0 and payback_period < 50 else "N/A"
                st.metric("Payback Period", payback_display)
            
            # AI Business Impact Analysis
            st.markdown(f"""
            <div class="recommendation-box">
                <h4>ü§ñ AI Business Impact Analysis</h4>
                <p><strong>Business Value:</strong> The recommended migration strategy delivers {recommendations['cost_efficiency']} cost efficiency 
                with {recommendations['risk_level']} risk profile.</p>
                <p><strong>Performance Impact:</strong> Expected {recommendations['estimated_performance']['network_efficiency']:.1%} network efficiency 
                with {recommendations['estimated_performance']['throughput_mbps']:.0f} Mbps sustained throughput.</p>
                <p><strong>Strategic Recommendation:</strong> {recommendations['rationale']}</p>
            </div>
            """, unsafe_allow_html=True)
            
        # ADD this new method AFTER the render_network_analytics_tab method (around line 2200):

    def render_comprehensive_migration_analysis(self, config, metrics):
            """Render comprehensive migration analysis with all methods"""
            
            st.markdown('<div class="section-header">üìä Comprehensive Migration Analysis</div>', unsafe_allow_html=True)
            
            # Check if comprehensive analysis is enabled
            if config.get('analyze_all_methods', False):
                
                # Analyze all migration options
                migration_options = self.migration_analyzer.analyze_all_options(config)
                
                # Display comparison table
                st.subheader("üîç Migration Methods Comparison")
                
                comparison_data = []
                for method_key, method_data in migration_options.items():
                    method_info = method_data['method_info']
                    
                    comparison_data.append({
                        "Method": method_info['name'],
                        "Best For": ", ".join(method_info['best_for'][:2]),
                        "Throughput": f"{method_data['throughput_mbps']}" if isinstance(method_data['throughput_mbps'], str) else f"{method_data['throughput_mbps']:.0f} Mbps",
                        "Timeline": f"{method_data['transfer_days']:.1f} days" if isinstance(method_data['transfer_days'], (int, float)) else str(method_data['transfer_days']),
                        "Est. Cost": f"${method_data['estimated_cost']:,.0f}",
                        "Complexity": method_info['setup_complexity'],
                        "Score": f"{method_data['score']:.0f}/100",
                        "Recommendation": method_data['recommendation_level']
                    })
                
                df_comparison = pd.DataFrame(comparison_data)
                self.safe_dataframe_display(df_comparison)
                
                # Get AI analysis if enabled
                if config.get('enable_ai_analysis', False) and self.claude_ai.available:
                    st.subheader("ü§ñ Claude AI Strategic Analysis")
                    
                    ai_analysis = self.claude_ai.analyze_migration_strategy(config, metrics, migration_options)
                    
                    st.markdown(f"""
                    <div class="ai-insight">
                        <h4>üß† Claude AI Recommendation ({ai_analysis['source']})</h4>
                        <p><strong>Confidence Level:</strong> {ai_analysis['confidence']}</p>
                        <div style="white-space: pre-wrap;">{ai_analysis['analysis']}</div>
                    </div>
                    """, unsafe_allow_html=True)
            
            
        
    def render_network_conclusion_tab(self, config, metrics):
            """Render network conclusion tab with full functionality"""
            st.markdown('<div class="section-header">üéØ Final Strategic Recommendation & Executive Decision</div>', unsafe_allow_html=True)
            
            recommendations = metrics['networking_recommendations']
            
            # Calculate overall recommendation score
            performance_score = min(100, (metrics['optimized_throughput'] / 1000) * 50)
            cost_score = min(50, max(0, 50 - (metrics['cost_breakdown']['total'] / config['budget_allocated'] - 1) * 100))
            timeline_score = min(30, max(0, 30 - (metrics['transfer_days'] / config['max_transfer_days'] - 1) * 100))
            risk_score = {"Low": 20, "Medium": 15, "High": 10, "Critical": 5}.get(recommendations['risk_level'], 15)
            
            overall_score = performance_score + cost_score + timeline_score + risk_score
            
            # Determine strategy status
            if overall_score >= 140:
                strategy_status = "‚úÖ RECOMMENDED"
                strategy_action = "PROCEED"
                status_color = "success"
            elif overall_score >= 120:
                strategy_status = "‚ö†Ô∏è CONDITIONAL"
                strategy_action = "PROCEED WITH OPTIMIZATIONS"
                status_color = "warning"
            elif overall_score >= 100:
                strategy_status = "üîÑ REQUIRES MODIFICATION"
                strategy_action = "REVISE CONFIGURATION"
                status_color = "info"
            else:
                strategy_status = "‚ùå NOT RECOMMENDED"
                strategy_action = "RECONSIDER APPROACH"
                status_color = "error"
            
            # Executive Summary Section
            st.header("üìã Executive Summary")
            
            if status_color == "success":
                st.success(f"""
                **STRATEGIC RECOMMENDATION: {strategy_status}**
                
                **Action Required:** {strategy_action}
                
                **Overall Strategy Score:** {overall_score:.0f}/150
                
                **Success Probability:** {85 + (overall_score - 100) * 0.3:.0f}%
                """)
            elif status_color == "warning":
                st.warning(f"""
                **STRATEGIC RECOMMENDATION: {strategy_status}**
                
                **Action Required:** {strategy_action}
                
                **Overall Strategy Score:** {overall_score:.0f}/150
                
                **Success Probability:** {85 + (overall_score - 100) * 0.3:.0f}%
                """)
            elif status_color == "info":
                st.info(f"""
                **STRATEGIC RECOMMENDATION: {strategy_status}**
                
                **Action Required:** {strategy_action}
                
                **Overall Strategy Score:** {overall_score:.0f}/150
                
                **Success Probability:** {85 + (overall_score - 100) * 0.3:.0f}%
                """)
            else:
                st.error(f"""
                **STRATEGIC RECOMMENDATION: {strategy_status}**
                
                **Action Required:** {strategy_action}
                
                **Overall Strategy Score:** {overall_score:.0f}/150
                
                **Success Probability:** {85 + (overall_score - 100) * 0.3:.0f}%
                """)
            
            # Project Overview Metrics
            st.header("üìä Project Overview")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Project", config['project_name'])
                st.metric("Data Volume", f"{metrics['data_size_tb']:.1f} TB")
            
            with col2:
                st.metric("Expected Throughput", f"{recommendations['estimated_performance']['throughput_mbps']:.0f} Mbps")
                st.metric("Estimated Duration", f"{metrics['transfer_days']:.1f} days")
            
            with col3:
                st.metric("Total Investment", f"${metrics['cost_breakdown']['total']:,.0f}")
                st.metric("Cost per TB", f"${metrics['cost_breakdown']['total']/metrics['data_size_tb']:.0f}")
            
            with col4:
                st.metric("Risk Assessment", recommendations['risk_level'])
                st.metric("Business Impact", metrics['business_impact']['level'])
            
            # Final recommendations and next steps
            st.header("üéØ Next Steps and Implementation")
            
            next_steps = []
            
            if strategy_action == "PROCEED":
                next_steps = [
                    "1. ‚úÖ Finalize migration timeline and resource allocation",
                    "2. üîß Implement recommended DataSync configuration", 
                    "3. üåê Configure network optimizations (TCP, MTU, WAN)",
                    "4. üîí Set up security controls and compliance monitoring",
                    "5. üìä Establish performance monitoring and alerting",
                    "6. üöÄ Begin pilot migration with non-critical data",
                    "7. üìà Scale to full production migration"
                ]
            elif strategy_action == "PROCEED WITH OPTIMIZATIONS":
                next_steps = [
                    "1. ‚ö†Ô∏è Address identified performance bottlenecks",
                    "2. üí∞ Review and optimize cost configuration",
                    "3. üîß Implement AI-recommended instance upgrades",
                    "4. üåê Upgrade network bandwidth if needed",
                    "5. ‚úÖ Re-validate configuration and projections", 
                    "6. üìä Begin controlled pilot migration",
                    "7. üìà Monitor and adjust based on results"
                ]
            else:
                next_steps = [
                    "1. üîÑ Review and modify current configuration",
                    "2. üìä Reassess data size and transfer requirements",
                    "3. üåê Evaluate network infrastructure upgrades",
                    "4. üí∞ Adjust budget allocation and timeline",
                    "5. ü§ñ Recalculate with AI recommendations",
                    "6. ‚úÖ Validate revised approach",
                    "7. üìã Restart planning with optimized settings"
                ]
            
            st.info("**Recommended Next Steps:**")
            for step in next_steps:
                st.write(step)
            
            st.success("üéØ **Network migration analysis complete!** Use the recommendations above to proceed with your AWS migration strategy.")
        
        # =========================================================================
        # DATABASE MIGRATION METHODS (Simplified)
        # =========================================================================
        
    def render_database_migration_platform(self):
            """Render the complete database migration platform"""
            st.markdown('<div class="section-header">üóÑÔ∏è Database Migration Platform</div>', unsafe_allow_html=True)
            
            # FIXED: Updated database platform navigation with vROps and Bulk Upload
            col1, col2, col3, col4, col5, col6, col7, col8 = st.columns(8)
            
            with col1:
                if st.button("‚öôÔ∏è Configuration", key="db_nav_config"):
                    st.session_state.active_database_tab = "configuration"
            with col2:
                if st.button("üìä Sizing Analysis", key="db_nav_sizing"):
                    st.session_state.active_database_tab = "sizing"
            with col3:
                if st.button("üí∞ Cost Analysis", key="db_nav_cost"):
                    st.session_state.active_database_tab = "cost"
            with col4:
                if st.button("‚ö†Ô∏è Risk Assessment", key="db_nav_risk"):
                    st.session_state.active_database_tab = "risk"
            with col5:
                if st.button("üìã Migration Plan", key="db_nav_plan"):
                    st.session_state.active_database_tab = "plan"
            with col6:
                if st.button("üìà Dashboard", key="db_nav_dashboard"):
                    st.session_state.active_database_tab = "dashboard"
            with col7:
                if st.button("üì§ Bulk Upload", key="db_nav_bulk"):  # NEW
                    st.session_state.active_database_tab = "bulk_upload"
            with col8:
                if st.button("üîå vROps Integration", key="db_nav_vrops"):  # NEW
                    st.session_state.active_database_tab = "vrops"
            
            # Render appropriate database tab
            if st.session_state.active_database_tab == "configuration":
                self.render_database_configuration_tab()
            elif st.session_state.active_database_tab == "sizing":
                self.render_database_sizing_tab()
            elif st.session_state.active_database_tab == "cost":
                self.render_database_cost_tab()
            elif st.session_state.active_database_tab == "risk":
                self.render_database_risk_tab()
            elif st.session_state.active_database_tab == "plan":
                self.render_database_plan_tab()
            elif st.session_state.active_database_tab == "dashboard":
                self.render_database_dashboard_tab()
            elif st.session_state.active_database_tab == "bulk_upload":  # NEW
                self.render_bulk_upload_tab()
            elif st.session_state.active_database_tab == "vrops":  # NEW
                self.render_vrops_integration_tab()
                
    def render_database_configuration_tab(self):
            """Render database configuration tab"""
            st.markdown('<div class="section-header">‚öôÔ∏è Database Migration Configuration</div>', unsafe_allow_html=True)
            
            # Project Information
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("üìã Project Information")
                project_name = st.text_input("Project Name", value="DB Migration 2025")
                source_database = st.selectbox("Source Database", 
                    ["Microsoft SQL Server", "PostgreSQL", "Oracle", "MySQL", "Other"])
                target_platform = st.selectbox("Target Platform",
                    ["Amazon RDS", "Amazon Aurora", "EC2 with Database", "Hybrid"])
                migration_type = st.selectbox("Migration Type",
                    ["Homogeneous", "Heterogeneous"])
                environment = st.selectbox("Environment",
                    ["Development", "QA", "SQA", "Production"])
            
            with col2:
                st.subheader("üìä Database Characteristics")
                database_size_gb = st.number_input("Database Size (GB)", min_value=1, max_value=100000, value=500)
                workload_type = st.selectbox("Workload Type",
                    ["OLTP", "OLAP", "Mixed", "Analytics", "Reporting"])
                concurrent_connections = st.number_input("Peak Concurrent Connections", min_value=1, max_value=10000, value=200)
                transactions_per_second = st.number_input("Peak TPS", min_value=1, max_value=100000, value=2000)
                read_query_percentage = st.slider("Read Query Percentage", min_value=0, max_value=100, value=70)
            
            # Advanced Configuration
            st.subheader("üîß Advanced Configuration")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**Performance Requirements**")
                schema_complexity = st.selectbox("Schema Complexity", ["Simple", "Moderate", "Complex"])
                high_availability = st.checkbox("Multi-AZ Required", value=True)
                encryption_required = st.checkbox("Encryption Required", value=True)
                backup_retention_days = st.number_input("Backup Retention (Days)", min_value=1, max_value=365, value=30)
            
            with col2:
                st.markdown("**Growth & Scaling**")
                annual_growth_rate = st.slider("Annual Growth Rate (%)", min_value=0, max_value=100, value=20) / 100
                auto_scaling_enabled = st.checkbox("Auto Scaling", value=True)
                read_replica_scaling = st.checkbox("Auto Read Replica Scaling", value=True)
                
            with col3:
                st.markdown("**Migration Parameters**")
                downtime_tolerance = st.selectbox("Downtime Tolerance", ["Zero", "Low", "Medium", "High"])
                migration_method = st.selectbox("Preferred Migration Method", ["DMS", "Native Tools", "Hybrid"])
                data_sync_method = st.selectbox("Data Sync Method", ["Continuous", "Batch", "One-time"])
            
            # Business Configuration
            st.subheader("üíº Business Configuration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                business_criticality = st.selectbox("Business Criticality", ["Low", "Medium", "High", "Critical"])
                user_base_size = st.selectbox("User Base Size", ["Small", "Medium", "Large", "Enterprise"])
                compliance_frameworks = st.multiselect("Compliance Requirements",
                    ["SOX", "GDPR", "HIPAA", "PCI-DSS", "SOC2", "ISO27001", "FedRAMP"])
            
            with col2:
                team_experience = st.selectbox("Team Experience Level", ["Expert", "Experienced", "Novice"])
                rollback_plan = st.selectbox("Rollback Plan", ["Comprehensive", "Basic", "None"])
                testing_strategy = st.selectbox("Testing Strategy", ["Comprehensive", "Adequate", "Minimal"])
            
            # Network Configuration
            st.subheader("üåê Network Configuration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                connectivity_type = st.selectbox("Connectivity Type",
                    ["Direct Connect", "VPN", "Public Internet", "AWS PrivateLink"])
                target_region = st.selectbox("Target AWS Region",
                    ["us-east-1", "us-east-2", "us-west-1", "us-west-2", "eu-west-1", "eu-central-1"])
            
            with col2:
                vpc_endpoints = st.checkbox("Use VPC Endpoints", value=True)
                enhanced_monitoring = st.checkbox("Enhanced Monitoring", value=True)
            
            # Compile configuration
            config = {
                "project_name": project_name,
                "source_database": source_database,
                "target_platform": target_platform,
                "migration_type": migration_type,
                "environment": environment,
                "database_size_gb": database_size_gb,
                "workload_type": workload_type,
                "concurrent_connections": concurrent_connections,
                "transactions_per_second": transactions_per_second,
                "read_query_percentage": read_query_percentage,
                "schema_complexity": schema_complexity,
                "high_availability": high_availability,
                "encryption_required": encryption_required,
                "backup_retention_days": backup_retention_days,
                "annual_growth_rate": annual_growth_rate,
                "auto_scaling_enabled": auto_scaling_enabled,
                "read_replica_scaling": read_replica_scaling,
                "downtime_tolerance": downtime_tolerance,
                "migration_method": migration_method,
                "data_sync_method": data_sync_method,
                "connectivity_type": connectivity_type,
                "target_region": target_region,
                "vpc_endpoints": vpc_endpoints,
                "enhanced_monitoring": enhanced_monitoring,
                "business_criticality": business_criticality,
                "user_base_size": user_base_size,
                "compliance_frameworks": compliance_frameworks,
                "team_experience": team_experience,
                "rollback_plan": rollback_plan,
                "testing_strategy": testing_strategy
            }
            
            # Save configuration and run analysis
            if st.button("üöÄ Run Database Analysis", type="primary"):
                with st.spinner("Running comprehensive database migration analysis..."):
                    # Perform sizing analysis
                    sizing_config = self.database_sizing_engine.calculate_sizing_requirements(config)
                    
                    # Store analysis results
                    st.session_state.current_database_analysis = {
                        "config": config,
                        "sizing": sizing_config,
                        "timestamp": datetime.now()
                    }
                    
                    # Log the analysis
                    self.log_audit_event("DB_ANALYSIS_COMPLETED", f"Database analysis for {project_name}")
                    
                    st.success("‚úÖ Database analysis completed! Navigate to other tabs to view results.")
        
        # Simplified database tab renderers
    def render_database_sizing_tab(self):
        """Render comprehensive database sizing analysis tab"""
        if not st.session_state.current_database_analysis:
            st.warning("‚ö†Ô∏è Please run analysis in Configuration tab first.")
            return
        
        st.markdown('<div class="section-header">üìä AI-Powered Database Sizing Analysis</div>', unsafe_allow_html=True)
        
        analysis = st.session_state.current_database_analysis
        config = analysis['config']
        sizing = analysis['sizing']
        
        # Executive Summary
        st.markdown('<div class="section-header">üìã Executive Summary</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Database Size", f"{config['database_size_gb']:,} GB")
            st.metric("Workload Type", config['workload_type'])
        
        with col2:
            st.metric("Recommended Writer", sizing['writer_instance']['instance_type'])
            st.metric("CPU Utilization", f"{sizing['writer_instance']['cpu_utilization']:.1f}%")
        
        with col3:
            st.metric("Memory Utilization", f"{sizing['writer_instance']['memory_utilization']:.1f}%")
            st.metric("Read Replicas", len(sizing['reader_instances']))
        
        with col4:
            monthly_cost = sizing['writer_instance']['specs']['cost_factor'] * 24 * 30
            st.metric("Est. Monthly Cost", f"${monthly_cost:,.0f}")
            st.metric("Environment", config['environment'])
        
        # AI Analysis Display
        st.markdown(f"""
        <div class="ai-insight">
            <strong>üß† AI Sizing Rationale:</strong> {sizing['sizing_rationale']}
        </div>
        """, unsafe_allow_html=True)
        
        # Writer Instance Details
        st.markdown('<div class="section-header">üñ•Ô∏è Writer Instance Configuration</div>', unsafe_allow_html=True)
        
        writer = sizing['writer_instance']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown(f"""
            <div class="recommendation-box">
                <h4>üìä Recommended Writer Instance</h4>
                <p><strong>Instance Type:</strong> {writer['instance_type']}</p>
                <p><strong>vCPUs:</strong> {writer['specs']['vcpu']}</p>
                <p><strong>Memory:</strong> {writer['specs']['memory']} GB</p>
                <p><strong>Network:</strong> {writer['specs']['network']}</p>
                <p><strong>Use Case:</strong> {writer['specs']['use_case']}</p>
                <p><strong>Efficiency Score:</strong> {writer['efficiency_score']:.2f}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="recommendation-box">
                <h4>üìà Resource Utilization</h4>
                <p><strong>CPU Utilization:</strong> {writer['cpu_utilization']:.1f}%</p>
                <p><strong>Memory Utilization:</strong> {writer['memory_utilization']:.1f}%</p>
                <p><strong>Peak Connections:</strong> {config['concurrent_connections']:,}</p>
                <p><strong>Peak TPS:</strong> {config['transactions_per_second']:,}</p>
                <p><strong>Read/Write Ratio:</strong> {config['read_query_percentage']}% reads</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Read Replicas Configuration
        if sizing['reader_instances']:
            st.markdown('<div class="section-header">üìñ Read Replica Configuration</div>', unsafe_allow_html=True)
            
            replica_data = []
            for i, replica in enumerate(sizing['reader_instances']):
                replica_data.append({
                    "Replica": f"Read Replica {i+1}",
                    "Instance Type": replica['instance_type'],
                    "Role": replica['role'],
                    "Cross-AZ": "Yes" if replica.get('cross_az', False) else "No",
                    "vCPUs": replica['specs']['vcpu'],
                    "Memory (GB)": replica['specs']['memory'],
                    "Est. Cost/Month": f"${replica['specs']['cost_factor'] * 24 * 30:,.0f}"
                })
            
            replica_df = pd.DataFrame(replica_data)
            self.safe_dataframe_display(replica_df)
        
        # Storage Configuration
        st.markdown('<div class="section-header">üíæ Storage Configuration</div>', unsafe_allow_html=True)
        
        storage = sizing['storage_config']
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <h4>üì¶ Storage Specifications</h4>
                <p><strong>Storage Type:</strong> {storage['storage_type']}</p>
                <p><strong>Allocated Storage:</strong> {storage['allocated_storage_gb']:,} GB</p>
                <p><strong>Provisioned IOPS:</strong> {storage['provisioned_iops']:,}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="metric-card">
                <h4>üîí Backup & Security</h4>
                <p><strong>Backup Retention:</strong> {storage['backup_retention_days']} days</p>
                <p><strong>Multi-AZ:</strong> {'Yes' if storage['multi_az'] else 'No'}</p>
                <p><strong>Encryption:</strong> {'Yes' if storage['encryption_enabled'] else 'No'}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div class="metric-card">
                <h4>üìã Maintenance</h4>
                <p><strong>Snapshot Frequency:</strong> {storage['snapshot_frequency']}</p>
                <p><strong>Availability:</strong> {sizing['environment_config']['availability']}</p>
                <p><strong>Scaling Factor:</strong> {sizing['environment_config']['scaling_factor']}</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Growth Projections
        st.markdown('<div class="section-header">üìà Growth Projections</div>', unsafe_allow_html=True)
        
        growth_data = []
        for year_key, projection in sizing['growth_projections'].items():
            year_num = year_key.split('_')[1]
            growth_data.append({
                "Year": f"Year {year_num}",
                "Projected Storage (GB)": f"{projection['storage_gb']:,}",
                "Growth Factor": f"{projection['growth_factor']:.2f}x",
                "Compute Recommendation": projection['compute_recommendation'],
                "Est. Cost Increase": projection['estimated_cost_increase']
            })
        
        growth_df = pd.DataFrame(growth_data)
        self.safe_dataframe_display(growth_df)
        
        # Sizing Comparison Chart
        st.markdown('<div class="section-header">üìä Instance Comparison</div>', unsafe_allow_html=True)
        
        # Create comparison with current selection and alternatives
        instance_options = ["db.m5.large", "db.m5.xlarge", "db.m5.2xlarge", "db.r5.large", "db.r5.xlarge"]
        instance_costs = [4.2, 8.4, 16.8, 5.5, 11.0]  # Cost factors
        
        fig_comparison = go.Figure()
        
        colors = ['gold' if inst == writer['instance_type'] else 'lightblue' for inst in instance_options]
        
        fig_comparison.add_trace(go.Bar(
            x=instance_options,
            y=instance_costs,
            marker_color=colors,
            text=[f"${cost * 24 * 30:.0f}/mo" for cost in instance_costs],
            textposition='auto'
        ))
        
        fig_comparison.update_layout(
            title="Instance Type Cost Comparison (Recommended in Gold)",
            xaxis_title="Instance Type",
            yaxis_title="Monthly Cost ($)",
            height=400
        )
        st.plotly_chart(fig_comparison, use_container_width=True)
        
    def render_database_cost_tab(self):
        """Render comprehensive database cost analysis tab"""
        if not st.session_state.current_database_analysis:
            st.warning("‚ö†Ô∏è Please run analysis in Configuration tab first.")
            return
        
        st.markdown('<div class="section-header">üí∞ Comprehensive Database Cost Analysis</div>', unsafe_allow_html=True)
        
        analysis = st.session_state.current_database_analysis
        config = analysis['config']
        sizing = analysis['sizing']
        
        # Calculate comprehensive costs
        writer = sizing['writer_instance']
        readers = sizing['reader_instances']
        storage = sizing['storage_config']
        
        # Monthly costs calculation
        writer_monthly_cost = writer['specs']['cost_factor'] * 24 * 30
        
        reader_monthly_cost = 0
        for reader in readers:
            reader_monthly_cost += reader['specs']['cost_factor'] * 24 * 30
        
        # Storage costs (simplified calculation)
        storage_monthly_cost = storage['allocated_storage_gb'] * 0.10  # $0.10 per GB/month for gp3
        
        if storage['storage_type'] in ['io1', 'io2']:
            storage_monthly_cost += storage['provisioned_iops'] * 0.065  # IOPS pricing
        
        # Backup costs
        backup_storage_gb = storage['allocated_storage_gb'] * 1.2  # 20% overhead for backups
        backup_monthly_cost = backup_storage_gb * 0.095  # Backup storage pricing
        
        # Data transfer costs (estimated)
        data_transfer_monthly_cost = 100  # Estimated based on usage
        
        # Additional services
        monitoring_cost = 30 if config.get('enhanced_monitoring', False) else 0
        compliance_cost = len(config.get('compliance_frameworks', [])) * 200
        
        total_monthly_cost = (writer_monthly_cost + reader_monthly_cost + storage_monthly_cost + 
                            backup_monthly_cost + data_transfer_monthly_cost + monitoring_cost + compliance_cost)
        
        # Cost Dashboard
        st.markdown('<div class="section-header">üìä Cost Dashboard</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Monthly Total", f"${total_monthly_cost:,.0f}")
            st.metric("Annual Total", f"${total_monthly_cost * 12:,.0f}")
        
        with col2:
            st.metric("Compute Cost", f"${writer_monthly_cost + reader_monthly_cost:,.0f}")
            st.metric("Storage Cost", f"${storage_monthly_cost:,.0f}")
        
        with col3:
            cost_per_gb = total_monthly_cost / config['database_size_gb']
            st.metric("Cost per GB", f"${cost_per_gb:.2f}")
            st.metric("Cost per User", f"${total_monthly_cost / max(config['concurrent_connections'], 1):.2f}")
        
        with col4:
            # Compare with on-premises (estimated)
            on_prem_monthly = config['database_size_gb'] * 0.50  # $0.50 per GB on-premises
            savings = max(0, on_prem_monthly - total_monthly_cost)
            st.metric("vs On-Premises", f"${savings:,.0f} saved/mo" if savings > 0 else f"${abs(savings):,.0f} more/mo")
            roi = (savings * 12) / (total_monthly_cost * 12) * 100 if total_monthly_cost > 0 else 0
            st.metric("Annual ROI", f"{roi:.1f}%")
        
        # Cost Breakdown Chart
        st.markdown('<div class="section-header">üìä Cost Breakdown</div>', unsafe_allow_html=True)
        
        cost_categories = {
            'Writer Instance': writer_monthly_cost,
            'Read Replicas': reader_monthly_cost,
            'Storage': storage_monthly_cost,
            'Backups': backup_monthly_cost,
            'Data Transfer': data_transfer_monthly_cost,
            'Monitoring': monitoring_cost,
            'Compliance': compliance_cost
        }
        
        # Filter out zero costs
        cost_categories = {k: v for k, v in cost_categories.items() if v > 0}
        
        fig_pie = go.Figure(data=[go.Pie(
            labels=list(cost_categories.keys()),
            values=list(cost_categories.values()),
            hole=.3
        )])
        
        fig_pie.update_layout(title="Monthly Cost Distribution", height=500)
        st.plotly_chart(fig_pie, use_container_width=True)
        
        # Detailed Cost Table
        st.markdown('<div class="section-header">üìã Detailed Cost Breakdown</div>', unsafe_allow_html=True)
        
        cost_details = []
        
        cost_details.append({
            "Component": "Writer Instance",
            "Type": writer['instance_type'],
            "Specification": f"{writer['specs']['vcpu']} vCPU, {writer['specs']['memory']} GB RAM",
            "Monthly Cost": f"${writer_monthly_cost:,.0f}",
            "Annual Cost": f"${writer_monthly_cost * 12:,.0f}"
        })
        
        for i, reader in enumerate(readers):
            cost_details.append({
                "Component": f"Read Replica {i+1}",
                "Type": reader['instance_type'],
                "Specification": f"{reader['specs']['vcpu']} vCPU, {reader['specs']['memory']} GB RAM",
                "Monthly Cost": f"${reader['specs']['cost_factor'] * 24 * 30:,.0f}",
                "Annual Cost": f"${reader['specs']['cost_factor'] * 24 * 30 * 12:,.0f}"
            })
        
        cost_details.append({
            "Component": "Storage",
            "Type": storage['storage_type'],
            "Specification": f"{storage['allocated_storage_gb']:,} GB, {storage['provisioned_iops']:,} IOPS",
            "Monthly Cost": f"${storage_monthly_cost:,.0f}",
            "Annual Cost": f"${storage_monthly_cost * 12:,.0f}"
        })
        
        cost_details.append({
            "Component": "Backup Storage",
            "Type": "Automated Backups",
            "Specification": f"{backup_storage_gb:,.0f} GB, {storage['backup_retention_days']} day retention",
            "Monthly Cost": f"${backup_monthly_cost:,.0f}",
            "Annual Cost": f"${backup_monthly_cost * 12:,.0f}"
        })
        
        if monitoring_cost > 0:
            cost_details.append({
                "Component": "Enhanced Monitoring",
                "Type": "CloudWatch Enhanced",
                "Specification": "Detailed metrics and performance insights",
                "Monthly Cost": f"${monitoring_cost:,.0f}",
                "Annual Cost": f"${monitoring_cost * 12:,.0f}"
            })
        
        if compliance_cost > 0:
            cost_details.append({
                "Component": "Compliance Tools",
                "Type": "Automated Compliance",
                "Specification": f"{len(config.get('compliance_frameworks', []))} frameworks",
                "Monthly Cost": f"${compliance_cost:,.0f}",
                "Annual Cost": f"${compliance_cost * 12:,.0f}"
            })
        
        cost_df = pd.DataFrame(cost_details)
        self.safe_dataframe_display(cost_df)
        
        # Cost Optimization Recommendations
        st.markdown('<div class="section-header">üí° Cost Optimization Recommendations</div>', unsafe_allow_html=True)
        
        optimizations = []
        
        if writer['cpu_utilization'] < 50:
            smaller_instance = "db.m5.large" if "xlarge" in writer['instance_type'] else writer['instance_type']
            optimizations.append(f"üí∞ Consider downsizing writer instance to {smaller_instance} (CPU utilization: {writer['cpu_utilization']:.1f}%)")
        
        if len(readers) > 2 and config['read_query_percentage'] < 70:
            optimizations.append(f"üí∞ Consider reducing read replicas from {len(readers)} to 2 (read queries: {config['read_query_percentage']}%)")
        
        if storage['storage_type'] == 'io2' and storage['provisioned_iops'] > 16000:
            optimizations.append(f"üí∞ Consider using gp3 storage instead of io2 for cost savings")
        
        if not config.get('auto_scaling_enabled', False):
            optimizations.append(f"üìà Enable auto-scaling to optimize costs during low-usage periods")
        
        if storage['backup_retention_days'] > 30 and config['environment'] != 'Production':
            optimizations.append(f"üí∞ Reduce backup retention to 7 days for non-production environments")
        
        optimizations.append(f"üí∞ Consider Reserved Instances for 30-60% cost savings on predictable workloads")
        optimizations.append(f"üìä Enable cost monitoring and budget alerts")
        
        for opt in optimizations:
            st.write(f"‚Ä¢ {opt}")
        
        # Cost Projection Chart
        st.markdown('<div class="section-header">üìà Cost Growth Projections</div>', unsafe_allow_html=True)
        
        # Create cost projection over 3 years
        months = list(range(1, 37))  # 3 years
        growth_rate = config.get('annual_growth_rate', 0.2)
        monthly_growth_rate = growth_rate / 12
        
        projected_costs = []
        for month in months:
            projected_cost = total_monthly_cost * (1 + monthly_growth_rate) ** month
            projected_costs.append(projected_cost)
        
        fig_projection = go.Figure()
        
        fig_projection.add_trace(go.Scatter(
            x=months,
            y=projected_costs,
            mode='lines+markers',
            name='Projected Costs',
            line=dict(color='#e74c3c', width=2)
        ))
        
        fig_projection.update_layout(
            title="3-Year Cost Projection",
            xaxis_title="Month",
            yaxis_title="Monthly Cost ($)",
            height=400
        )
        st.plotly_chart(fig_projection, use_container_width=True)
        
    def render_database_risk_tab(self):
        """Render comprehensive database risk assessment tab"""
        if not st.session_state.current_database_analysis:
            st.warning("‚ö†Ô∏è Please run analysis in Configuration tab first.")
            return
        
        st.markdown('<div class="section-header">‚ö†Ô∏è Comprehensive Risk Assessment</div>', unsafe_allow_html=True)
        
        analysis = st.session_state.current_database_analysis
        config = analysis['config']
        sizing = analysis['sizing']
        
        # Calculate overall risk score
        risk_factors = {}
        total_risk_score = 0
        
        # Technical risks
        if config['database_size_gb'] > 10000:
            risk_factors['Large Database Size'] = {'score': 15, 'level': 'Medium', 'impact': 'Migration complexity and time'}
            total_risk_score += 15
        
        if config['schema_complexity'] == 'Complex':
            risk_factors['Schema Complexity'] = {'score': 20, 'level': 'High', 'impact': 'Conversion and testing challenges'}
            total_risk_score += 20
        
        if config['migration_type'] == 'Heterogeneous':
            risk_factors['Heterogeneous Migration'] = {'score': 25, 'level': 'High', 'impact': 'Data type conversions and compatibility'}
            total_risk_score += 25
        
        if config['downtime_tolerance'] == 'Zero':
            risk_factors['Zero Downtime Requirement'] = {'score': 30, 'level': 'Critical', 'impact': 'Complex migration strategy required'}
            total_risk_score += 30
        
        # Business risks
        if config['business_criticality'] == 'Critical':
            risk_factors['Business Criticality'] = {'score': 20, 'level': 'High', 'impact': 'High business impact of failures'}
            total_risk_score += 20
        
        if config['team_experience'] == 'Novice':
            risk_factors['Team Experience'] = {'score': 25, 'level': 'High', 'impact': 'Potential execution challenges'}
            total_risk_score += 25
        
        if config['rollback_plan'] == 'None':
            risk_factors['No Rollback Plan'] = {'score': 35, 'level': 'Critical', 'impact': 'No recovery option if migration fails'}
            total_risk_score += 35
        
        # Compliance risks
        if config['compliance_frameworks'] and not config.get('encryption_required', False):
            risk_factors['Encryption Not Enabled'] = {'score': 20, 'level': 'High', 'impact': 'Compliance violations'}
            total_risk_score += 20
        
        # Performance risks
        writer = sizing['writer_instance']
        if writer['cpu_utilization'] > 80:
            risk_factors['High CPU Utilization'] = {'score': 15, 'level': 'Medium', 'impact': 'Performance bottlenecks'}
            total_risk_score += 15
        
        if writer['memory_utilization'] > 85:
            risk_factors['High Memory Utilization'] = {'score': 15, 'level': 'Medium', 'impact': 'Memory pressure and swapping'}
            total_risk_score += 15
        
        # Determine overall risk level
        if total_risk_score >= 80:
            overall_risk = 'Critical'
            risk_color = '#dc3545'
        elif total_risk_score >= 60:
            overall_risk = 'High'
            risk_color = '#fd7e14'
        elif total_risk_score >= 40:
            overall_risk = 'Medium'
            risk_color = '#ffc107'
        else:
            overall_risk = 'Low'
            risk_color = '#28a745'
        
        # Risk Dashboard
        st.markdown('<div class="section-header">üìä Risk Dashboard</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Overall Risk Level", overall_risk)
            st.metric("Risk Score", f"{total_risk_score}/200")
        
        with col2:
            technical_risks = sum(1 for rf in risk_factors.values() if 'Migration' in str(rf) or 'Database' in str(rf) or 'Schema' in str(rf))
            st.metric("Technical Risks", str(technical_risks))
            business_risks = sum(1 for rf in risk_factors.values() if 'Business' in str(rf) or 'Team' in str(rf) or 'Rollback' in str(rf))
            st.metric("Business Risks", str(business_risks))
        
        with col3:
            critical_risks = sum(1 for rf in risk_factors.values() if rf['level'] == 'Critical')
            st.metric("Critical Risks", str(critical_risks))
            high_risks = sum(1 for rf in risk_factors.values() if rf['level'] == 'High')
            st.metric("High Risks", str(high_risks))
        
        with col4:
            success_probability = max(20, 100 - total_risk_score)
            st.metric("Success Probability", f"{success_probability}%")
            mitigation_required = "Yes" if total_risk_score > 40 else "Optional"
            st.metric("Mitigation Required", mitigation_required)
        
        # Overall Risk Status
        st.markdown(f"""
        <div class="risk-{overall_risk.lower()}" style="background: {risk_color}20; border-left: 4px solid {risk_color}; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h4>üéØ Overall Risk Assessment: {overall_risk} Risk</h4>
            <p><strong>Risk Score:</strong> {total_risk_score}/200</p>
            <p><strong>Success Probability:</strong> {success_probability}%</p>
            <p><strong>Recommendation:</strong> {'Immediate mitigation required' if total_risk_score > 80 else 'Proceed with risk mitigation plan' if total_risk_score > 40 else 'Acceptable risk level'}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Detailed Risk Analysis
        st.markdown('<div class="section-header">üìã Detailed Risk Analysis</div>', unsafe_allow_html=True)
        
        if risk_factors:
            risk_data = []
            for risk_name, risk_info in risk_factors.items():
                risk_data.append({
                    "Risk Factor": risk_name,
                    "Risk Level": risk_info['level'],
                    "Risk Score": f"{risk_info['score']}/50",
                    "Potential Impact": risk_info['impact'],
                    "Priority": "üî¥ Critical" if risk_info['level'] == 'Critical' else 
                            "üü† High" if risk_info['level'] == 'High' else 
                            "üü° Medium" if risk_info['level'] == 'Medium' else "üü¢ Low"
                })
            
            risk_df = pd.DataFrame(risk_data)
            self.safe_dataframe_display(risk_df)
        else:
            st.success("‚úÖ No significant risks identified!")
        
        # Risk Mitigation Strategies
        st.markdown('<div class="section-header">üõ°Ô∏è Risk Mitigation Strategies</div>', unsafe_allow_html=True)
        
        mitigation_strategies = []
        
        if 'Large Database Size' in risk_factors:
            mitigation_strategies.append({
                "Risk": "Large Database Size",
                "Strategy": "Implement phased migration approach",
                "Action Items": [
                    "Break migration into smaller chunks",
                    "Use parallel data loading",
                    "Implement incremental migration",
                    "Set up monitoring for each phase"
                ],
                "Timeline": "2-4 weeks additional preparation"
            })
        
        if 'Schema Complexity' in risk_factors:
            mitigation_strategies.append({
                "Risk": "Schema Complexity",
                "Strategy": "Comprehensive schema analysis and testing",
                "Action Items": [
                    "Perform detailed schema mapping",
                    "Create automated conversion scripts",
                    "Implement comprehensive testing",
                    "Engage database experts"
                ],
                "Timeline": "3-6 weeks additional preparation"
            })
        
        if 'Heterogeneous Migration' in risk_factors:
            mitigation_strategies.append({
                "Risk": "Heterogeneous Migration",
                "Strategy": "Use AWS Database Migration Service (DMS)",
                "Action Items": [
                    "Set up DMS replication instance",
                    "Create and test conversion rules",
                    "Perform data type mapping",
                    "Test application compatibility"
                ],
                "Timeline": "4-8 weeks for full testing"
            })
        
        if 'Zero Downtime Requirement' in risk_factors:
            mitigation_strategies.append({
                "Risk": "Zero Downtime Requirement",
                "Strategy": "Implement continuous data replication",
                "Action Items": [
                    "Set up real-time replication",
                    "Implement application-level failover",
                    "Test cutover procedures",
                    "Create rollback mechanisms"
                ],
                "Timeline": "6-10 weeks preparation"
            })
        
        if 'Team Experience' in risk_factors:
            mitigation_strategies.append({
                "Risk": "Limited Team Experience",
                "Strategy": "Training and expert consultation",
                "Action Items": [
                    "Provide AWS certification training",
                    "Engage migration consultants",
                    "Implement mentorship program",
                    "Create detailed runbooks"
                ],
                "Timeline": "2-4 weeks training period"
            })
        
        if 'No Rollback Plan' in risk_factors:
            mitigation_strategies.append({
                "Risk": "No Rollback Plan",
                "Strategy": "Develop comprehensive rollback procedures",
                "Action Items": [
                    "Create automated rollback scripts",
                    "Maintain standby systems",
                    "Test rollback procedures",
                    "Document recovery processes"
                ],
                "Timeline": "1-2 weeks preparation"
            })
        
        # Display mitigation strategies
        for strategy in mitigation_strategies:
            with st.expander(f"üõ°Ô∏è Mitigation: {strategy['Risk']}"):
                st.markdown(f"**Strategy:** {strategy['Strategy']}")
                st.markdown(f"**Timeline:** {strategy['Timeline']}")
                st.markdown("**Action Items:**")
                for item in strategy['Action Items']:
                    st.write(f"‚Ä¢ {item}")
        
        # Risk Heat Map
        st.markdown('<div class="section-header">üî• Risk Heat Map</div>', unsafe_allow_html=True)
        
        if risk_factors:
            risk_names = list(risk_factors.keys())
            risk_scores = [rf['score'] for rf in risk_factors.values()]
            risk_levels = [rf['level'] for rf in risk_factors.values()]
            
            color_map = {'Critical': '#dc3545', 'High': '#fd7e14', 'Medium': '#ffc107', 'Low': '#28a745'}
            colors = [color_map.get(level, '#6c757d') for level in risk_levels]
            
            fig_risk = go.Figure()
            
            fig_risk.add_trace(go.Bar(
                y=risk_names,
                x=risk_scores,
                orientation='h',
                marker_color=colors,
                text=[f"{score}/50" for score in risk_scores],
                textposition='auto'
            ))
            
            fig_risk.update_layout(
                title="Risk Factor Analysis",
                xaxis_title="Risk Score",
                yaxis_title="Risk Factors",
                height=max(400, len(risk_factors) * 50)
            )
            st.plotly_chart(fig_risk, use_container_width=True)
        
    def render_database_plan_tab(self):
        """Render comprehensive database migration plan tab"""
        if not st.session_state.current_database_analysis:
            st.warning("‚ö†Ô∏è Please run analysis in Configuration tab first.")
            return
        
        st.markdown('<div class="section-header">üìã Comprehensive Migration Plan</div>', unsafe_allow_html=True)
        
        analysis = st.session_state.current_database_analysis
        config = analysis['config']
        sizing = analysis['sizing']
        
        # Calculate timeline based on complexity and size
        base_timeline_weeks = 8  # Base timeline
        
        # Adjust timeline based on factors
        if config['database_size_gb'] > 1000:
            base_timeline_weeks += 2
        if config['database_size_gb'] > 10000:
            base_timeline_weeks += 4
        
        if config['schema_complexity'] == 'Complex':
            base_timeline_weeks += 3
        elif config['schema_complexity'] == 'Moderate':
            base_timeline_weeks += 1
        
        if config['migration_type'] == 'Heterogeneous':
            base_timeline_weeks += 4
        
        if config['downtime_tolerance'] == 'Zero':
            base_timeline_weeks += 3
        
        if config['team_experience'] == 'Novice':
            base_timeline_weeks += 2
        
        if len(config.get('compliance_frameworks', [])) > 2:
            base_timeline_weeks += 2
        
        # Migration phases
        phases = [
            {
                "phase": "Phase 1: Planning & Assessment",
                "duration_weeks": 2,
                "tasks": [
                    "Complete detailed database assessment",
                    "Finalize target architecture design",
                    "Create detailed migration plan",
                    "Set up project team and governance",
                    "Establish communication protocols",
                    "Define success criteria and KPIs"
                ],
                "deliverables": [
                    "Migration Strategy Document",
                    "Technical Architecture Design",
                    "Project Charter",
                    "Risk Assessment Report"
                ],
                "dependencies": "Business approval and team allocation"
            },
            {
                "phase": "Phase 2: Environment Setup",
                "duration_weeks": 2,
                "tasks": [
                    "Set up AWS target environment",
                    "Configure RDS/Aurora instances",
                    "Set up network connectivity",
                    "Configure security groups and IAM",
                    "Set up monitoring and alerting",
                    "Install and configure migration tools"
                ],
                "deliverables": [
                    "Target AWS Environment",
                    "Network Configuration",
                    "Security Configuration",
                    "Monitoring Setup"
                ],
                "dependencies": "AWS account setup and network planning"
            },
            {
                "phase": "Phase 3: Schema Migration & Testing",
                "duration_weeks": 3,
                "tasks": [
                    "Export and convert database schema",
                    "Create data type mapping rules",
                    "Set up Database Migration Service",
                    "Perform initial schema validation",
                    "Execute test data migration",
                    "Validate data integrity and performance"
                ],
                "deliverables": [
                    "Converted Database Schema",
                    "DMS Configuration",
                    "Test Migration Results",
                    "Data Validation Reports"
                ],
                "dependencies": "Target environment ready"
            },
            {
                "phase": "Phase 4: Application Testing",
                "duration_weeks": 3,
                "tasks": [
                    "Update application connection strings",
                    "Perform application compatibility testing",
                    "Execute performance testing",
                    "Test backup and recovery procedures",
                    "Validate monitoring and alerting",
                    "User acceptance testing"
                ],
                "deliverables": [
                    "Application Test Results",
                    "Performance Test Reports",
                    "Backup/Recovery Validation",
                    "UAT Sign-off"
                ],
                "dependencies": "Schema migration completed"
            },
            {
                "phase": "Phase 5: Production Migration",
                "duration_weeks": 1,
                "tasks": [
                    "Execute production data migration",
                    "Perform final data synchronization",
                    "Switch application traffic",
                    "Validate production performance",
                    "Monitor for issues",
                    "Execute rollback if needed"
                ],
                "deliverables": [
                    "Production Database",
                    "Migration Execution Report",
                    "Performance Validation",
                    "Go-Live Confirmation"
                ],
                "dependencies": "All testing completed and approved"
            },
            {
                "phase": "Phase 6: Post-Migration Optimization",
                "duration_weeks": 2,
                "tasks": [
                    "Performance tuning and optimization",
                    "Cost optimization review",
                    "Security configuration review",
                    "Documentation updates",
                    "Team training on new environment",
                    "Project retrospective"
                ],
                "deliverables": [
                    "Optimized Production Environment",
                    "Cost Optimization Report",
                    "Updated Documentation",
                    "Training Materials",
                    "Project Closure Report"
                ],
                "dependencies": "Successful production migration"
            }
        ]
        
        # Executive Summary
        st.markdown('<div class="section-header">üìä Migration Timeline Overview</div>', unsafe_allow_html=True)
        
        total_weeks = sum(phase['duration_weeks'] for phase in phases)
        estimated_completion = datetime.now() + timedelta(weeks=total_weeks)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Duration", f"{total_weeks} weeks")
            st.metric("Estimated Completion", estimated_completion.strftime("%B %Y"))
        
        with col2:
            st.metric("Migration Phases", str(len(phases)))
            st.metric("Risk Level", "Medium")  # This could be calculated from risk assessment
        
        with col3:
            st.metric("Team Size Required", "6-8 people")
            st.metric("Downtime Window", config['downtime_tolerance'])
        
        with col4:
            effort_hours = total_weeks * 40 * 6  # 6 people * 40 hours/week
            st.metric("Total Effort", f"{effort_hours:,} hours")
            st.metric("Migration Method", config['migration_method'])
        
        # Gantt Chart
        st.markdown('<div class="section-header">üìÖ Migration Timeline (Gantt Chart)</div>', unsafe_allow_html=True)
        
        # Create Gantt chart data
        start_date = datetime.now()
        gantt_data = []
        current_date = start_date
        
        for phase in phases:
            end_date = current_date + timedelta(weeks=phase['duration_weeks'])
            gantt_data.append({
                'Phase': phase['phase'],
                'Start': current_date,
                'End': end_date,
                'Duration': f"{phase['duration_weeks']} weeks"
            })
            current_date = end_date
        
        # Create Gantt chart
        fig_gantt = go.Figure()
        
        for i, phase_data in enumerate(gantt_data):
            fig_gantt.add_trace(go.Scatter(
                x=[phase_data['Start'], phase_data['End']],
                y=[i, i],
                mode='lines',
                line=dict(width=20, color=f'rgb({50 + i*30}, {100 + i*20}, {200 - i*20})'),
                name=phase_data['Phase'],
                hovertemplate=f"<b>{phase_data['Phase']}</b><br>" +
                            f"Duration: {phase_data['Duration']}<br>" +
                            f"Start: {phase_data['Start'].strftime('%Y-%m-%d')}<br>" +
                            f"End: {phase_data['End'].strftime('%Y-%m-%d')}<extra></extra>"
            ))
        
        fig_gantt.update_layout(
            title="Migration Timeline - Gantt Chart",
            xaxis_title="Date",
            yaxis_title="Migration Phases",
            yaxis=dict(
                tickmode='array',
                tickvals=list(range(len(gantt_data))),
                ticktext=[phase['phase'].replace('Phase ', 'P') for phase in phases]
            ),
            height=400,
            showlegend=False
        )
        st.plotly_chart(fig_gantt, use_container_width=True)
        
        # Detailed Phase Breakdown
        st.markdown('<div class="section-header">üìã Detailed Phase Breakdown</div>', unsafe_allow_html=True)
        
        for i, phase in enumerate(phases):
            with st.expander(f"{phase['phase']} ({phase['duration_weeks']} weeks)"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**üìã Key Tasks:**")
                    for task in phase['tasks']:
                        st.write(f"‚Ä¢ {task}")
                    
                    st.markdown(f"**üîó Dependencies:** {phase['dependencies']}")
                
                with col2:
                    st.markdown("**üì¶ Deliverables:**")
                    for deliverable in phase['deliverables']:
                        st.write(f"‚Ä¢ {deliverable}")
        
        # Resource Allocation
        st.markdown('<div class="section-header">üë• Resource Allocation</div>', unsafe_allow_html=True)
        
        resources = [
            {"Role": "Project Manager", "Allocation": "100%", "Duration": f"{total_weeks} weeks", "Responsibilities": "Overall project coordination and management"},
            {"Role": "Database Architect", "Allocation": "80%", "Duration": f"{total_weeks} weeks", "Responsibilities": "Schema design and performance optimization"},
            {"Role": "Cloud Engineer", "Allocation": "60%", "Duration": f"{total_weeks-2} weeks", "Responsibilities": "AWS infrastructure setup and configuration"},
            {"Role": "DBA", "Allocation": "100%", "Duration": f"{total_weeks} weeks", "Responsibilities": "Database migration execution and validation"},
            {"Role": "Application Developer", "Allocation": "40%", "Duration": f"{phases[3]['duration_weeks'] + phases[4]['duration_weeks']} weeks", "Responsibilities": "Application testing and cutover"},
            {"Role": "QA Engineer", "Allocation": "60%", "Duration": f"{phases[2]['duration_weeks'] + phases[3]['duration_weeks']} weeks", "Responsibilities": "Testing and validation"},
            {"Role": "Security Engineer", "Allocation": "30%", "Duration": f"{phases[1]['duration_weeks'] + phases[2]['duration_weeks']} weeks", "Responsibilities": "Security configuration and compliance"}
        ]
        
        resource_df = pd.DataFrame(resources)
        self.safe_dataframe_display(resource_df)
        
        # Critical Success Factors
        st.markdown('<div class="section-header">üéØ Critical Success Factors</div>', unsafe_allow_html=True)
        
        success_factors = [
            "üéØ **Executive Sponsorship**: Strong leadership support and clear decision-making authority",
            "üë• **Skilled Team**: Experienced team members with AWS and database migration expertise",
            "üìã **Detailed Planning**: Comprehensive migration plan with clear milestones and dependencies",
            "üß™ **Thorough Testing**: Extensive testing at each phase to identify and resolve issues early",
            "üìä **Continuous Monitoring**: Real-time monitoring during migration to detect and address problems",
            "üîÑ **Rollback Plan**: Well-tested rollback procedures in case of migration failure",
            "üí¨ **Clear Communication**: Regular updates to stakeholders and transparent issue reporting",
            "üéì **Training**: Adequate training for operations team on new AWS environment"
        ]
        
        for factor in success_factors:
            st.write(factor)
        
        # Risk Mitigation in Timeline
        st.markdown('<div class="section-header">‚ö†Ô∏è Timeline Risk Factors</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**‚ö†Ô∏è Potential Delays:**")
            st.write("‚Ä¢ Complex schema conversion issues (+2-4 weeks)")
            st.write("‚Ä¢ Application compatibility problems (+1-3 weeks)")
            st.write("‚Ä¢ Performance optimization requirements (+1-2 weeks)")
            st.write("‚Ä¢ Compliance/security review delays (+1-2 weeks)")
            st.write("‚Ä¢ Unexpected data quality issues (+1-3 weeks)")
        
        with col2:
            st.markdown("**‚ö° Acceleration Opportunities:**")
            st.write("‚Ä¢ Parallel execution of independent tasks (-1-2 weeks)")
            st.write("‚Ä¢ Automated testing and validation (-1 week)")
            st.write("‚Ä¢ Pre-migration schema optimization (-1 week)")
            st.write("‚Ä¢ Dedicated migration team (-1-2 weeks)")
            st.write("‚Ä¢ AWS Professional Services engagement (-2-3 weeks)")
        
    def render_database_dashboard_tab(self):
        """Render comprehensive database migration executive dashboard"""
        if not st.session_state.current_database_analysis:
            st.warning("‚ö†Ô∏è Please run analysis in Configuration tab first.")
            return
        
        st.markdown('<div class="section-header">üìà Database Migration Executive Dashboard</div>', unsafe_allow_html=True)
        
        analysis = st.session_state.current_database_analysis
        config = analysis['config']
        sizing = analysis['sizing']
        
        # Executive Summary Section
        st.markdown('<div class="section-header">üìä Executive Summary</div>', unsafe_allow_html=True)
        
        # Calculate key metrics
        writer = sizing['writer_instance']
        readers = sizing['reader_instances']
        storage = sizing['storage_config']
        
        # Cost calculations
        writer_monthly_cost = writer['specs']['cost_factor'] * 24 * 30
        reader_monthly_cost = sum(reader['specs']['cost_factor'] * 24 * 30 for reader in readers)
        storage_monthly_cost = storage['allocated_storage_gb'] * 0.10
        total_monthly_cost = writer_monthly_cost + reader_monthly_cost + storage_monthly_cost + 200  # Additional services
        
        # Timeline estimation
        base_weeks = 8
        if config['database_size_gb'] > 1000: base_weeks += 2
        if config['schema_complexity'] == 'Complex': base_weeks += 3
        if config['migration_type'] == 'Heterogeneous': base_weeks += 4
        
        # Risk assessment
        risk_score = 0
        if config['database_size_gb'] > 10000: risk_score += 15
        if config['schema_complexity'] == 'Complex': risk_score += 20
        if config['migration_type'] == 'Heterogeneous': risk_score += 25
        if config['downtime_tolerance'] == 'Zero': risk_score += 30
        if config['team_experience'] == 'Novice': risk_score += 25
        
        risk_level = 'Critical' if risk_score >= 80 else 'High' if risk_score >= 60 else 'Medium' if risk_score >= 40 else 'Low'
        
        # Top-level metrics
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Monthly Cost", f"${total_monthly_cost:,.0f}")
            st.metric("Annual Cost", f"${total_monthly_cost * 12:,.0f}")
        
        with col2:
            st.metric("Timeline", f"{base_weeks} weeks")
            completion_date = datetime.now() + timedelta(weeks=base_weeks)
            st.metric("Est. Completion", completion_date.strftime("%b %Y"))
        
        with col3:
            st.metric("Risk Level", risk_level)
            success_rate = max(20, 100 - risk_score)
            st.metric("Success Rate", f"{success_rate}%")
        
        with col4:
            st.metric("Database Size", f"{config['database_size_gb']:,} GB")
            st.metric("Target Instance", writer['instance_type'])
        
        with col5:
            roi = ((config['database_size_gb'] * 0.50 - total_monthly_cost) * 12) / (total_monthly_cost * 12) * 100
            st.metric("Annual ROI", f"{max(0, roi):.1f}%")
            st.metric("Migration Type", config['migration_type'])
        
        # Status Indicators
        st.markdown('<div class="section-header">üö¶ Project Status</div>', unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Budget status
            if total_monthly_cost < 5000:
                budget_status = "üü¢ Within Budget"
            elif total_monthly_cost < 10000:
                budget_status = "üü° Monitor Budget"
            else:
                budget_status = "üî¥ Over Budget"
            
            st.markdown(f"""
            <div class="metric-card">
                <h4>üí∞ Budget Status</h4>
                <p><strong>Status:</strong> {budget_status}</p>
                <p><strong>Monthly:</strong> ${total_monthly_cost:,.0f}</p>
                <p><strong>Annual:</strong> ${total_monthly_cost * 12:,.0f}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            # Timeline status
            if base_weeks <= 12:
                timeline_status = "üü¢ On Track"
            elif base_weeks <= 16:
                timeline_status = "üü° Delayed"
            else:
                timeline_status = "üî¥ Critical Delay"
            
            st.markdown(f"""
            <div class="metric-card">
                <h4>‚è∞ Timeline Status</h4>
                <p><strong>Status:</strong> {timeline_status}</p>
                <p><strong>Duration:</strong> {base_weeks} weeks</p>
                <p><strong>Completion:</strong> {completion_date.strftime('%B %Y')}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            # Risk status
            risk_colors = {'Low': 'üü¢', 'Medium': 'üü°', 'High': 'üü†', 'Critical': 'üî¥'}
            risk_status = f"{risk_colors.get(risk_level, '‚ö™')} {risk_level} Risk"
            
            st.markdown(f"""
            <div class="metric-card">
                <h4>‚ö†Ô∏è Risk Status</h4>
                <p><strong>Status:</strong> {risk_status}</p>
                <p><strong>Score:</strong> {risk_score}/200</p>
                <p><strong>Success Rate:</strong> {success_rate}%</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Key Recommendations
        st.markdown('<div class="section-header">üéØ Key Recommendations</div>', unsafe_allow_html=True)
        
        recommendations = []
        
        if writer['cpu_utilization'] > 80:
            recommendations.append("‚ö†Ô∏è Consider upgrading to a larger instance type to avoid CPU bottlenecks")
        
        if len(readers) == 0 and config['read_query_percentage'] > 60:
            recommendations.append("üìñ Add read replicas to distribute read workload and improve performance")
        
        if config['migration_type'] == 'Heterogeneous':
            recommendations.append("üîÑ Plan for extensive testing due to heterogeneous migration complexity")
        
        if config['downtime_tolerance'] == 'Zero':
            recommendations.append("‚è∞ Implement continuous replication strategy for zero-downtime migration")
        
        if risk_score > 60:
            recommendations.append("üõ°Ô∏è Develop comprehensive risk mitigation plan before proceeding")
        
        if config['team_experience'] == 'Novice':
            recommendations.append("üéì Invest in team training or consider engaging AWS Professional Services")
        
        recommendations.append("üí∞ Consider Reserved Instances for 30-60% cost savings on predictable workloads")
        recommendations.append("üìä Implement comprehensive monitoring and alerting from day one")
        
        for i, rec in enumerate(recommendations[:6]):  # Show top 6 recommendations
            st.write(f"{i+1}. {rec}")
        
        # Cost Breakdown Visualization
        st.markdown('<div class="section-header">üí∞ Cost Breakdown</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Cost pie chart
            cost_categories = {
                'Writer Instance': writer_monthly_cost,
                'Read Replicas': reader_monthly_cost,
                'Storage': storage_monthly_cost,
                'Additional Services': 200
            }
            
            fig_pie = go.Figure(data=[go.Pie(
                labels=list(cost_categories.keys()),
                values=list(cost_categories.values()),
                hole=.3
            )])
            
            fig_pie.update_layout(title="Monthly Cost Distribution", height=400)
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # Cost projection over time
            months = list(range(1, 25))  # 2 years
            growth_rate = config.get('annual_growth_rate', 0.2)
            monthly_growth_rate = growth_rate / 12
            
            projected_costs = []
            for month in months:
                projected_cost = total_monthly_cost * (1 + monthly_growth_rate) ** month
                projected_costs.append(projected_cost)
            
            fig_projection = go.Figure()
            
            fig_projection.add_trace(go.Scatter(
                x=months,
                y=projected_costs,
                mode='lines+markers',
                name='Projected Costs',
                line=dict(color='#e74c3c', width=2)
            ))
            
            fig_projection.update_layout(
                title="24-Month Cost Projection",
                xaxis_title="Month",
                yaxis_title="Monthly Cost ($)",
                height=400
            )
            st.plotly_chart(fig_projection, use_container_width=True)
        
        # Performance Metrics
        st.markdown('<div class="section-header">‚ö° Performance Metrics</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Peak TPS", f"{config['transactions_per_second']:,}")
            st.metric("Peak Connections", f"{config['concurrent_connections']:,}")
        
        with col2:
            st.metric("CPU Utilization", f"{writer['cpu_utilization']:.1f}%")
            st.metric("Memory Utilization", f"{writer['memory_utilization']:.1f}%")
        
        with col3:
            st.metric("Storage IOPS", f"{storage['provisioned_iops']:,}")
            st.metric("Storage Type", storage['storage_type'])
        
        with col4:
            st.metric("Read Queries", f"{config['read_query_percentage']}%")
            st.metric("Multi-AZ", "Yes" if storage['multi_az'] else "No")
        
        # Migration Readiness Assessment
        st.markdown('<div class="section-header">‚úÖ Migration Readiness Assessment</div>', unsafe_allow_html=True)
        
        readiness_factors = [
            {"Factor": "Technical Architecture", "Status": "‚úÖ Complete" if writer else "‚ùå Incomplete", "Score": 100 if writer else 0},
            {"Factor": "Cost Planning", "Status": "‚úÖ Complete", "Score": 100},
            {"Factor": "Risk Assessment", "Status": "‚úÖ Complete" if risk_score < 100 else "‚ö†Ô∏è Needs Attention", "Score": max(0, 100 - risk_score)},
            {"Factor": "Team Readiness", "Status": "‚úÖ Ready" if config['team_experience'] != 'Novice' else "‚ö†Ô∏è Training Needed", "Score": 100 if config['team_experience'] != 'Novice' else 60},
            {"Factor": "Compliance Review", "Status": "‚úÖ Complete" if config.get('compliance_frameworks') else "‚ö†Ô∏è Pending", "Score": 100 if config.get('compliance_frameworks') else 70},
            {"Factor": "Rollback Plan", "Status": "‚úÖ Complete" if config['rollback_plan'] != 'None' else "‚ùå Missing", "Score": 100 if config['rollback_plan'] != 'None' else 0}
        ]
        
        readiness_df = pd.DataFrame(readiness_factors)
        self.safe_dataframe_display(readiness_df[['Factor', 'Status']])
        
        overall_readiness = sum(factor['Score'] for factor in readiness_factors) / len(readiness_factors)
        
        if overall_readiness >= 90:
            readiness_status = "üü¢ Ready to Proceed"
            readiness_color = "#28a745"
        elif overall_readiness >= 70:
            readiness_status = "üü° Minor Issues"
            readiness_color = "#ffc107"
        else:
            readiness_status = "üî¥ Not Ready"
            readiness_color = "#dc3545"
        
        st.markdown(f"""
        <div style="background: {readiness_color}20; border-left: 4px solid {readiness_color}; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h4>üéØ Overall Migration Readiness: {readiness_status}</h4>
            <p><strong>Readiness Score:</strong> {overall_readiness:.1f}/100</p>
            <p><strong>Recommendation:</strong> {'Proceed with migration' if overall_readiness >= 90 else 'Address issues before proceeding' if overall_readiness >= 70 else 'Significant preparation required'}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Action Items
        st.markdown('<div class="section-header">üìã Next Steps</div>', unsafe_allow_html=True)
        
        next_steps = []
        
        if overall_readiness >= 90:
            next_steps = [
                "1. ‚úÖ Finalize migration timeline and resource allocation",
                "2. üîß Set up AWS target environment",
                "3. üìä Implement monitoring and alerting",
                "4. üß™ Begin pilot migration testing",
                "5. üìã Conduct final stakeholder review",
                "6. üöÄ Execute migration plan"
            ]
        elif overall_readiness >= 70:
            next_steps = [
                "1. ‚ö†Ô∏è Address identified readiness gaps",
                "2. üéì Complete team training if needed",
                "3. üõ°Ô∏è Finalize risk mitigation strategies",
                "4. üìã Review and approve rollback procedures",
                "5. ‚úÖ Re-assess migration readiness",
                "6. üöÄ Proceed with migration when ready"
            ]
        else:
            next_steps = [
                "1. üî¥ Address critical readiness issues",
                "2. üéì Invest in team training and expertise",
                "3. üìã Develop comprehensive rollback plan",
                "4. üõ°Ô∏è Create detailed risk mitigation strategy",
                "5. üí∞ Review and adjust budget allocation",
                "6. ‚è∞ Extend timeline to address preparation needs"
            ]
        
        for step in next_steps:
            st.write(step)
        
    def render_bulk_upload_tab(self):
            """Render bulk upload functionality"""
            st.markdown('<div class="section-header">üì§ Bulk Database Configuration Upload</div>', unsafe_allow_html=True)
            
            st.info("Upload CSV/Excel files with multiple database configurations for batch analysis.")
            
            uploaded_file = st.file_uploader(
                "Choose CSV or Excel file", 
                type=['csv', 'xlsx', 'xls'],
                help="Upload file with database configurations for bulk analysis"
            )
            
            if uploaded_file is not None:
                try:
                    # Show file details
                    file_details = {
                        "Filename": uploaded_file.name,
                        "File size": f"{uploaded_file.size / 1024:.2f} KB",
                        "File type": uploaded_file.type
                    }
                    st.write("**File Details:**")
                    for key, value in file_details.items():
                        st.write(f"- {key}: {value}")
                    
                    if uploaded_file.name.endswith('.csv'):
                        df = pd.read_csv(uploaded_file)
                    else:
                        df = pd.read_excel(uploaded_file)
                    
                    st.success(f"‚úÖ File uploaded successfully! Found {len(df)} database configurations.")
                    st.dataframe(df.head(10), use_container_width=True)
                    
                    # Validate required columns
                    required_columns = ['database_name', 'size_gb', 'workload_type', 'connections', 'environment']
                    missing_columns = [col for col in required_columns if col not in df.columns]
                    
                    if missing_columns:
                        st.warning(f"‚ö†Ô∏è Missing required columns: {', '.join(missing_columns)}")
                        st.info("Please ensure your file contains all required columns. Use the sample template as reference.")
                    else:
                        st.success("‚úÖ All required columns present!")
                        
                        if st.button("üöÄ Process Bulk Configurations"):
                            with st.spinner("Processing bulk configurations..."):
                                processed_configs = []
                                
                                for index, row in df.iterrows():
                                    try:
                                        config = {
                                            "database_name": row.get('database_name', f'DB_{index}'),
                                            "database_size_gb": float(row.get('size_gb', 100)),
                                            "workload_type": row.get('workload_type', 'Mixed'),
                                            "concurrent_connections": int(row.get('connections', 100)),
                                            "environment": row.get('environment', 'Production'),
                                            "transactions_per_second": int(row.get('tps', 1000)),
                                            "read_query_percentage": float(row.get('read_pct', 70)),
                                            "high_availability": row.get('ha_required', 'Yes').lower() == 'yes',
                                            "annual_growth_rate": float(row.get('growth_rate', 20)) / 100
                                        }
                                        
                                        # Run sizing analysis
                                        sizing_result = self.database_sizing_engine.calculate_sizing_requirements(config)
                                        
                                        processed_configs.append({
                                            "Database": config['database_name'],
                                            "Size (GB)": config['database_size_gb'],
                                            "Recommended Instance": sizing_result['writer_instance']['instance_type'],
                                            "Estimated Monthly Cost": f"${sizing_result['writer_instance']['specs']['cost_factor'] * 24 * 30:.0f}",
                                            "Environment": config['environment'],
                                            "HA": "Yes" if config['high_availability'] else "No"
                                        })
                                        
                                    except Exception as e:
                                        st.error(f"Error processing row {index}: {str(e)}")
                                
                                if processed_configs:
                                    st.success(f"‚úÖ Processed {len(processed_configs)} configurations!")
                                    
                                    results_df = pd.DataFrame(processed_configs)
                                    st.dataframe(results_df, use_container_width=True)
                                    
                                    # Export results
                                    csv_results = results_df.to_csv(index=False)
                                    st.download_button(
                                        label="üì• Download Results",
                                        data=csv_results,
                                        file_name=f"bulk_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                    
                except Exception as e:
                    st.error(f"Error reading file: {str(e)}")
            
            # Sample template download
            st.subheader("üì• Sample Template")
            if st.button("üìÑ Download Sample Template"):
                sample_data = {
                    'database_name': ['ProductionDB', 'AnalyticsDB', 'DevDB'],
                    'size_gb': [1000, 500, 100],
                    'workload_type': ['OLTP', 'OLAP', 'Mixed'],
                    'connections': [500, 200, 50],
                    'environment': ['Production', 'Production', 'Development'],
                    'tps': [5000, 1000, 100],
                    'read_pct': [70, 90, 60],
                    'ha_required': ['Yes', 'Yes', 'No'],
                    'growth_rate': [20, 15, 10]
                }
                sample_df = pd.DataFrame(sample_data)
                csv = sample_df.to_csv(index=False)
                st.download_button(
                    label="Download CSV Template",
                    data=csv,
                    file_name="database_migration_template.csv",
                    mime="text/csv"
                )

    def render_vrops_integration_tab(self):
            """Render vROps integration section"""
            st.markdown('<div class="section-header">üîå vRealize Operations Integration</div>', unsafe_allow_html=True)
            
            if not st.session_state.vrops_connected:
                st.info("Connect to vROps to import real performance data for accurate sizing analysis.")
                
                with st.form("vrops_connection"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        vrops_host = st.text_input("vROps Host/IP", placeholder="vrops.company.com")
                        username = st.text_input("Username", placeholder="admin@local")
                    
                    with col2:
                        password = st.text_input("Password", type="password")
                        verify_ssl = st.checkbox("Verify SSL Certificate", value=True)
                    
                    submitted = st.form_submit_button("üîó Connect to vROps")
                    
                    if submitted and vrops_host and username and password:
                        with st.spinner("Connecting to vROps..."):
                            if self.vrops_connector.connect(vrops_host, username, password, verify_ssl):
                                st.session_state.vrops_connected = True
                                st.success("‚úÖ Successfully connected to vROps!")
                                st.rerun()
                            else:
                                st.error("‚ùå Failed to connect to vROps. Please check your credentials.")
            else:
                st.success("‚úÖ Connected to vROps")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("üìä Import Database Metrics"):
                        with st.spinner("Importing vROps metrics..."):
                            metrics = self.vrops_connector.get_database_metrics()
                            if metrics:
                                st.success(f"‚úÖ Imported metrics for {len(metrics)} resources")
                                st.dataframe(pd.DataFrame(metrics), use_container_width=True)
                
                with col2:
                    if st.button("üìà Performance Analysis"):
                        st.info("Running performance analysis on imported vROps data...")
                        # Add performance analysis logic here
                
                with col3:
                    if st.button("üîå Disconnect"):
                        st.session_state.vrops_connected = False
                        st.success("Disconnected from vROps")
                        st.rerun()
                
                # vROps configuration status
                if st.session_state.vrops_connected:
                    st.subheader("üìä vROps Data Overview")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("Connected Resources", "0", help="Number of resources discovered")
                    with col2:
                        st.metric("Database Instances", "0", help="Database instances found")
                    with col3:
                        st.metric("Performance Metrics", "0", help="Available performance metrics")
                    with col4:
                        st.metric("Last Sync", "Never", help="Last synchronization time")
                    
                    # Sample vROps data visualization
                    st.subheader("üìà Sample Performance Data")
                    st.info("vROps integration provides real-time performance data for accurate database sizing recommendations.")
                    
                    # Help section
                    with st.expander("üîç vROps Integration Help"):
                        st.markdown("""
                        **What is vROps Integration?**
                        
                        vRealize Operations Manager integration allows you to:
                        
                        ‚Ä¢ **Import Real Performance Data**: Get actual CPU, memory, and I/O metrics from your existing databases
                        ‚Ä¢ **Accurate Sizing**: Use historical performance data for precise AWS instance sizing
                        ‚Ä¢ **Trend Analysis**: Analyze growth patterns and seasonal variations
                        ‚Ä¢ **Risk Mitigation**: Identify performance bottlenecks before migration
                        
                        **Required Permissions:**
                        - Read access to database resources
                        - Performance metrics access
                        - API access to vROps REST endpoints
                        
                        **Supported Metrics:**
                        - CPU utilization (average, peak)
                        - Memory usage (active, consumed)
                        - Disk I/O (IOPS, throughput)
                        - Network utilization
                        - Connection counts
                        """)
            
            # Benefits section
            st.subheader("üéØ Benefits of vROps Integration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **Accuracy Improvements:**
                ‚Ä¢ 95%+ sizing accuracy vs 70% with estimates
                ‚Ä¢ Real workload patterns vs assumptions
                ‚Ä¢ Historical trend analysis
                ‚Ä¢ Peak vs average workload identification
                """)
            
            with col2:
                st.markdown("""
                **Risk Reduction:**
                ‚Ä¢ Identify performance bottlenecks
                ‚Ä¢ Validate migration assumptions
                ‚Ä¢ Optimize instance selection
                ‚Ä¢ Prevent over/under-provisioning
                """)
                
    def run(self):
            """Main application entry point"""
            # Render unified header and navigation
            self.render_header()
            self.render_main_navigation()
            
            # Real-time update indicator
            current_time = datetime.now()
            time_since_update = (current_time - self.last_update_time).seconds
            
            st.markdown(f"""
            <div style="text-align: right; color: #666; font-size: 0.8em; margin-bottom: 1rem;">
                <span class="real-time-indicator"></span>Last updated: {current_time.strftime('%H:%M:%S')} | Platform: Enterprise Migration Suite
            </div>
            """, unsafe_allow_html=True)
            
            # Render appropriate main tab
            if st.session_state.active_main_tab == "overview":
                self.render_overview_tab()
            elif st.session_state.active_main_tab == "network":
                self.render_network_migration_platform()
            elif st.session_state.active_main_tab == "database":
                self.render_database_migration_platform()
            elif st.session_state.active_main_tab == "unified":
                self.render_unified_analytics_tab()
            elif st.session_state.active_main_tab == "reports":
                self.render_reports_tab()
            
            # Update timestamp
            self.last_update_time = current_time
            
            # Footer
            st.markdown("---")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**üè¢ Enterprise Migration Platform v2.0**")
                st.markdown("*Unified Network & Database Migration*")
            
            with col2:
                st.markdown("**ü§ñ AI-Powered Features**")
                st.markdown("‚Ä¢ Intelligent Sizing & Cost Analysis")
                st.markdown("‚Ä¢ Real-time Performance Optimization")
            
            with col3:
                st.markdown("**üîí Enterprise Security**")
                st.markdown("‚Ä¢ SOC2 Type II Certified")
                st.markdown("‚Ä¢ Zero Trust Architecture")    
        
        
        # =========================================================================
        # UNIFIED ANALYTICS AND REPORTS
        # =========================================================================
        
    def render_unified_analytics_tab(self):
            """Render unified analytics combining network and database insights"""
            st.markdown('<div class="section-header">üìä Unified Migration Analytics</div>', unsafe_allow_html=True)
            
            # Check if we have both analyses
            has_network = st.session_state.current_network_analysis is not None
            has_database = st.session_state.current_database_analysis is not None
            
            if not has_network and not has_database:
                st.warning("‚ö†Ô∏è No migration analyses available. Please run network or database analysis first.")
                return
            
            # Combined metrics dashboard
            col1, col2, col3, col4 = st.columns(4)
            
            if has_network:
                st.info("üåê Network analysis integration available")
            
            if has_database:
                st.info("üóÑÔ∏è Database analysis integration available")
            
            st.markdown("""
            <div class="ai-insight">
                <strong>ü§ñ Unified Platform:</strong> This platform combines network and database migration analysis for comprehensive enterprise planning. 
                Run analyses in both sections for unified insights and recommendations.
            </div>
            """, unsafe_allow_html=True)
        
    def render_reports_tab(self):
            """Render comprehensive reporting dashboard"""
            st.markdown('<div class="section-header">üìã Enterprise Migration Reports</div>', unsafe_allow_html=True)
            
            # Report generation options
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("üìä Available Reports")
                
                if st.session_state.current_network_analysis:
                    if st.button("üìÑ Network Migration Report", type="primary"):
                        self.generate_network_report()
                
                if st.session_state.current_database_analysis:
                    if st.button("üìÑ Database Migration Report", type="primary"):
                        self.generate_database_report()
                
                if not st.session_state.current_network_analysis and not st.session_state.current_database_analysis:
                    st.warning("‚ö†Ô∏è No analyses available for reporting. Please run migration analysis first.")
            
            with col2:
                st.subheader("üìã Audit Trail")
                
                if st.session_state.audit_log:
                    # Show recent audit events
                    recent_events = st.session_state.audit_log[-10:]  # Last 10 events
                    
                    audit_data = []
                    for event in reversed(recent_events):
                        timestamp = datetime.fromisoformat(event['timestamp']).strftime("%Y-%m-%d %H:%M")
                        audit_data.append({
                            "Timestamp": timestamp,
                            "Type": event['type'],
                            "Details": event['details'][:50] + "..." if len(event['details']) > 50 else event['details'],
                            "User": event['user']
                        })
                    
                    audit_df = pd.DataFrame(audit_data)
                    self.safe_dataframe_display(audit_df)
                    
                    # Export audit log
                    if st.button("üì§ Export Audit Log"):
                        audit_json = json.dumps(st.session_state.audit_log, indent=2)
                        st.download_button(
                            label="Download Audit Log",
                            data=audit_json,
                            file_name=f"migration_audit_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                else:
                    st.info("No audit events recorded yet.")
        
    def generate_network_report(self):
            """Generate network migration report"""
            st.success("‚úÖ Network migration report feature available!")
            st.info("Generate comprehensive PDF reports with network analysis and recommendations.")
        
    def generate_database_report(self):
            """Generate database migration report"""
            st.success("‚úÖ Database migration report feature available!")
            st.info("Generate comprehensive PDF reports with database sizing and migration plans.")
        
    def run(self):
            """Main application entry point"""
            # Render unified header and navigation
            self.render_header()
            self.render_main_navigation()
            
            # Real-time update indicator
            current_time = datetime.now()
            time_since_update = (current_time - self.last_update_time).seconds
            
            st.markdown(f"""
            <div style="text-align: right; color: #666; font-size: 0.8em; margin-bottom: 1rem;">
                <span class="real-time-indicator"></span>Last updated: {current_time.strftime('%H:%M:%S')} | Platform: Enterprise Migration Suite
            </div>
            """, unsafe_allow_html=True)
            
            # Render appropriate main tab
            if st.session_state.active_main_tab == "overview":
                self.render_overview_tab()
            elif st.session_state.active_main_tab == "network":
                self.render_network_migration_platform()
            elif st.session_state.active_main_tab == "database":
                self.render_database_migration_platform()
            elif st.session_state.active_main_tab == "unified":
                self.render_unified_analytics_tab()
            elif st.session_state.active_main_tab == "reports":
                self.render_reports_tab()
            
            # Update timestamp
            self.last_update_time = current_time
            
            # Footer
            st.markdown("---")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**üè¢ Enterprise Migration Platform v2.0**")
                st.markdown("*Unified Network & Database Migration*")
            
            with col2:
                st.markdown("**ü§ñ AI-Powered Features**")
                st.markdown("‚Ä¢ Intelligent Sizing & Cost Analysis")
                st.markdown("‚Ä¢ Real-time Performance Optimization")
            
            with col3:
                st.markdown("**üîí Enterprise Security**")
                st.markdown("‚Ä¢ SOC2 Type II Certified")
                st.markdown("‚Ä¢ Zero Trust Architecture")

def main():
    """Main function to run the Enterprise Migration Platform"""
    try:
        # Initialize and run the unified platform
        platform = EnterpriseMigrationPlatform()
        platform.run()
        
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        st.write("Please check your configuration and try again.")
        
        # Log the error for debugging
        st.write("**Debug Information:**")
        st.code(f"Error: {str(e)}")
        
        # Provide support contact
        st.info("If the problem persists, please contact support at admin@futureminds.com")

# Application entry point
if __name__ == "__main__":
    main()